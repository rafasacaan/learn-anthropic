{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and optimize RAG using the Anthropic documentation.\n",
    "\n",
    "- Set-up **basic RAG** system in-memory vector db\n",
    "\n",
    "- Build **robust evaluation** suite\n",
    "\n",
    "- Advanced tehcniques: \n",
    "    - *summary indexing*: add a LLM created summary and include in embedding generation. \n",
    "    - *re-ranking*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Metric results**:\n",
    "- Avg Precision: 0.43 --> 0.44\n",
    "- Avg Recall: 0.66 --> 0.69\n",
    "- Avg F1 Score: 0.52 --> 0.54\n",
    "- Avg Mean Reciprocal Rank (MRR): 0.74 --> 0.87\n",
    "- End-to-End Accuracy: 71% --> 81%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import anthropic\n",
    "\n",
    "load_dotenv()\n",
    "client = anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize vector db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use an in-memory DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import voyageai\n",
    "\n",
    "class VectorDB:\n",
    "    def __init__(self, name, api_key=None):\n",
    "        if api_key is None:\n",
    "            api_key = os.getenv(\"VOYAGE_API_KEY\")\n",
    "            \n",
    "        self.client = voyageai.Client(api_key=api_key)\n",
    "        self.name = name\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        self.query_cache = {}\n",
    "        self.db_path = f\"./data/{name}/vector_db.pkl\"\n",
    "\n",
    "    def load_data(self, data):\n",
    "        \n",
    "        if self.embeddings and self.metadata:\n",
    "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
    "            return\n",
    "        \n",
    "        if os.path.exists(self.db_path):\n",
    "            print(\"Loading vector database from disk.\")\n",
    "            self.load_db()\n",
    "            return\n",
    "\n",
    "        texts = [f\"Heading: {item['chunk_heading']}\\n\\n Chunk Text:{item['text']}\" for item in data]\n",
    "        self._embed_and_store(texts, data)\n",
    "        self.save_db()\n",
    "        print(\"Vector database loaded and saved.\")\n",
    "\n",
    "    def _embed_and_store(self, texts, data):\n",
    "        batch_size = 128\n",
    "        result = [\n",
    "            self.client.embed(\n",
    "                texts[i : i + batch_size],\n",
    "                model=\"voyage-2\"\n",
    "            ).embeddings\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "        self.embeddings = [embedding for batch in result for embedding in batch]\n",
    "        self.metadata = data\n",
    "\n",
    "    def search(self, query, k=5, similarity_threshold=0.75):\n",
    "        if query in self.query_cache:\n",
    "            query_embedding = self.query_cache[query]\n",
    "        else:\n",
    "            query_embedding = self.client.embed([query], model=\"voyage-2\").embeddings[0]\n",
    "            self.query_cache[query] = query_embedding\n",
    "\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No data loaded in the vector database.\")\n",
    "\n",
    "        similarities = np.dot(self.embeddings, query_embedding)\n",
    "        top_indices = np.argsort(similarities)[::-1]\n",
    "        top_examples = []\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] >= similarity_threshold:\n",
    "                example = {\n",
    "                    \"metadata\": self.metadata[idx],\n",
    "                    \"similarity\": similarities[idx],\n",
    "                }\n",
    "                top_examples.append(example)\n",
    "                \n",
    "                if len(top_examples) >= k:\n",
    "                    break\n",
    "        self.save_db()\n",
    "        return top_examples\n",
    "\n",
    "    def save_db(self):\n",
    "        data = {\n",
    "            \"embeddings\": self.embeddings,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"query_cache\": json.dumps(self.query_cache),\n",
    "        }\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        with open(self.db_path, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_db(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            raise ValueError(\"Vector database file not found. Use load_data to create a new database.\")\n",
    "        with open(self.db_path, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        self.embeddings = data[\"embeddings\"]\n",
    "        self.metadata = data[\"metadata\"]\n",
    "        self.query_cache = json.loads(data[\"query_cache\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1: basic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bare bones RAG or *naive* RAG. The steps are:\n",
    "\n",
    "1. **Chunk docs** by heading (use subheading)\n",
    "2. **Embed each doc**\n",
    "3. Use cosine similarity to **retreive docs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from typing import Callable, List, Dict, Any, Tuple, Set\n",
    "\n",
    "# Load the evaluation dataset\n",
    "with open('evaluation/docs_evaluation_dataset.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "# Load the Anthropic documentation\n",
    "with open('data/anthropic_docs.json', 'r') as f:\n",
    "    anthropic_docs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'efc09699',\n",
       " 'question': 'How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?',\n",
       " 'correct_chunks': ['https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool#creating-test-cases',\n",
       "  'https://docs.anthropic.com/en/docs/build-with-claude/develop-tests#building-evals-and-test-cases'],\n",
       " 'correct_answer': \"To create multiple test cases in the Anthropic Evaluation tool, click the 'Add Test Case' button, fill in values for each variable in your prompt, and repeat the process to create additional test case scenarios.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunk_link': 'https://docs.anthropic.com/en/docs/welcome#get-started',\n",
       " 'chunk_heading': 'Get started',\n",
       " 'text': 'Get started\\n\\n\\nIf you’re new to Claude, start here to learn the essentials and make your first API call.\\nIntro to ClaudeExplore Claude’s capabilities and development flow.QuickstartLearn how to make your first API call in minutes.Prompt LibraryExplore example prompts for inspiration.\\nIntro to ClaudeExplore Claude’s capabilities and development flow.\\n\\nIntro to Claude\\nExplore Claude’s capabilities and development flow.\\nQuickstartLearn how to make your first API call in minutes.\\n\\nQuickstart\\nLearn how to make your first API call in minutes.\\nPrompt LibraryExplore example prompts for inspiration.\\n\\nPrompt Library\\nExplore example prompts for inspiration.\\n'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anthropic_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize vector DB**\n",
    "\n",
    "We take metadata (**chunk_heading + text**) and embed it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database loaded and saved.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the VectorDB\n",
    "db = VectorDB(\"anthropic_docs\")\n",
    "db.load_data(anthropic_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(232, 232, 1024)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anthropic_docs), len(db.embeddings), len(db.embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build two functions:\n",
    "1) **retrieve**: match query with closest text in the vector db.\n",
    "2) **answer**: provide context to the prompt to get an informed answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_base(query, db):\n",
    "    results = db.search(query, k=3)\n",
    "    context = \"\"\n",
    "    for result in results:\n",
    "        chunk = result['metadata']\n",
    "        context += f\"\\n{chunk['text']}\\n\"\n",
    "    return results, context\n",
    "\n",
    "def answer_query_base(query, db):\n",
    "    documents, context = retrieve_base(query, db)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You have been tasked with helping us to answer the following query: \n",
    "    <query>\n",
    "    {query}\n",
    "    </query>\n",
    "    You have access to the following documents which are meant to provide context as you answer the query:\n",
    "    <documents>\n",
    "    {context}\n",
    "    </documents>\n",
    "    Please remain faithful to the underlying context, and only deviate from it if you are 100% sure that you know the answer already. \n",
    "    Answer the question now, and avoid providing preamble such as 'Here is the answer', etc\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=2500,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When **evaluating RAG applications**, it's critical to evaluate **two things**:\n",
    "- the performance of the retrieval system, and \n",
    "- the end to end system separately\n",
    "\n",
    "We **synthetically generated an evaluation dataset** consisting of 100 samples which include the following:\n",
    "\n",
    "- A question\n",
    "- Chunks from our docs which are relevant to that question. This is what we expect our retrieval system to retrieve when the question is asked\n",
    "- A correct answer to the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def preview_json(file_path, num_items=3):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "        if isinstance(data, list):\n",
    "            preview_data = data[:num_items]\n",
    "        elif isinstance(data, dict):\n",
    "            preview_data = dict(list(data.items())[:num_items])\n",
    "        else:\n",
    "            print(f\"Unexpected data type: {type(data)}. Cannot preview.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Preview of the first {num_items} items from {file_path}:\")\n",
    "        print(json.dumps(preview_data, indent=2))\n",
    "        print(f\"\\nTotal number of items: {len(data)}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON in file: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the first 3 items from evaluation/docs_evaluation_dataset.json:\n",
      "[\n",
      "  {\n",
      "    \"id\": \"efc09699\",\n",
      "    \"question\": \"How can you create multiple test cases for an evaluation in the Anthropic Evaluation tool?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/test-and-evaluate/eval-tool#creating-test-cases\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/develop-tests#building-evals-and-test-cases\"\n",
      "    ],\n",
      "    \"correct_answer\": \"To create multiple test cases in the Anthropic Evaluation tool, click the 'Add Test Case' button, fill in values for each variable in your prompt, and repeat the process to create additional test case scenarios.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1305ea00\",\n",
      "    \"question\": \"What embeddings provider does Anthropic recommend for customized domain-specific models, and what capabilities does this provider offer?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/embeddings#before-implementing-embeddings\",\n",
      "      \"https://docs.anthropic.com/en/docs/build-with-claude/embeddings#how-to-get-embeddings-with-anthropic\"\n",
      "    ],\n",
      "    \"correct_answer\": \"Anthropic recommends Voyage AI for embedding models. Voyage AI offers customized models for specific industry domains like finance and healthcare, as well as bespoke fine-tuned models for individual customers. They have a wide variety of options and capabilities.\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"1811c10d\",\n",
      "    \"question\": \"What are some key success metrics to consider when evaluating Claude's performance on a classification task, and how do they relate to choosing the right model to reduce latency?\",\n",
      "    \"correct_chunks\": [\n",
      "      \"https://docs.anthropic.com/en/docs/about-claude/use-cases/classification#evaluation-metrics\",\n",
      "      \"https://docs.anthropic.com/en/docs/test-and-evaluate/strengthen-guardrails/reduce-latency#1-choose-the-right-model\"\n",
      "    ],\n",
      "    \"correct_answer\": \"When evaluating Claude's performance on a classification task, some key success metrics to consider include accuracy, F1 score, consistency, structure, speed, bias and fairness. Choosing the right model that fits your specific requirements in terms of speed and output quality is a straightforward way to reduce latency and meet the acceptable response time for your use case.\"\n",
      "  }\n",
      "]\n",
      "\n",
      "Total number of items: 100\n"
     ]
    }
   ],
   "source": [
    "preview_json('evaluation/docs_evaluation_dataset.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric definitions\n",
    "https://github.com/anthropics/anthropic-cookbook/blob/main/skills/retrieval_augmented_generation/guide.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision**\n",
    "\n",
    "Precision represents the proportion of retrieved chunks that are actually relevant. It answers the question: \n",
    "\n",
    "**\"Of the chunks we retrieved, how many were correct?\"**\n",
    "\n",
    "Key points:\n",
    "\n",
    "- High precision indicates an efficient system with few false positives.\n",
    "- Low precision suggests many irrelevant chunks are being retrieved.\n",
    "- Our system retrieves a minimum of 3 chunks per query, which may affect precision scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall**\n",
    "\n",
    "Recall measures the completeness of our retrieval system. It answers the question: \n",
    "\n",
    "**\"Of all the correct chunks that exist, how many did we manage to retrieve?\"**\n",
    "\n",
    "Key points:\n",
    "\n",
    "- High recall indicates comprehensive coverage of necessary information.\n",
    "- Low recall suggests important chunks are being missed.\n",
    "- Recall is crucial for ensuring the LLM has access to all needed information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1 Score**\n",
    "\n",
    "The F1 score provides a balanced measure between precision and recall. It's particularly useful when you need a single metric to evaluate system performance, especially with uneven class distributions.\n",
    "\n",
    "Key points:\n",
    "\n",
    "- F1 score ranges from 0 to 1, with 1 representing perfect precision and recall.\n",
    "- It's the harmonic mean of precision and recall, tending towards the lower of the two values.\n",
    "- Useful in scenarios where both false positives and false negatives are important.\n",
    "\n",
    "Interpreting F1 score:\n",
    "\n",
    "- An F1 score of 1.0 indicates perfect precision and recall.\n",
    "- An F1 score of 0.0 indicates the worst performance.\n",
    "- Generally, the higher the F1 score, the better the overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Balancing Precision, Recall, and F1 Score:**\n",
    "- There's often a trade-off between precision and recall.\n",
    "- Our system's minimum chunk retrieval favors recall over precision.\n",
    "- The optimal balance depends on the specific use case.\n",
    "- **In many RAG systems, high recall is often prioritized, as LLMs can filter out less relevant information during generation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mean Reciprocal Rank (MRR) @k**\n",
    "\n",
    "MRR measures how well our system ranks relevant information. It helps us understand how quickly a user would find what they're looking for if they started from the top of our retrieved results.\n",
    "\n",
    "Key points:\n",
    "\n",
    "- MRR ranges from 0 to 1, where 1 is perfect (correct answer always first).\n",
    "- It only considers the rank of the first correct result for each query.\n",
    "- Higher MRR indicates better ranking of relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End to End Metrics**\n",
    "\n",
    "- End to End Accuracy\n",
    "    - We use an LLM-as-judge (Claude 3.5 Sonnet) to evaluate whether the generated answer is correct based on the question and ground truth answer.\n",
    "\n",
    "This metric evaluates the entire pipeline, from retrieval to answer generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**\n",
    "\n",
    "Lets build functions to evaluate the retreival and the end to end pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mrr(retrieved_links: List[str], correct_links: Set[str]) -> float:\n",
    "    for i, link in enumerate(retrieved_links, 1):\n",
    "        if link in correct_links:\n",
    "            return 1 / i\n",
    "    return 0\n",
    "\n",
    "def evaluate_retrieval(retrieval_function: Callable, evaluation_data: List[Dict[str, Any]], db: Any) -> Tuple[float, float, float, float, List[float], List[float], List[float]]:\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    mrrs = []\n",
    "    \n",
    "    for i, item in enumerate(tqdm(evaluation_data, desc=\"Evaluating Retrieval\")):\n",
    "        try:\n",
    "            retrieved_chunks, _ = retrieval_function(item['question'], db)\n",
    "            retrieved_links = [chunk['metadata'].get('chunk_link', chunk['metadata'].get('url', '')) for chunk in retrieved_chunks]\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in retrieval function: {e}\")\n",
    "            continue\n",
    "\n",
    "        correct_links = set(item['correct_chunks'])\n",
    "        \n",
    "        true_positives = len(set(retrieved_links) & correct_links)\n",
    "        precision = true_positives / len(retrieved_links) if retrieved_links else 0\n",
    "        recall = true_positives / len(correct_links) if correct_links else 0\n",
    "        mrr = calculate_mrr(retrieved_links, correct_links)\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        mrrs.append(mrr)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Processed {i + 1}/{len(evaluation_data)} items. Current Avg Precision: {sum(precisions) / len(precisions):.4f}, Avg Recall: {sum(recalls) / len(recalls):.4f}, Avg MRR: {sum(mrrs) / len(mrrs):.4f}\")\n",
    "    \n",
    "    avg_precision = sum(precisions) / len(precisions) if precisions else 0\n",
    "    avg_recall = sum(recalls) / len(recalls) if recalls else 0\n",
    "    avg_mrr = sum(mrrs) / len(mrrs) if mrrs else 0\n",
    "    f1 = 2 * (avg_precision * avg_recall) / (avg_precision + avg_recall) if (avg_precision + avg_recall) > 0 else 0\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs\n",
    "\n",
    "def evaluate_end_to_end(answer_query_function, db, eval_data):\n",
    "    correct_answers = 0\n",
    "    results = []\n",
    "    total_questions = len(eval_data)\n",
    "    \n",
    "    for i, item in enumerate(tqdm(eval_data, desc=\"Evaluating End-to-End\")):\n",
    "        query = item['question']\n",
    "        correct_answer = item['correct_answer']\n",
    "        generated_answer = answer_query_function(query, db)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        You are an AI assistant tasked with evaluating the correctness of answers to questions about Anthropic's documentation.\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Correct Answer: {correct_answer}\n",
    "        \n",
    "        Generated Answer: {generated_answer}\n",
    "        \n",
    "        Is the Generated Answer correct based on the Correct Answer? You should pay attention to the substance of the answer, and ignore minute details that may differ. \n",
    "        \n",
    "        Small differences or changes in wording don't matter. If the generated answer and correct answer are saying essentially the same thing then that generated answer should be marked correct. \n",
    "        \n",
    "        However, if there is any critical piece of information which is missing from the generated answer in comparison to the correct answer, then we should mark this as incorrect. \n",
    "        \n",
    "        Finally, if there are any direct contradictions between the correect answer and generated answer, we should deem the generated answer to be incorrect.\n",
    "        \n",
    "        Respond in the following XML format:\n",
    "        <evaluation>\n",
    "        <content>\n",
    "        <explanation>Your explanation here</explanation>\n",
    "        <is_correct>true/false</is_correct>\n",
    "        </content>\n",
    "        </evaluation>\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = client.messages.create(\n",
    "                model=\"claude-3-5-sonnet-20241022\",\n",
    "                max_tokens=1500,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                    {\"role\": \"assistant\", \"content\": \"<evaluation>\"}\n",
    "                ],\n",
    "                temperature=0,\n",
    "                stop_sequences=[\"</evaluation>\"]\n",
    "            )\n",
    "            \n",
    "            response_text = response.content[0].text\n",
    "            print(response_text)\n",
    "            evaluation = ET.fromstring(response_text)\n",
    "            is_correct = evaluation.find('is_correct').text.lower() == 'true'\n",
    "            \n",
    "            if is_correct:\n",
    "                correct_answers += 1\n",
    "            results.append(is_correct)\n",
    "            \n",
    "            logging.info(f\"Question {i + 1}/{total_questions}: {query}\")\n",
    "            logging.info(f\"Correct: {is_correct}\")\n",
    "            logging.info(\"---\")\n",
    "            \n",
    "        except ET.ParseError as e:\n",
    "            logging.error(f\"XML parsing error: {e}\")\n",
    "            is_correct = 'true' in response_text.lower()\n",
    "            results.append(is_correct)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error: {e}\")\n",
    "            results.append(False)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            current_accuracy = correct_answers / (i + 1)\n",
    "            print(f\"Processed {i + 1}/{total_questions} questions. Current Accuracy: {current_accuracy:.4f}\")\n",
    "        # time.sleep(2)\n",
    "    accuracy = correct_answers / total_questions\n",
    "    return accuracy, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_performance(results_folder='evaluation/json_results', include_methods=None, colors=None):\n",
    "    # Set default colors\n",
    "    default_colors = ['skyblue', 'lightgreen', 'salmon']\n",
    "    if colors is None:\n",
    "        colors = default_colors\n",
    "    \n",
    "    # Load JSON files\n",
    "    results = []\n",
    "    for filename in os.listdir(results_folder):\n",
    "        if filename.endswith('.json'):\n",
    "            file_path = os.path.join(results_folder, filename)\n",
    "            with open(file_path, 'r') as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                    if 'name' not in data:\n",
    "                        print(f\"Warning: {filename} does not contain a 'name' field. Skipping.\")\n",
    "                        continue\n",
    "                    if include_methods is None or data['name'] in include_methods:\n",
    "                        results.append(data)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Warning: {filename} is not a valid JSON file. Skipping.\")\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No JSON files found with matching 'name' fields.\")\n",
    "        return\n",
    "    \n",
    "    # Validate data\n",
    "    required_metrics = [\"average_precision\", \"average_recall\", \"average_f1\", \"average_mrr\", \"end_to_end_accuracy\"]\n",
    "    for result in results.copy():\n",
    "        if not all(metric in result for metric in required_metrics):\n",
    "            print(f\"Warning: {result['name']} is missing some required metrics. Skipping.\")\n",
    "            results.remove(result)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No valid results remaining after validation.\")\n",
    "        return\n",
    "    \n",
    "    # Sort results based on end-to-end accuracy\n",
    "    results.sort(key=lambda x: x['end_to_end_accuracy'])\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    methods = [result['name'] for result in results]\n",
    "    metrics = required_metrics\n",
    "    \n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    \n",
    "    x = range(len(metrics))\n",
    "    width = 0.8 / len(results)\n",
    "    \n",
    "    # Create color palette\n",
    "    num_methods = len(results)\n",
    "    color_palette = colors[:num_methods] + sns.color_palette(\"husl\", num_methods - len(colors))\n",
    "    \n",
    "    # Plot bars for each method\n",
    "    for i, (result, color) in enumerate(zip(results, color_palette)):\n",
    "        values = [result[metric] for metric in metrics]\n",
    "        offset = (i - len(results)/2 + 0.5) * width\n",
    "        bars = plt.bar([xi + offset for xi in x], values, width, label=result['name'], color=color)\n",
    "        \n",
    "        # Add value labels on the bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                     f'{height:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Metrics', fontsize=12)\n",
    "    plt.ylabel('Values', fontsize=12)\n",
    "    plt.title('RAG Performance Metrics (Sorted by End-to-End Accuracy)', fontsize=16)\n",
    "    plt.xticks(x, metrics, rotation=45, ha='right')\n",
    "    plt.legend(title='Methods', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  10%|█         | 10/100 [00:02<00:22,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/100 items. Current Avg Precision: 0.5000, Avg Recall: 0.8000, Avg MRR: 0.8333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  20%|██        | 20/100 [00:05<00:22,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20/100 items. Current Avg Precision: 0.3833, Avg Recall: 0.6500, Avg MRR: 0.6333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  30%|███       | 30/100 [00:07<00:18,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30/100 items. Current Avg Precision: 0.4000, Avg Recall: 0.6556, Avg MRR: 0.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  40%|████      | 40/100 [00:10<00:16,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40/100 items. Current Avg Precision: 0.4500, Avg Recall: 0.6917, Avg MRR: 0.7250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  50%|█████     | 50/100 [00:13<00:13,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/100 items. Current Avg Precision: 0.4333, Avg Recall: 0.6733, Avg MRR: 0.7200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  60%|██████    | 60/100 [00:15<00:10,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 60/100 items. Current Avg Precision: 0.4278, Avg Recall: 0.6722, Avg MRR: 0.7333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  70%|███████   | 70/100 [00:18<00:07,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 70/100 items. Current Avg Precision: 0.4167, Avg Recall: 0.6440, Avg MRR: 0.7071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  80%|████████  | 80/100 [00:21<00:05,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 80/100 items. Current Avg Precision: 0.4396, Avg Recall: 0.6823, Avg MRR: 0.7312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  90%|█████████ | 90/100 [00:24<00:02,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 90/100 items. Current Avg Precision: 0.4352, Avg Recall: 0.6750, Avg MRR: 0.7296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval: 100%|██████████| 100/100 [00:27<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/100 items. Current Avg Precision: 0.4283, Avg Recall: 0.6592, Avg MRR: 0.7333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   1%|          | 1/100 [00:05<08:52,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect. While it provides general guidance about test case development and evaluation, it misses the critical specific procedural detail provided in the correct answer about HOW to actually create multiple test cases in the Anthropic Evaluation tool - namely, that you need to click the 'Add Test Case' button and fill in values for variables in your prompt. The generated answer describes conceptual steps for test case development but doesn't include the actual mechanical process of creating them in the tool. This is a significant omission of practical implementation details that were central to the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   2%|▏         | 2/100 [00:11<09:16,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct in substance compared to the Correct Answer. Both answers identify Voyage AI as Anthropic's recommended embeddings provider and both mention that Voyage AI offers customized/fine-tuned models for specific domains and individual customers. While the Generated Answer provides more specific details about Voyage AI's model offerings that aren't mentioned in the Correct Answer, this additional information doesn't contradict the Correct Answer - it merely elaborates on it. The core claims about Voyage AI's capabilities for domain-specific customization and bespoke fine-tuning are consistent between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   3%|▎         | 3/100 [00:17<09:41,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it covers all the key points mentioned in the Correct Answer and even provides additional helpful details. Both answers mention the same key success metrics: accuracy, F1 score, consistency, structure, speed, and bias/fairness. Both answers also discuss how choosing the right model affects latency and performance. While the Generated Answer goes into more specific detail about model choices (mentioning claude-3-haiku and Sonnet specifically), this additional information doesn't contradict the Correct Answer - it simply elaborates on it. The core message about balancing speed and output quality is consistent between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   4%|▍         | 4/100 [00:23<09:49,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures the same two key benefits of Claude for Sheets mentioned in the Correct Answer:\n",
      "\n",
      "1. Both answers mention the ability to test prompts across evaluation suites in parallel (with the Correct Answer adding that this is faster than sequential chained prompts)\n",
      "\n",
      "2. Both answers mention that Claude for Sheets excels at office tasks like survey analysis and online data processing\n",
      "\n",
      "While the Correct Answer explicitly mentions that parallel testing is \"faster than running chained prompts sequentially,\" this additional detail doesn't change the core point about parallel testing capability. The Generated Answer conveys the same essential information and benefits, just worded slightly differently.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   5%|▌         | 5/100 [00:29<09:05,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same core information - that missing the \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns in the prompt will result in an API error. The Generated Answer actually provides slightly more context by explaining that these turns are expected to indicate the start of human input and assistant response, but this additional detail doesn't change the fundamental correctness of the answer. There are no contradictions between the two answers, and no critical information from the Correct Answer is missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   6%|▌         | 6/100 [00:35<09:09,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it conveys the same essential information as the Correct Answer. Both answers emphasize that:\n",
      "\n",
      "1. Tool use requests are priced based on total tokens like regular requests\n",
      "2. There are additional tokens required for tool use beyond regular input/output tokens\n",
      "3. These additional tokens include the tools parameter, tool use/result content blocks, and system prompts\n",
      "4. These extra tokens contribute to the total cost\n",
      "\n",
      "While the Generated Answer provides some specific numbers (like 294 tokens for tool choice and 261 for system prompt) that aren't in the Correct Answer, this additional detail doesn't contradict the core message. The fundamental pricing mechanism and the impact of additional tokens on total cost are accurately represented in both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   7%|▋         | 7/100 [00:39<08:10,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It contains all the essential information from the Correct Answer - specifically the release date (June 27th, 2024) and what features will be available (API usage, billing details, and rate limits). While the Correct Answer provides slightly more detail by mentioning the specific tabs (Usage, Cost, and Rate Limits), this is a minor detail that doesn't change the core meaning. Both answers convey the same fundamental information about what will be available and when.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   8%|▊         | 8/100 [00:48<10:05,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer captures the key elements from the Correct Answer, though it expands on them in more detail. Both answers emphasize:\n",
      "\n",
      "1. The need to consider whether the task requires in-depth thinking/analysis (the Generated Answer elaborates on this with specific examples like complex math problems)\n",
      "\n",
      "2. The impact on latency due to increased output length (both answers explicitly mention this trade-off)\n",
      "\n",
      "The Generated Answer provides more detail and examples, but the core considerations match those in the Correct Answer. There are no contradictions between the two answers, and no critical pieces of information from the Correct Answer are missing from the Generated Answer. The differences are mainly in the level of detail provided, not in the substance of the guidance.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   9%|▉         | 9/100 [00:55<10:04,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it conveys the same core concept as the Correct Answer - that Claude can be used to summarize PDF documents to make them easier to digest. While the Generated Answer provides more detailed steps and mentions additional capabilities, its fundamental message aligns with the Correct Answer's main point about using Claude to summarize PDFs to understand key points without reading everything. The additional details in the Generated Answer don't contradict the Correct Answer, they simply elaborate on it. There are no critical pieces of information from the Correct Answer that are missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  10%|█         | 10/100 [00:58<08:32,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. According to the Correct Answer, rate limits can be viewed in the \"Rate Limits tab\" of the Developer Console. However, the Generated Answer states they can be found in the \"Plans and Billing section\". These are two different locations, representing a direct contradiction. The Generated Answer provides incorrect information about where to find this specific information in the Anthropic Console.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 10/100 questions. Current Accuracy: 0.8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  11%|█         | 11/100 [01:06<09:24,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect when compared to the correct answer. While the generated answer provides several valid metrics for evaluating a classification system (F1 score, consistency, structure, speed, bias/fairness), it misses the specific metrics mentioned in the correct answer. The correct answer specifically mentions two key metrics: 95th percentile response time and average cost per classification. While the generated answer does mention \"speed\" in general terms, it doesn't mention the specific 95th percentile metric, and more importantly, it completely omits any mention of cost measurements. Since cost per classification is a critical piece of information present in the correct answer but missing from the generated answer, we must mark this as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  12%|█▏        | 12/100 [01:12<09:14,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It conveys the same key information as the Correct Answer:\n",
      "\n",
      "1. For Text Completions API: Both answers specify that the system prompt goes before the first \"\\n\\nHuman:\" turn in the prompt text.\n",
      "\n",
      "2. For Messages API: Both answers indicate that the system prompt is specified as a separate \"system\" parameter in the API request.\n",
      "\n",
      "The Generated Answer actually provides more detail through code examples, but the core substance matches the Correct Answer perfectly. There are no contradictions or missing critical pieces of information between the two answers. The differences are merely in presentation and level of detail, not in substance.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:XML parsing error: mismatched tag: line 3, column 805\n",
      "Evaluating End-to-End:  13%|█▎        | 13/100 [01:19<09:27,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer, while detailed and structured, misses a critical element from the correct answer. The correct answer specifically mentions combining XML tags with chain of thought reasoning where \"Claude explains its step-by-step reasoning process\" and gives a specific example using the <thinking> tag for showing Claude's reasoning process. While the generated answer talks about using XML tags and breaking things into steps, it doesn't explicitly address the core concept of using tags to capture Claude's own reasoning process. The correct answer emphasizes the combination of XML tags with Claude's chain of thought reasoning, while the generated answer focuses more on general prompt structuring and task organization. This represents a significant omission of the key concept.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  14%|█▍        | 14/100 [01:28<10:23,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect for several reasons:\n",
      "\n",
      "1. While it correctly identifies the three types of metrics (accuracy, cost, and latency/response time), it fails to provide the specific numerical results that were given in the correct answer.\n",
      "\n",
      "2. The generated answer uses placeholder text ([RESULT_ACCURACY], [RESULT_COST], [RESULT_LATENCY]) instead of the actual values from the correct answer (89.01%, $0.0004, and 1.61 seconds).\n",
      "\n",
      "3. The generated answer refers to \"average response time\" while the correct answer specifically mentions \"95th percentile response time,\" which is a different measurement.\n",
      "\n",
      "While the general framework of measuring accuracy, cost, and latency is correct, the absence of the specific numerical results and the difference in how the latency metric is described make this answer incomplete and partially incorrect compared to the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  15%|█▌        | 15/100 [01:36<10:32,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect. While it provides detailed steps for implementing Claude more broadly, it does not match the specific pre-prompt engineering requirements mentioned in the correct answer. The correct answer focuses on three key elements that should be in place before starting prompt engineering:\n",
      "1. Clear definition of success criteria\n",
      "2. Ways to empirically test against those criteria\n",
      "3. A first draft prompt to improve\n",
      "\n",
      "The generated answer instead discusses broader implementation steps like scoping use cases, designing integrations, and preparing data. While these may be useful steps generally, they are different from the specific prerequisites for prompt engineering that Anthropic recommends. The generated answer misses these core elements and provides different information entirely.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  16%|█▌        | 16/100 [01:42<09:46,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is partially correct but contains additional claims that go beyond what is established in the correct answer and cannot be verified based on the given correct answer. While point #1 accurately captures the key distinction described in the correct answer about how the two APIs handle mid-response prompting (Messages API using assistant role vs Text Completions using direct prompt pre-filling), points #2 and #3 make claims about streaming formats and response behavior that are not mentioned in or supported by the correct answer. Since these additional claims cannot be verified against the correct answer, and we need to be conservative in our evaluation, the generated answer should be marked as incorrect despite getting the core distinction right.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  17%|█▋        | 17/100 [01:49<09:35,  6.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same core message that Claude's response is more insightful, structured, and actionable when given a specific role (CFO) through a system prompt. While the Generated Answer provides more specific details and examples, these additional details don't contradict the Correct Answer - they merely elaborate on it. The fundamental comparison between role-based and non-role-based responses remains consistent between both answers: the role-based response is more detailed, structured, and provides better analysis and recommendations. There are no critical pieces of information from the Correct Answer that are missing from the Generated Answer, and there are no contradictions between the two.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  18%|█▊        | 18/100 [01:59<10:41,  7.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it covers the key elements from the Correct Answer and expands on them appropriately. Both answers mention:\n",
      "\n",
      "1. Key quantitative metrics including F1 score, accuracy, and other relevant metrics\n",
      "2. The importance of using benchmarks and prior experiments to set targets\n",
      "\n",
      "While the Generated Answer provides more specific examples and detailed metrics (like response time and toxicity) that weren't in the Correct Answer, this additional detail doesn't contradict the Correct Answer - it merely elaborates on it. The Generated Answer maintains the core substance of what makes a good evaluation framework for sentiment analysis models.\n",
      "\n",
      "The Generated Answer also aligns with the Correct Answer's point about using benchmarks and prior experiments to determine targets, though it expresses this slightly differently.\n",
      "\n",
      "There are no critical omissions or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:XML parsing error: mismatched tag: line 9, column 182\n",
      "Evaluating End-to-End:  19%|█▉        | 19/100 [02:04<09:31,  7.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the key elements from the Correct Answer:\n",
      "1. The core concept of combining XML tags with other prompt engineering techniques\n",
      "2. Specifically mentions multishot prompting using <examples> tags\n",
      "3. Mentions chain of thought using <thinking> and <answer> tags\n",
      "4. Notes that this creates \"super-structured, high-performance prompts\"\n",
      "\n",
      "While the wording is slightly different, the substance and meaning are identical. There are no missing critical pieces of information and no contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  20%|██        | 20/100 [02:12<09:39,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the key elements from the Correct Answer:\n",
      "\n",
      "1. It explains that you need to provide both the output to grade and a detailed rubric to Claude\n",
      "2. It indicates that the LLM should evaluate based on the rubric criteria\n",
      "3. It mentions that the output should be a simple correct/incorrect judgment\n",
      "\n",
      "While the Generated Answer goes into more detail and provides additional implementation steps, it doesn't contradict anything in the Correct Answer. The core concept - using Claude to evaluate outputs against a rubric and provide a correct/incorrect judgment - is preserved. The additional detail simply elaborates on how to implement this approach, but doesn't change or omit any of the essential information from the Correct Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 20/100 questions. Current Accuracy: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  21%|██        | 21/100 [02:20<09:50,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it contains all the essential steps and information present in the Correct Answer. Both answers outline the same key process:\n",
      "1. Accessing/subscribing to the model on AWS Marketplace\n",
      "2. Selecting the model and agreeing to terms\n",
      "3. Getting the Product ARN for the region\n",
      "4. Using JupyterLab in SageMaker Studio\n",
      "5. Following notebook instructions to deploy using the ARN\n",
      "\n",
      "The Generated Answer actually provides slightly more detail in its step-by-step breakdown, but the core information and process remains the same. There are no contradictions between the two answers, and no critical information is missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  22%|██▏       | 22/100 [02:26<09:22,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it misses a key critical element from the Correct Answer. While both answers discuss using tools to generate JSON output, the Correct Answer specifically emphasizes setting the tool_choice parameter to explicitly instruct the model to use the tool, which is completely missing from the Generated Answer. Additionally, the Correct Answer mentions an important point about ensuring tool names and descriptions are from the model's perspective, which is also absent from the Generated Answer. The Generated Answer instead focuses on more general aspects like specifying schemas and testing, which, while not incorrect, don't capture the specific technical requirements outlined in the Correct Answer. These omissions represent significant differences in the technical implementation details that are crucial for proper tool usage with Claude.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  23%|██▎       | 23/100 [02:36<10:03,  7.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect for two key reasons:\n",
      "\n",
      "1. It directly contradicts the correct answer regarding speed. The correct answer states that Claude 3 Haiku is \"faster\" than Claude Instant 1.2, while the generated answer incorrectly states that \"Claude Instant 1.2 model is described as a 'fast and efficient' predecessor\" and implies it's faster than Claude 3 Haiku.\n",
      "\n",
      "2. It includes potentially incorrect or unverified information about costs and context windows that isn't mentioned in the correct answer, which could be misleading.\n",
      "\n",
      "The core attributes mentioned in the correct answer (vision capabilities, better performance, more up-to-date training data) are partially covered in the generated answer, but the critical error regarding speed/performance makes this answer incorrect overall. When there's a direct contradiction with the correct answer on a key point like performance speed, we must mark the generated answer as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  24%|██▍       | 24/100 [02:43<09:43,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers emphasize the same key point - that using examples helps reduce misinterpretation of instructions and leads to more accurate outputs from Claude. While the Generated Answer includes some additional details about improving consistency and handling complex tasks, the core benefit about reducing misinterpretation is present in both answers. There are no contradictions between the answers, and the Generated Answer includes all critical information from the Correct Answer, just with additional elaboration.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  25%|██▌       | 25/100 [02:50<09:14,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer focuses on different advantages (resource efficiency, cost, and speed) compared to the Correct Answer, which emphasizes the ability to adapt models through context in prompts without retraining. While both answers discuss advantages of prompt engineering over fine-tuning, they highlight completely different benefits. The Generated Answer misses the key point about providing domain-specific context in prompts being the main advantage, and instead discusses operational benefits that weren't mentioned in the Correct Answer. Since the Generated Answer fails to capture the core advantage specified in the Correct Answer (adaptation through contextual prompts), it should be considered incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  26%|██▌       | 26/100 [02:56<08:47,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it conveys the same core information as the Correct Answer - that users can get started quickly by making a copy of a provided Claude for Sheets template workbook. While the Generated Answer breaks this down into more detailed steps, the fundamental message is the same. There are no contradictions between the answers, and no critical information is missing from the Generated Answer compared to the Correct Answer. The additional detail in the Generated Answer doesn't change the essential correctness of the response.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  27%|██▋       | 27/100 [03:02<07:59,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it conveys the same essential meaning as the Correct Answer. Both answers explain that:\n",
      "\n",
      "1. The \"index\" field indicates which content block the text relates to\n",
      "2. The field is used to identify/track specific content blocks within the response\n",
      "3. The field is connected to the streaming of text content\n",
      "\n",
      "While the Generated Answer uses slightly different wording and adds some additional detail about the \"delta\" object, it maintains the core concept that the index field serves to identify which content block is being modified/streamed. There are no contradictions between the answers, and no critical information is missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  28%|██▊       | 28/100 [03:07<07:27,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. While it provides additional accurate details about image handling, it misses a critical piece of information from the Correct Answer: that images need to be provided as base64-encoded content blocks within the messages array. Instead, the Generated Answer incorrectly states that you can \"upload the image file directly to the API.\" This is a direct contradiction of the correct implementation method specified in the Correct Answer. While both answers correctly list the supported image formats (JPEG, PNG, GIF, and WebP), the method of including images in the API request is fundamentally different between the two answers.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  29%|██▉       | 29/100 [03:14<07:29,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same core concept that TTFT is a specific component of overall latency, measuring specifically the time to generate the first token of a response. The Generated Answer actually provides additional relevant context about factors affecting TTFT and latency, but this extra information doesn't contradict the Correct Answer - it merely elaborates on it. The key relationship between TTFT and latency is accurately captured in both answers, with both emphasizing that TTFT is a specific measure that contributes to overall latency. The Generated Answer maintains the same essential meaning as the Correct Answer, particularly regarding TTFT's role in measuring model responsiveness.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  30%|███       | 30/100 [03:21<07:37,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the core concept from the Correct Answer that providing edge case examples helps improve Claude's performance in routing support tickets. The Generated Answer actually expands on this concept by providing detailed explanations of how different types of edge cases (implicit requests, emotional prioritization, intent vs. routing, and issue prioritization) can improve ticket routing accuracy. While it provides more detail than the Correct Answer, it doesn't contradict it and maintains the same fundamental message about edge case examples improving Claude's routing performance. The key elements from the Correct Answer about improving performance in scenarios where misclassification might occur are present in the Generated Answer, just expressed in more detail.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 30/100 questions. Current Accuracy: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  31%|███       | 31/100 [03:30<08:41,  7.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures all the essential elements of the Correct Answer. Both answers explain that:\n",
      "\n",
      "1. The \"tool_use\" stop_reason indicates Claude has determined a tool is needed for the query\n",
      "2. This involves Claude constructing a tool use request\n",
      "3. The tool input needs to be extracted\n",
      "4. The tool code needs to be run client-side\n",
      "5. The results need to be sent back to Claude\n",
      "\n",
      "While the wording and structure differ slightly, the Generated Answer conveys the same core workflow and meaning as the Correct Answer. There are no missing critical pieces of information or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  32%|███▏      | 32/100 [03:41<09:32,  8.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It contains all the key elements from the Correct Answer:\n",
      "1. It correctly identifies the error event as \"overloaded_error\"\n",
      "2. It specifies this occurs during periods of high usage\n",
      "3. It correctly states that this corresponds to HTTP 529 error code in non-streaming contexts\n",
      "4. It properly contextualizes this within streaming responses\n",
      "\n",
      "The Generated Answer uses slightly different wording but conveys exactly the same information as the Correct Answer. There are no missing critical pieces of information and no contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  33%|███▎      | 33/100 [03:45<08:01,  7.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It identifies both types of deltas that can be contained in a content_block_delta event: text_delta and input_json_delta. While the formatting and presentation are slightly different (using a numbered list instead of prose), the substance and key information are exactly the same as the Correct Answer. Both answers convey the same two specific delta types without any omissions or contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  34%|███▍      | 34/100 [04:01<10:39,  9.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. According to the Correct Answer, Claude 3.5 Sonnet and tool use became generally available on different dates:\n",
      "- Claude 3.5 Sonnet: June 20th, 2024\n",
      "- Tool use: May 30th, 2024\n",
      "\n",
      "The Generated Answer incorrectly states that both became available on June 20th, 2024. This is a critical factual error as it misses the distinction between the two release dates. The timing difference of several weeks between these features' availability is an important detail that shouldn't be overlooked.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  35%|███▌      | 35/100 [04:08<09:41,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same essential information: that Anthropic launched Claude.ai and the Claude iOS app in Europe first (in May 2024) and then in Canada (in June 2024). The Generated Answer provides specific dates (May 13th and June 5th) while the Correct Answer uses more general timing (May and June), but this level of detail doesn't change the fundamental accuracy of the sequence of events. Both answers maintain the same chronological order and cover the same key information about the launches in both regions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  36%|███▌      | 36/100 [04:14<08:43,  8.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures all the essential elements from the Correct Answer:\n",
      "\n",
      "1. It correctly explains that a \"tool_use\" stop_reason indicates Claude has decided to use a tool\n",
      "2. It outlines the exact same steps that need to be taken:\n",
      "   - Extracting the tool name and input from Claude's request\n",
      "   - Executing the tool code on the client side\n",
      "   - Sending back a new message with a tool_result content block\n",
      "\n",
      "While the wording and structure differ slightly, the substance and key information are identical. The Generated Answer even provides additional context about why this process is useful (to help Claude formulate a final response), but this extra information doesn't contradict anything in the Correct Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  37%|███▋      | 37/100 [04:19<07:19,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it conveys the same essential information as the Correct Answer. Both answers indicate that the anthropic library is used to interact with Claude/Anthropic's AI capabilities. While the Generated Answer provides slightly more detail by explaining what the anthropic library does, the core substance - that the anthropic library is the Python library used in the example - is consistent between both answers. There are no contradictions or missing critical pieces of information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  38%|███▊      | 38/100 [04:25<07:09,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures both main authentication methods described in the Correct Answer:\n",
      "\n",
      "1. Direct provision of AWS credentials (access key, secret key, and optional session token)\n",
      "2. Using default AWS credential providers (including both the ~/.aws/credentials file and environment variables)\n",
      "\n",
      "The Generated Answer conveys the same essential information as the Correct Answer, just with slightly different wording. There are no missing critical pieces of information and no contradictions between the two answers. The substance and meaning are equivalent.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  39%|███▉      | 39/100 [04:31<06:41,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the same two key factors mentioned in the Correct Answer:\n",
      "\n",
      "1. The risk/potential of prompt leaks (protecting sensitive information)\n",
      "2. The impact on model performance due to added complexity\n",
      "\n",
      "While the Generated Answer elaborates more on each factor with additional examples and details, the core substance and trade-off described is identical to the Correct Answer. Both answers emphasize the need to balance protecting against leaks with maintaining model performance. There are no contradictions between the two answers, and no critical information from the Correct Answer is missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  40%|████      | 40/100 [04:37<06:22,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it conveys the same core message as the Correct Answer. Both answers emphasize that:\n",
      "\n",
      "1. Anthropic offers different Claude models with varying capabilities and performance characteristics\n",
      "2. Selecting the right model allows users to optimize for their specific needs\n",
      "3. The choice helps balance factors like speed, intelligence, and effectiveness\n",
      "\n",
      "While the Generated Answer provides more specific details about different models and use cases, these additional details don't contradict the Correct Answer - they merely elaborate on it. The fundamental point about choosing the appropriate model to reduce latency while maintaining needed capabilities is preserved in both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 40/100 questions. Current Accuracy: 0.6750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  41%|████      | 41/100 [04:44<06:21,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It contains all the essential information from the Correct Answer and even provides more detailed implementation examples. Both answers highlight the key points that:\n",
      "\n",
      "1. You use the client.messages.stream() method\n",
      "2. You iterate over the stream.text_stream attribute\n",
      "\n",
      "The Generated Answer expands on this with a practical code example and additional context, but the core information matches the Correct Answer completely. There are no contradictions or missing critical pieces between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  42%|████▏     | 42/100 [04:53<07:07,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same key information:\n",
      "\n",
      "1. They both explain that you can guide/shape Claude's response by pre-filling/including text in the response (though they describe the technical implementation slightly differently)\n",
      "\n",
      "2. They both specifically mention that the \"max_tokens\" parameter is used to generate short responses\n",
      "\n",
      "While the exact wording differs, the substance of both answers is the same. There are no critical missing pieces of information and no contradictions between the answers. The Generated Answer accurately captures the core concepts about pre-filling responses and using max_tokens to limit response length.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  43%|████▎     | 43/100 [04:59<06:42,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers make the same core assertion that having a larger volume of automated test cases is more important than having fewer human-graded test cases when building an eval set. The Generated Answer expands on this with additional context and explanation, but the fundamental point being made is identical to the Correct Answer. There are no contradictions between the two answers, and no critical pieces of information from the Correct Answer are missing from the Generated Answer. While the Generated Answer is more verbose, the substance and main conclusion are the same.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  44%|████▍     | 44/100 [05:04<05:58,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. According to the Correct Answer, the two required fields are \"index\" and \"delta\" (where \"delta\" contains the type and text), but the Generated Answer incorrectly states that the required fields are \"type\" and \"text\". This is a substantive difference in the structure of the required fields, not just a minor wording variation. The Generated Answer misses the critical \"index\" field requirement and incorrectly elevates \"type\" and \"text\" (which are actually nested within the \"delta\" field) to top-level required fields.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  45%|████▌     | 45/100 [05:09<05:15,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect. While it correctly mentions the Anthropic Cookbook as one interactive way to learn Claude's capabilities, it fails to mention the Developer Console and its prompt generator tool, which is a key component mentioned in the correct answer. Instead, it references the \"More Resources\" section and documentation, which weren't identified in the correct answer as interactive learning methods. The generated answer therefore misses one of the two main interactive learning tools specified in the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  46%|████▌     | 46/100 [05:14<05:04,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. The core concept from the Correct Answer - that breaking tasks into subtasks improves accuracy because each subtask gets Claude's full attention and reduces errors compared to handling everything at once - is fully captured in the Generated Answer's first point about accuracy. While the Generated Answer goes on to provide additional points about clarity and traceability, these are supplementary details that don't contradict the core concept. The essential reasoning about improved accuracy through focused attention on subtasks is present and aligned between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  47%|████▋     | 47/100 [05:19<04:54,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. The key point from the Correct Answer - that Messages streaming responses can contain multiple content blocks of varying types, making it more complex than Text Completions streaming - is accurately captured in the Generated Answer's first point. While the Generated Answer provides additional details about the differences between the two formats, these extra details don't contradict the core concept presented in the Correct Answer. The Generated Answer effectively communicates the main distinction between the two streaming formats, which is the ability to handle multiple content blocks of different types in Messages responses versus the simpler format of Text Completions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  48%|████▊     | 48/100 [05:24<04:41,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. The Correct Answer states that users can experiment with Claude through claude.ai and Anthropic's web Console, while the Generated Answer mentions completely different methods - using the API Quickstart and Workbench. These are substantively different approaches and do not align with what's stated in the Correct Answer. The Generated Answer is not just using different wording to describe the same things, but is actually describing entirely different methods of accessing Claude. Therefore, this represents a material difference in substance, not just presentation.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  49%|████▉     | 49/100 [05:30<04:32,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct and actually provides more detailed elaboration on the core concept presented in the Correct Answer. Both answers share the same fundamental point: that chain prompts help reduce errors and inconsistencies by breaking complex tasks into smaller, more manageable subtasks that Claude can focus on individually. The Generated Answer expands on this with additional benefits and mechanisms, but does not contradict or omit any critical information from the Correct Answer. The additional detail provided (like traceability and mitigation of hallucinations) simply builds upon the core concept rather than changing or contradicting it.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  50%|█████     | 50/100 [05:34<04:10,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers state that an overloaded_error event corresponds to HTTP status code 529 in a non-streaming context for the Anthropic API. While the Correct Answer uses slightly more formal language (\"would normally correspond to\"), the core information - the 529 status code - is identical in both answers. The difference in phrasing does not change the fundamental meaning or accuracy of the response.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 50/100 questions. Current Accuracy: 0.6800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  51%|█████     | 51/100 [05:39<04:05,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the same two key ways to specify the embedding format as described in the Correct Answer:\n",
      "\n",
      "1. Both answers indicate that leaving the format unspecified will return embeddings as lists of floating-point numbers\n",
      "2. Both answers state that setting the format to \"base64\" will return the embeddings in Base64 encoded format\n",
      "\n",
      "The Generated Answer presents the same information in a slightly more structured format with bullet points, but the substance and technical details are identical to the Correct Answer. There are no missing critical pieces of information or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  52%|█████▏    | 52/100 [05:44<04:07,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it conveys the same essential information as the Correct Answer. Both answers explain that:\n",
      "\n",
      "1. Tool use content blocks are sent as partial JSON strings in content_block_delta events\n",
      "2. The client needs to accumulate these partial JSON strings\n",
      "3. The complete JSON can be parsed once a content_block_stop event is received\n",
      "4. Parsing can be done using Pydantic or SDK helpers\n",
      "\n",
      "The Generated Answer actually provides additional helpful detail by showing an example of the delta structure, but this doesn't contradict anything in the Correct Answer. The core concepts and process are described accurately and consistently between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  53%|█████▎    | 53/100 [05:48<03:47,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It accurately identifies both tutorials (GitHub and Google Sheets) and correctly characterizes their key differences. The Generated Answer maintains that the GitHub tutorial is more comprehensive and example-driven, while the Google Sheets version is lighter-weight, which aligns with the Correct Answer. The Generated Answer also correctly notes that both tutorials cover prompt engineering concepts. While the wording differs slightly, the substance and key information are consistent between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  54%|█████▍    | 54/100 [05:55<04:13,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct and actually provides more comprehensive detail than the Correct Answer. It covers all the key points mentioned in the Correct Answer:\n",
      "\n",
      "1. The 200K token context window\n",
      "2. Tool use capabilities for integration with specialized applications\n",
      "3. Multimodal input capabilities\n",
      "4. Enterprise-grade security and data handling for sensitive information\n",
      "\n",
      "The Generated Answer expands on these points and provides additional relevant details about Claude's enterprise capabilities, but does not contradict any information in the Correct Answer. While it is more detailed, the core capabilities mentioned in the Correct Answer are all present and accurately represented in the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  55%|█████▌    | 55/100 [05:59<03:36,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it omits a critical piece of information. While it correctly states that Claude.ai API and iOS app are available in Canada and Europe, it fails to mention the United States as one of the available regions. The Correct Answer clearly states that the services are available in \"the United States, Canada, and Europe.\" The omission of the United States represents a significant missing piece of information about the service's availability.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  56%|█████▌    | 56/100 [06:04<03:37,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures all the key points from the Correct Answer:\n",
      "\n",
      "1. It correctly identifies the two main approaches (push-based with webhooks and pull-based)\n",
      "2. It accurately describes that the push-based approach is more scalable\n",
      "3. It mentions the security implications of exposing a public endpoint for the push-based approach\n",
      "4. It correctly states that the pull-based approach is easier to implement but has efficiency drawbacks due to unnecessary system calls\n",
      "\n",
      "The Generated Answer actually provides more detail and context than the Correct Answer, but crucially doesn't contradict any points in the Correct Answer. The substance and main points are completely aligned between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  57%|█████▋    | 57/100 [06:08<03:22,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it omits a critical piece of information - the release date (May 10th, 2024). While the Generated Answer correctly states that the tool is available through the Developer Console interface, the timing of the release is an important factual detail that was included in the Correct Answer but missing from the Generated Answer. When dealing with product releases and announcements, the timing is typically considered crucial information.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  58%|█████▊    | 58/100 [06:15<03:45,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it directly contradicts the Correct Answer. The Correct Answer specifically states that Claude 3 Sonnet provides the best balance of intelligence and speed for high-throughput tasks, while the Generated Answer claims that Claude 3 Haiku is the best model for these tasks. This is a fundamental contradiction in terms of which model is most appropriate for the specified use case. While both answers discuss speed and intelligence tradeoffs, they come to opposite conclusions about which model strikes the optimal balance for tasks like sales forecasting and targeted marketing.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  59%|█████▉    | 59/100 [06:21<03:42,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It conveys the same core information as the Correct Answer - that similarity between Voyage embedding vectors can be calculated using either dot product or cosine similarity, and these are equivalent because the vectors are normalized to length 1. In fact, the Generated Answer goes into more helpful detail explaining why this is the case mathematically, but the fundamental point being made is identical to the Correct Answer. There are no contradictions between the two answers, and no critical information is missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  60%|██████    | 60/100 [06:26<03:38,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures all the key points from the Correct Answer and even expands on them in a helpful way. Both answers emphasize that examples can:\n",
      "1. Reduce misinterpretation of instructions\n",
      "2. Help enforce consistent structure and style\n",
      "3. Guide Claude toward desired output/performance\n",
      "\n",
      "The Generated Answer provides additional details and examples, but these don't contradict the Correct Answer - they simply elaborate on the core concepts. The substance of both answers is fundamentally the same, even though the Generated Answer is more detailed. There are no critical omissions or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 60/100 questions. Current Accuracy: 0.6833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  61%|██████    | 61/100 [06:32<03:41,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It accurately identifies and describes both types of content block deltas:\n",
      "\n",
      "1. It correctly identifies \"Input JSON delta\" and explains that it contains partial JSON strings for tool input, which aligns with the Correct Answer's description of deltas containing a \"partial_json\" field with parts of the JSON object for tool input.\n",
      "\n",
      "2. It correctly identifies \"Text delta\" and explains that it contains text updates, which matches the Correct Answer's description of text deltas containing text strings.\n",
      "\n",
      "While the exact wording differs between the two answers, the substance and key information about both types of deltas are effectively the same. The Generated Answer even provides some additional context about how the deltas work (like mentioning content_block_stop events), but this extra information doesn't contradict the Correct Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  62%|██████▏   | 62/100 [06:38<03:31,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it focuses on different capabilities than those specified in the Correct Answer. The Correct Answer specifically highlights \"question answering\" and \"text analysis\" capabilities, with emphasis on understanding sentiment and preferences. While the Generated Answer discusses valid capabilities of Claude (text/code generation and multimodal input/tool use), these are not the same capabilities mentioned in the Correct Answer as being key to building interactive systems and personalized experiences. The Generated Answer misses the core capabilities of question answering and text analysis/understanding that are central to the Correct Answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  63%|██████▎   | 63/100 [06:43<03:21,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures all the key elements from the Correct Answer and presents them in essentially the same order:\n",
      "\n",
      "1. Both answers mention the message_start event coming first\n",
      "2. Both describe the content block sequence (start, delta, stop)\n",
      "3. Both mention message_delta events\n",
      "4. Both mention the message_stop event as the final event\n",
      "5. Both note that ping events can occur throughout\n",
      "\n",
      "The Generated Answer breaks down the information in a more structured numbered list format, but the substance and sequence of events is identical to the Correct Answer. There are no contradictions or missing critical pieces of information between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  64%|██████▍   | 64/100 [06:48<03:13,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the key information from the Correct Answer - specifically that the Anthropic API allows up to 20 images per request while claude.ai has a limit of 5 images per turn. While the Generated Answer is more concise and uses slightly different wording, it communicates the same essential information and numerical limits. There are no contradictions or missing critical details between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  65%|██████▌   | 65/100 [06:54<03:10,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. The two answers provide fundamentally different solutions to the problem:\n",
      "\n",
      "1. Correct Answer: Simply increase the max_tokens parameter to get the complete response from Claude, including the full tool use block.\n",
      "\n",
      "2. Generated Answer: Extract partial information from the incomplete tool use, execute the tool client-side, and continue the conversation with the tool results.\n",
      "\n",
      "These are contradictory approaches. The Correct Answer provides a straightforward solution of just increasing max_tokens, while the Generated Answer suggests a more complex workaround that involves parsing incomplete tool use blocks and executing tools client-side, which is not the recommended approach according to the documentation.\n",
      "\n",
      "The Generated Answer misses the key point that the simple solution is to increase max_tokens, and instead proposes a different and more complicated approach that could potentially lead to errors or unexpected behavior.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  66%|██████▌   | 66/100 [06:57<02:48,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. While both answers mention \"develop your test cases\" as one of the steps, they differ on the second step. The Correct Answer states that you need to \"take a look at Anthropic's guide to developing test cases\" while the Generated Answer states you need to \"build a strong input prompt.\" These are substantively different steps. The Generated Answer misses the critical requirement of consulting Anthropic's guide, which is explicitly mentioned in the Correct Answer, and instead introduces a different step that isn't mentioned in the Correct Answer. This represents a meaningful difference in the preparation process, not just a difference in wording.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  67%|██████▋   | 67/100 [07:02<02:37,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the key point that you can influence Claude's response by including an \"assistant\" message at the end of the messages list to pre-fill or shape the response. While the Generated Answer provides some additional context about conversation simulation and system prompts, the core concept about using the content parameter with an assistant role to influence Claude's output matches the Correct Answer. There are no contradictions between the two answers, and no critical information from the Correct Answer is missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  68%|██████▊   | 68/100 [07:08<02:44,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures both key advantages mentioned in the Correct Answer:\n",
      "\n",
      "1. It accurately conveys that prompt engineering is more effective at helping models understand and utilize external content/retrieved documents.\n",
      "\n",
      "2. It correctly explains that prompt engineering preserves the model's general knowledge/capabilities while fine-tuning risks catastrophic forgetting.\n",
      "\n",
      "The Generated Answer even uses very similar phrasing and provides the same substantive information, just structured slightly differently. While it adds citations to \"the documents,\" this doesn't change the core meaning and actually reinforces the points being made. There are no missing critical pieces of information and no contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  69%|██████▉   | 69/100 [07:13<02:45,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. While it describes some technical aspects of using the API, it misses one of the key initial setup steps specified in the Correct Answer - installing and configuring the AWS CLI. The Generated Answer jumps straight into authentication and client creation details, but skips over the fundamental prerequisite of having the AWS CLI installed and configured. Additionally, the Correct Answer mentions the need to install an SDK for accessing Bedrock, which is also not explicitly mentioned in the Generated Answer. These are important initial setup steps that are materially different from just providing authentication credentials.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:XML parsing error: mismatched tag: line 3, column 615\n",
      "Evaluating End-to-End:  70%|███████   | 70/100 [07:18<02:34,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is completely correct. It provides exactly the same AWS CLI command as the Correct Answer (`aws bedrock list-foundation-models --region=<region> --by-provider anthropic --query \"modelSummaries[*].modelId\"`), explains that you need to replace `<region>` with your desired AWS region (giving the same example of `us-west-2`), and correctly states that this will list the available Claude models in that region. The substance and technical details are identical between both answers, with only minor differences in wording that don't affect the accuracy of the information provided.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 70/100 questions. Current Accuracy: 0.6571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  71%|███████   | 71/100 [07:24<02:32,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same core information - that the `input_type` argument can be passed to specify whether the input is a \"query\" or \"document\". The Generated Answer actually provides additional context about how query inputs are handled, but this extra detail doesn't contradict or detract from the core correct information. The essential substance of both answers is identical - the use of the `input_type` parameter with its two possible values.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  72%|███████▏  | 72/100 [07:29<02:23,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is missing a critical piece of information that is present in the Correct Answer. While it correctly describes the basic difference in delta formats between tool_use (partial JSON strings) and text content blocks (direct text deltas), it fails to mention that tool_use deltas may have delays between streaming events as the model emits one complete key-value pair at a time. This timing/delay characteristic is an important distinction mentioned in the Correct Answer that is completely absent from the Generated Answer. Since this represents a meaningful omission of a key difference between the two formats, the Generated Answer cannot be considered fully correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  73%|███████▎  | 73/100 [07:32<02:07,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It contains the exact same key information as the Correct Answer - specifically that the API has a 5MB per image limit while claude.ai has a 10MB per image limit. The Generated Answer simply presents this information in a slightly different format (bullet points) and adds a minor additional detail about error messages, but the core substance regarding the file size limits is identical to the Correct Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  74%|███████▍  | 74/100 [07:40<02:22,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it conveys the same core concept as the Correct Answer - the need to balance performance characteristics when selecting a Claude model for low-latency use cases. While the Generated Answer provides more specific examples (mentioning Haiku, Sonnet, and Opus models), and goes into more detail about model sizes, the fundamental message about choosing an appropriate model that balances speed and capabilities matches the substance of the Correct Answer. Both answers emphasize the importance of evaluating and selecting the right model based on specific use case requirements. There are no contradictions between the two answers, and no critical information from the Correct Answer is missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  75%|███████▌  | 75/100 [07:45<02:15,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it contains all the key information from the Correct Answer:\n",
      "1. It correctly identifies the recommended model as \"voyage-code-2\"\n",
      "2. It mentions the 17% better performance compared to alternatives\n",
      "3. It notes the state-of-the-art performance on general-purpose corpora\n",
      "4. The overall meaning and substance is identical, with only minor differences in wording and sentence structure\n",
      "\n",
      "There are no critical omissions or contradictions between the two answers. The only difference is in how the information is phrased, but the core facts and claims remain the same.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  76%|███████▌  | 76/100 [07:51<02:12,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. While it provides more detail than the Correct Answer, it captures the key points about the Cookbook providing interactive Jupyter notebooks for working with PDFs and embeddings. The second point about examples and tutorials is an extension of the same core functionality described in the Correct Answer, not a contradiction. The Generated Answer effectively communicates the same substance as the Correct Answer while providing additional context. The essential elements about Jupyter notebooks and working with PDFs/embeddings are present in both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  77%|███████▋  | 77/100 [07:57<02:13,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures the key relationship between context window size and RAG effectiveness described in the Correct Answer. Both answers emphasize that:\n",
      "\n",
      "1. A larger context window allows the model to work with more retrieved information\n",
      "2. This capability impacts how well the model can utilize the retrieved information\n",
      "3. The size of the context window affects the quality of the generated response\n",
      "\n",
      "While the Generated Answer provides additional details about coherence and knowledge base quality that aren't in the Correct Answer, it doesn't contradict the core concept. The fundamental point about context window size determining how much retrieved information can be utilized is present in both answers, just expressed differently.\n",
      "\n",
      "There are no critical omissions or contradictions between the two answers - they're expressing the same essential concept about the relationship between context window size and RAG effectiveness.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  78%|███████▊  | 78/100 [08:03<02:10,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It covers all the key points from the Correct Answer and even expands on them in a clearer, more structured way. Both answers emphasize:\n",
      "1. Identifying edge cases where prompts might not perform well\n",
      "2. Rating individual results to assess prompt performance\n",
      "3. Ensuring consistent performance across different inputs\n",
      "4. Using insights to refine and improve prompts\n",
      "5. Spotting patterns to make informed adjustments\n",
      "\n",
      "The Generated Answer breaks these points down more explicitly, but the core substance is identical to the Correct Answer. There are no contradictions or missing critical pieces of information between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  79%|███████▉  | 79/100 [08:08<01:54,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers identify Claude 3 Haiku as having the fastest comparative latency. The Generated Answer provides additional context and detail about why Haiku is the fastest, but the core claim matches exactly with the Correct Answer. There are no contradictions between the two answers, and no critical information is missing from the Generated Answer compared to the Correct Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  80%|████████  | 80/100 [08:15<01:59,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the key concept from the Correct Answer that you need to send the full conversation history with each request since the API is stateless. The Generated Answer actually goes into more helpful detail by providing a concrete code example showing how to maintain and pass the conversation history through multiple turns. While it includes additional implementation details, it doesn't contradict or miss any critical information from the Correct Answer. Both answers emphasize the core concept of maintaining and passing the full conversation history with each API call.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 80/100 questions. Current Accuracy: 0.6875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  81%|████████  | 81/100 [08:23<02:07,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures the core message of the Correct Answer. Both answers emphasize that using XML tags with a specific role (like General Counsel) helps Claude identify critical legal issues and risks in contracts that might otherwise be missed. While the Generated Answer provides more detail and additional benefits, it doesn't contradict the Correct Answer, and it maintains the key point about improved analysis leading to better risk identification. The mention of \"saving the company millions of dollars\" from the Correct Answer is implicit in the Generated Answer's discussion of identifying critical risks and issues. The additional detail in the Generated Answer simply expands on the core concept without changing its fundamental meaning.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  82%|████████▏ | 82/100 [08:28<01:49,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer captures the core distinction between how the two models handle missing information in tool calls, though it uses slightly different wording. Both answers convey that Opus is more likely to ask for clarification/missing information, while Sonnet is more likely to make inferences/assumptions to fill in missing parameters. While the Generated Answer includes some additional context about capabilities that isn't in the Correct Answer, this doesn't contradict or detract from the key point about how they handle missing information differently. The substance of the distinction is preserved.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  83%|████████▎ | 83/100 [08:35<01:46,  6.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it covers all the key points mentioned in the Correct Answer and even provides additional helpful detail. Both answers emphasize:\n",
      "\n",
      "1. Implementing retry logic for error handling\n",
      "2. Conducting thorough staging/testing\n",
      "3. Load testing\n",
      "4. Error handling and logging setup\n",
      "5. Gradual rollout process\n",
      "6. Documentation and training\n",
      "7. Monitoring and alerting\n",
      "\n",
      "The Generated Answer expands on these points with more specific implementation details, but the core recommendations align perfectly with the Correct Answer. There are no contradictions between the two answers, and no critical pieces of information from the Correct Answer are missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  84%|████████▍ | 84/100 [08:40<01:33,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer includes all three key elements from the correct answer (accuracy, cost, and speed) and expands on them with additional details. While the generated answer is more comprehensive and includes additional metrics like F1 score, consistency, structure, and bias considerations, it doesn't contradict the core elements specified in the correct answer. The essential components from the correct answer are present in the generated answer, just with more elaboration. Since the generated answer captures all the required elements without contradicting them, it should be considered correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  85%|████████▌ | 85/100 [08:46<01:32,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer and Correct Answer are essentially referring to the same two methods, though using slightly different wording. Both mention the interactive tutorial and a Google/Claude for Sheets tutorial/workbench. While the specific terminology differs slightly, the core substance of both answers points to the same two recommended learning methods - an interactive tutorial and a spreadsheet-based tutorial. The minor differences in phrasing don't change the fundamental meaning or recommendations being conveyed.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  86%|████████▌ | 86/100 [08:54<01:31,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is partially correct but contains some unsupported claims that go beyond the correct answer. The first two points about training process and capabilities align well with the correct answer - both mention the distinction between pretrained LLMs (trained on raw text) and Claude's additional RLHF training, as well as the resulting difference in capabilities for tasks like following instructions. However, points 3 and 4 about adaptability and interpretability make claims that aren't supported by the correct answer. While these additional points don't directly contradict the correct answer, they represent speculation beyond what's established in the correct answer. Since we're asked to evaluate based on alignment with the correct answer, and these additional claims aren't verified there, we should mark this as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  87%|████████▋ | 87/100 [09:02<01:29,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct and actually provides a more detailed expansion of the key points mentioned in the Correct Answer. It covers all the main advantages mentioned in the Correct Answer:\n",
      "\n",
      "1. Cost and resource efficiency (points 1 and 2)\n",
      "2. Speed and time efficiency (point 4)\n",
      "3. Less data requirements (point 5)\n",
      "4. Flexibility and rapid iteration (point 6)\n",
      "5. Preservation of general knowledge (point 9)\n",
      "6. Transparency (point 10)\n",
      "\n",
      "The Generated Answer elaborates on these points and provides additional context and examples, but doesn't contradict any information in the Correct Answer. While it's more verbose, the substance aligns completely with the Correct Answer and covers all the critical points mentioned in it. The additional details serve to reinforce and explain the core advantages rather than alter them.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  88%|████████▊ | 88/100 [09:06<01:13,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same core information - that you need to run the command `gcloud auth application-default login` to authenticate with GCP before accessing Claude models on Vertex AI. The Generated Answer simply provides slightly more context by explaining that this authenticates your local environment, but the fundamental instruction and requirement is identical. There are no missing critical pieces of information or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  89%|████████▉ | 89/100 [09:11<01:04,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer captures the core information about the Prompt Generator tool being introduced on May 10th, 2024, and its main purpose of helping users create tailored prompts for specific tasks. While the Correct Answer provides additional context about the Claude iOS app and Claude Team plan, these are supplementary details that don't affect the central claim about the Prompt Generator's capabilities. The Generated Answer accurately conveys the key functionality - that it helps users create high-quality prompts customized to their specific needs. The essence of both answers is aligned, even if the Generated Answer is more concise.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  90%|█████████ | 90/100 [09:15<00:52,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It conveys exactly the same information as the Correct Answer - that both Claude 3.5 Sonnet and the Artifacts feature became available on June 20th, 2024. While the wording is slightly different (omitting \"both\" and having a slightly different sentence structure), the core information and meaning are identical. There are no missing critical details or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 90/100 questions. Current Accuracy: 0.7111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  91%|█████████ | 91/100 [09:20<00:45,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same key information - that to limit Claude's response to a single token, you should use \"max_tokens\": 1 in the request. The Generated Answer uses slightly different wording by referring to it as a \"header\" rather than just a parameter in the request, but this minor difference doesn't change the core technical instruction being conveyed. Both answers specify the exact same value (1) and the same purpose (limiting to a single token).</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  92%|█████████▏| 92/100 [09:26<00:42,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same core concept that temperature controls randomness in the model's output generation. The Generated Answer simply provides more detail and elaboration about what higher and lower temperatures do specifically, but the fundamental meaning matches the Correct Answer. There are no contradictions between the two answers, and the Generated Answer includes all critical information from the Correct Answer while expanding on it.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  93%|█████████▎| 93/100 [09:33<00:40,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is partially correct but misses a key detail from the Correct Answer. While it correctly identifies that parameters can be added as additional arguments after the prompt and model, it does not specifically mention the exact format shown in the Correct Answer (e.g., \"max_tokens\", 3). Additionally, while it mentions that an API key can be passed, it doesn't explicitly state this as one of the two main ways to specify parameters, which the Correct Answer does. The Generated Answer's first point about \"Simple prompt\" is not actually one of the two ways to specify parameters mentioned in the Correct Answer. Due to these discrepancies and missing specifics, the Generated Answer cannot be considered fully correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  94%|█████████▍| 94/100 [09:37<00:32,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same key points:\n",
      "1. Prefilling with { makes Claude skip the preamble/explanation\n",
      "2. It causes Claude to output directly as JSON\n",
      "3. The result is more concise\n",
      "4. The output is easier for programs to parse\n",
      "\n",
      "The Generated Answer captures all the essential information from the Correct Answer, just using slightly different wording. There are no contradictions or missing critical pieces of information. The substance and meaning are effectively identical.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  95%|█████████▌| 95/100 [09:45<00:30,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is partially correct but contains additional information that is not verified by the Correct Answer. The Generated Answer correctly mentions the two key resources that are specified in the Correct Answer: the multimodal cookbook with tips and best practices, and the API reference documentation with example API calls. However, the Generated Answer also includes a third resource - the developer community - which is not mentioned in the Correct Answer and cannot be verified as accurate. Since we are instructed to mark answers as incorrect if they contain information that contradicts or goes beyond what is established in the Correct Answer, this addition makes the Generated Answer incorrect, even though its core components align with the Correct Answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  96%|█████████▌| 96/100 [09:54<00:28,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect for several reasons:\n",
      "\n",
      "1. While it correctly shows that you can specify the API key either through environment variables or directly when creating the client, the TypeScript example is completely wrong as it shows OpenAI's API configuration rather than Anthropic's SDK.\n",
      "\n",
      "2. The TypeScript example should be using Anthropic's client library, not OpenAI's Configuration and OpenAIApi classes.\n",
      "\n",
      "The core concept that the API key can be specified either through environment variables or as a parameter when creating the client is correct and matches the correct answer. However, the significant error in the TypeScript implementation makes this answer incorrect overall, as it provides misleading information that could cause implementation errors.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  97%|█████████▋| 97/100 [10:00<00:20,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the same two key benefits mentioned in the Correct Answer:\n",
      "\n",
      "1. Both answers mention identifying edge cases where prompts might fail/falter\n",
      "2. Both answers discuss ensuring consistent performance across different inputs/test cases\n",
      "\n",
      "The Generated Answer breaks these points down in a slightly different format but conveys the same core information. The substance and meaning are equivalent, with both answers focusing on how the tool helps identify problems and ensure reliability across different scenarios. There are no critical omissions or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  98%|█████████▊| 98/100 [10:07<00:13,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is correct in substance. Both answers emphasize the key distinction that the pretrained model lacks the ability to be helpful and follow instructions effectively, while the final Claude model available through the API has been enhanced through fine-tuning and RLHF to be more capable and aligned with human needs. While the generated answer goes into more detail about specific aspects like prompt engineering, cost, and latency, these additional details don't contradict the core message of the correct answer. The fundamental transformation from a basic language model to a helpful AI assistant through fine-tuning and RLHF is captured in both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  99%|█████████▉| 99/100 [10:12<00:06,  6.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is exactly identical to the Correct Answer, stating that Anthropic's IPv6 address range is 2607:6bc0::/48. There are no differences in wording or substance, and all critical information is present.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End: 100%|██████████| 100/100 [10:17<00:00,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It describes the same two methods for specifying the API key as mentioned in the Correct Answer:\n",
      "\n",
      "1. Passing the API key directly when initializing the Anthropic client\n",
      "2. Setting it as an environment variable named ANTHROPIC_API_KEY\n",
      "\n",
      "The Generated Answer even provides helpful code examples to illustrate both methods, though these weren't required to match the Correct Answer. The substance and key information is identical between both answers, just expressed in slightly different words.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 100/100 questions. Current Accuracy: 0.7100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'evaluation/csvs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:19\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/code/learn-anthropic/.venv/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    332\u001b[0m     )\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/code/learn-anthropic/.venv/lib/python3.12/site-packages/pandas/core/generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3965\u001b[0m )\n\u001b[0;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/code/learn-anthropic/.venv/lib/python3.12/site-packages/pandas/io/formats/format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1013\u001b[0m )\n\u001b[0;32m-> 1014\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/Desktop/code/learn-anthropic/.venv/lib/python3.12/site-packages/pandas/io/formats/csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    268\u001b[0m     )\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[0;32m~/Desktop/code/learn-anthropic/.venv/lib/python3.12/site-packages/pandas/io/common.py:749\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[0;32m--> 749\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/code/learn-anthropic/.venv/lib/python3.12/site-packages/pandas/io/common.py:616\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    614\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'evaluation/csvs'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Run evaluation on eval data\n",
    "avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs = evaluate_retrieval(retrieve_base, eval_data, db)\n",
    "\n",
    "# Run end-to-end evaluation\n",
    "e2e_accuracy, e2e_results = evaluate_end_to_end(answer_query_base, db, eval_data)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'question': [item['question'] for item in eval_data],\n",
    "    'retrieval_precision': precisions,\n",
    "    'retrieval_recall': recalls,\n",
    "    'retrieval_mrr': mrrs,\n",
    "    'e2e_correct': e2e_results\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('evaluation/csvs/evaluation_results_detailed.csv', index=False)\n",
    "print(\"Detailed results saved to evaluation/csvs/evaluation_results_one.csv\")\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "print(f\"Average F1: {f1:.4f}\")\n",
    "print(f\"End-to-End Accuracy: {e2e_accuracy:.4f}\")\n",
    "\n",
    "# Save the results to a file\n",
    "with open('evaluation/json_results/evaluation_results_one.json', 'w') as f:\n",
    "    json.dump({\n",
    "        \"name\": \"Basic RAG\",\n",
    "        \"average_precision\": avg_precision,\n",
    "        \"average_recall\": avg_recall,\n",
    "        \"average_f1\": f1,\n",
    "        \"average_mrr\": avg_mrr,\n",
    "        \"end_to_end_accuracy\": e2e_accuracy\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"Evaluation complete. Results saved to evaluation_results_one.json, evaluation_results_one.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed results saved to evaluation/csvs/evaluation_results_one.csv\n",
      "Average Precision: 0.4283\n",
      "Average Recall: 0.6592\n",
      "Average MRR: 0.7333\n",
      "Average F1: 0.5193\n",
      "End-to-End Accuracy: 0.7100\n",
      "Evaluation complete. Results saved to evaluation_results_one.json, evaluation_results_one.csv\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'question': [item['question'] for item in eval_data],\n",
    "    'retrieval_precision': precisions,\n",
    "    'retrieval_recall': recalls,\n",
    "    'retrieval_mrr': mrrs,\n",
    "    'e2e_correct': e2e_results\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('evaluation/csvs/evaluation_results_detailed.csv', index=False)\n",
    "print(\"Detailed results saved to evaluation/csvs/evaluation_results_one.csv\")\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "print(f\"Average F1: {f1:.4f}\")\n",
    "print(f\"End-to-End Accuracy: {e2e_accuracy:.4f}\")\n",
    "\n",
    "# Save the results to a file\n",
    "with open('evaluation/json_results/evaluation_results_one.json', 'w') as f:\n",
    "    json.dump({\n",
    "        \"name\": \"Basic RAG\",\n",
    "        \"average_precision\": avg_precision,\n",
    "        \"average_recall\": avg_recall,\n",
    "        \"average_f1\": f1,\n",
    "        \"average_mrr\": avg_mrr,\n",
    "        \"end_to_end_accuracy\": e2e_accuracy\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"Evaluation complete. Results saved to evaluation_results_one.json, evaluation_results_one.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_performance('evaluation/json_results', ['Basic RAG'], colors=['skyblue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2: Document summarization for enhanced retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets include document summaries: \n",
    "- instead of **embedding chunks directly from the docs** -> lets create a concise summary for each chunk and use this summary along with the original content in our embedding process.\n",
    "\n",
    "\n",
    "This approach aims to capture the essence of each document chunk more effectively, potentially leading to improved retrieval performance. The steps:\n",
    "\n",
    "1. We load the original document chunks.\n",
    "2. For each chunk, we generate a 2-3 sentence summary using Claude.\n",
    "3. We store both the original content and the summary for each chunk in a new json file: data/anthropic_summary_indexed_docs.json\n",
    "\n",
    "\n",
    "The idea is to rebuild the vector DB which includes the summary feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate summaries and storing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from anthropic import Anthropic\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_summaries(input_file, output_file):\n",
    " \n",
    "    # Load the original documents\n",
    "    with open(input_file, 'r') as f:\n",
    "        docs = json.load(f)\n",
    "\n",
    "    # Prepare the context about the overall knowledge base\n",
    "    knowledge_base_context = \"This is documentation for Anthropic's, a frontier AI lab building Claude, an LLM that excels at a variety of general purpose tasks. These docs contain model details and documentation on Anthropic's APIs.\"\n",
    "\n",
    "    summarized_docs = []\n",
    "\n",
    "    for doc in tqdm(docs, desc=\"Generating summaries\"):\n",
    "        prompt = f\"\"\"\n",
    "        You are tasked with creating a short summary of the following content from Anthropic's documentation. \n",
    "\n",
    "        Context about the knowledge base:\n",
    "        {knowledge_base_context}\n",
    "\n",
    "        Content to summarize:\n",
    "        Heading: {doc['chunk_heading']}\n",
    "        {doc['text']}\n",
    "\n",
    "        Please provide a brief summary of the above content in 2-3 sentences. The summary should capture the key points and be concise. We will be using it as a key part of our search pipeline when answering user queries about this content. \n",
    "\n",
    "        Avoid using any preamble whatsoever in your response. Statements such as 'here is the summary' or 'the summary is as follows' are prohibited. You should get straight into the summary itself and be concise. Every word matters.\n",
    "        \"\"\"\n",
    "\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=150,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "\n",
    "        summary = response.content[0].text.strip()\n",
    "\n",
    "        summarized_doc = {\n",
    "            \"chunk_link\": doc[\"chunk_link\"],\n",
    "            \"chunk_heading\": doc[\"chunk_heading\"],\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"summary\": summary\n",
    "        }\n",
    "        summarized_docs.append(summarized_doc)\n",
    "\n",
    "    # Save the summarized documents to a new JSON file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(summarized_docs, f, indent=2)\n",
    "\n",
    "    print(f\"Summaries generated and saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_summaries('data/anthropic_docs.json', 'data/anthropic_summary_indexed_docs.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary-Indexed Vector Database Creation\n",
    "\n",
    "Here, we're creating a new vector database that incorporates our summary-enhanced document chunks. This approach combines the **original text, the chunk heading, and the newly generated summary into a single text for embedding**.\n",
    "\n",
    "Steps:\n",
    "\n",
    "- We create embeddings for the combined text (heading + summary + original content) using the Voyage AI API.\n",
    "- The embeddings and full metadata (including summaries) are stored in our vector database.\n",
    "- We implement caching mechanisms to improve efficiency in repeated queries.\n",
    "- The database is saved to disk for persistence and quick loading in future sessions.\n",
    "\n",
    "This summary-indexed approach aims to create **more informative embeddings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import voyageai\n",
    "\n",
    "class SummaryIndexedVectorDB:\n",
    "    def __init__(self, name, api_key=None):\n",
    "        if api_key is None:\n",
    "            api_key = os.getenv(\"VOYAGE_API_KEY\")\n",
    "        self.client = voyageai.Client(api_key=api_key)\n",
    "        self.name = name\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "        self.query_cache = {}\n",
    "        self.db_path = f\"./data/{name}/summary_indexed_vector_db.pkl\"\n",
    "\n",
    "    def load_data(self, data_file):\n",
    "        # Check if the vector database is already loaded\n",
    "        if self.embeddings and self.metadata:\n",
    "            print(\"Vector database is already loaded. Skipping data loading.\")\n",
    "            return\n",
    "        # Check if vector_db.pkl exists\n",
    "        if os.path.exists(self.db_path):\n",
    "            print(\"Loading vector database from disk.\")\n",
    "            self.load_db()\n",
    "            return\n",
    "\n",
    "        with open(data_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        texts = [f\"{item['chunk_heading']}\\n\\n{item['text']}\\n\\n{item['summary']}\" for item in data]  # Embed Chunk Heading + Text + Summary Together\n",
    "        # Embed more than 128 documents with a for loop\n",
    "        batch_size = 128\n",
    "        result = [\n",
    "            self.client.embed(\n",
    "                texts[i : i + batch_size],\n",
    "                model=\"voyage-2\"\n",
    "            ).embeddings\n",
    "            for i in range(0, len(texts), batch_size)\n",
    "        ]\n",
    "\n",
    "        # Flatten the embeddings\n",
    "        self.embeddings = [embedding for batch in result for embedding in batch]\n",
    "        self.metadata = data  # Store the entire item as metadata\n",
    "        self.save_db()\n",
    "        # Save the vector database to disk\n",
    "        print(\"Vector database loaded and saved.\")\n",
    "\n",
    "    def search(self, query, k=3, similarity_threshold=0.75):\n",
    "        query_embedding = None\n",
    "        if query in self.query_cache:\n",
    "            query_embedding = self.query_cache[query]\n",
    "        else:\n",
    "            query_embedding = self.client.embed([query], model=\"voyage-2\").embeddings[0]\n",
    "            self.query_cache[query] = query_embedding\n",
    "\n",
    "        if not self.embeddings:\n",
    "            raise ValueError(\"No data loaded in the vector database.\")\n",
    "\n",
    "        similarities = np.dot(self.embeddings, query_embedding)\n",
    "        top_indices = np.argsort(similarities)[::-1]\n",
    "        top_examples = []\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] >= similarity_threshold:\n",
    "                example = {\n",
    "                    \"metadata\": self.metadata[idx],\n",
    "                    \"similarity\": similarities[idx],\n",
    "                }\n",
    "                top_examples.append(example)\n",
    "                \n",
    "                if len(top_examples) >= k:\n",
    "                    break\n",
    "        self.save_db()\n",
    "        return top_examples\n",
    "    \n",
    "    def save_db(self):\n",
    "        data = {\n",
    "            \"embeddings\": self.embeddings,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"query_cache\": json.dumps(self.query_cache),\n",
    "        }\n",
    "\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)\n",
    "        \n",
    "        with open(self.db_path, \"wb\") as file:\n",
    "            pickle.dump(data, file)\n",
    "\n",
    "    def load_db(self):\n",
    "        if not os.path.exists(self.db_path):\n",
    "            raise ValueError(\"Vector database file not found. Use load_data to create a new database.\")\n",
    "        \n",
    "        with open(self.db_path, \"rb\") as file:\n",
    "            data = pickle.load(file)\n",
    "        \n",
    "        self.embeddings = data[\"embeddings\"]\n",
    "        self.metadata = data[\"metadata\"]\n",
    "        self.query_cache = json.loads(data[\"query_cache\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key aspects of this updated retrieval process:\n",
    "\n",
    "- We search the vector database using the query embedding, retrieving the top k most similar documents.\n",
    "- For each retrieved document, we include the chunk heading, summary, and full text in the context provided to the LLM.\n",
    "- This enriched context is then used to generate an answer to the user's query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_level_two(query, db):\n",
    "    results = db.search(query, k=3)\n",
    "    context = \"\"\n",
    "    for result in results:\n",
    "        chunk = result['metadata']\n",
    "        context += f\"\\n <document> \\n {chunk['chunk_heading']}\\n\\nText\\n {chunk['text']} \\n\\nSummary: \\n {chunk['summary']} \\n </document> \\n\" #show model all 3 items\n",
    "    return results, context\n",
    "\n",
    "\n",
    "def answer_query_level_two(query, db):\n",
    "    documents, context = retrieve_base(query, db)\n",
    "    prompt = f\"\"\"\n",
    "    You have been tasked with helping us to answer the following query: \n",
    "    <query>\n",
    "    {query}\n",
    "    </query>\n",
    "    You have access to the following documents which are meant to provide context as you answer the query:\n",
    "    <documents>\n",
    "    {context}\n",
    "    </documents>\n",
    "    Please remain faithful to the underlying context, and only deviate from it if you are 100% sure that you know the answer already. \n",
    "    Answer the question now, and avoid providing preamble such as 'Here is the answer', etc\n",
    "    \"\"\"\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=2500,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database loaded and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  10%|█         | 10/100 [00:02<00:22,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/100 items. Current Avg Precision: 0.5000, Avg Recall: 0.8000, Avg MRR: 0.8500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  20%|██        | 20/100 [00:05<00:23,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 20/100 items. Current Avg Precision: 0.4000, Avg Recall: 0.6750, Avg MRR: 0.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  30%|███       | 30/100 [00:08<00:17,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 30/100 items. Current Avg Precision: 0.4222, Avg Recall: 0.6833, Avg MRR: 0.7111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  40%|████      | 40/100 [00:10<00:15,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 40/100 items. Current Avg Precision: 0.4583, Avg Recall: 0.7000, Avg MRR: 0.7583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  50%|█████     | 50/100 [00:13<00:14,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50/100 items. Current Avg Precision: 0.4533, Avg Recall: 0.7100, Avg MRR: 0.7633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  60%|██████    | 60/100 [00:16<00:11,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 60/100 items. Current Avg Precision: 0.4556, Avg Recall: 0.7278, Avg MRR: 0.7944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  70%|███████   | 70/100 [00:18<00:07,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 70/100 items. Current Avg Precision: 0.4381, Avg Recall: 0.6988, Avg MRR: 0.7548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  80%|████████  | 80/100 [00:21<00:05,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 80/100 items. Current Avg Precision: 0.4542, Avg Recall: 0.7240, Avg MRR: 0.7854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  90%|█████████ | 90/100 [00:24<00:02,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 90/100 items. Current Avg Precision: 0.4556, Avg Recall: 0.7231, Avg MRR: 0.7852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval: 100%|██████████| 100/100 [00:27<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/100 items. Current Avg Precision: 0.4500, Avg Recall: 0.7092, Avg MRR: 0.7700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   1%|          | 1/100 [00:04<07:10,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the key elements from the Correct Answer - namely that you can create multiple test cases by clicking the 'Add Test Case' button and filling in values for variables in your prompt, then repeating this process for additional test cases. The Generated Answer actually provides more detail than the Correct Answer by mentioning you can re-run the evaluation suite, but this additional information doesn't contradict the core information. The essential steps and process described are the same in both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   2%|▏         | 2/100 [00:10<09:04,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct in substance compared to the Correct Answer. Both answers identify Voyage AI as Anthropic's recommended embeddings provider and both mention that Voyage AI offers customized/fine-tuned models for specific domains and individual customers. While the Generated Answer provides more specific details about Voyage AI's model offerings that aren't mentioned in the Correct Answer, this additional information doesn't contradict the Correct Answer - it merely elaborates on it. The core claims about Voyage AI's capabilities for domain-specific customization and bespoke fine-tuning are consistent between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   3%|▎         | 3/100 [00:16<08:59,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures all the essential elements from the Correct Answer and even provides additional helpful details. Both answers mention the same key success metrics: accuracy, F1 score, consistency, structure, speed, and bias/fairness. Both answers also address the relationship between model choice and latency. While the Generated Answer goes into more specific detail about different Claude models (Haiku, Sonnet, Opus), this additional information doesn't contradict the Correct Answer but rather expands upon it. The core message about choosing the right model to balance speed and performance requirements is consistent between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   4%|▍         | 4/100 [00:20<08:18,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is partially correct but misses a key element from the correct answer. While both answers correctly identify parallel evaluation/testing as one advantage, the second point differs significantly. The correct answer specifically mentions Claude for Sheets' excellence at office tasks like survey analysis and online data processing, while the generated answer instead talks about an integrated workflow and centralized environment. This represents a substantial difference in the functionality being described. Since one of the two key advantages is missing from the generated answer, it cannot be considered fully correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   5%|▌         | 5/100 [00:26<08:25,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same core information - that missing the \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns in the prompt will result in an API error. The Generated Answer actually provides slightly more context by explaining that these turns are expected to indicate the start of human input and assistant response, but this additional detail doesn't change the fundamental correctness of the answer. There are no contradictions between the two answers, and no critical information from the Correct Answer is missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   6%|▌         | 6/100 [00:32<08:42,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it conveys the same essential information as the Correct Answer. Both answers emphasize that:\n",
      "\n",
      "1. Tool use requests are priced based on total tokens like regular requests\n",
      "2. There are additional tokens required for tool use beyond regular input/output tokens\n",
      "3. These additional tokens include the tools parameter, tool use/result content blocks, and system prompts\n",
      "4. These extra tokens contribute to the total cost\n",
      "\n",
      "While the Generated Answer provides some specific numbers (like 294 tokens for tool choice and 261 for system prompt) that aren't in the Correct Answer, this additional detail doesn't contradict the core message. The fundamental pricing mechanism and the impact of additional tokens on total cost are accurately represented in both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   7%|▋         | 7/100 [00:38<08:35,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It contains all the essential information from the Correct Answer - specifically the release date (June 27th, 2024) and what features will be available (API usage, billing details, and rate limits). While the Correct Answer provides slightly more detail by mentioning the specific tabs (Usage, Cost, and Rate Limits), this is a minor detail that doesn't change the core meaning. Both answers convey the same fundamental information about what will be available and when.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   8%|▊         | 8/100 [00:45<09:13,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer captures the key elements from the Correct Answer, though it expands on them in more detail. Both answers emphasize:\n",
      "\n",
      "1. The need to consider whether the task requires in-depth thinking/analysis (the Generated Answer elaborates on this with specific examples like complex math problems)\n",
      "\n",
      "2. The impact on latency due to increased output length (both answers explicitly mention this trade-off)\n",
      "\n",
      "The Generated Answer provides more detail and examples, but the core considerations match those in the Correct Answer. There are no contradictions between the two answers, and no critical pieces of information from the Correct Answer are missing from the Generated Answer. The differences are mainly in the level of detail provided, not in the substance of the guidance.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   9%|▉         | 9/100 [00:50<08:46,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it conveys the same core message as the Correct Answer. Both answers emphasize that Claude can be used to summarize PDF documents, making it easier to understand long documents without reading everything. While the Generated Answer includes additional details about the Anthropic Cookbook and text analysis capabilities, these are supplementary details that don't contradict the core message. The essential functionality - uploading PDFs and getting summaries to more easily digest content - is accurately represented in both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  10%|█         | 10/100 [00:55<08:13,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers indicate that you can view the API rate limits in a \"Rate Limits\" tab within Anthropic's console interface. While the Correct Answer specifically mentions \"Developer Console\" and the Generated Answer just says \"Anthropic Console,\" this is a minor difference in terminology that doesn't change the core substance of the answer. Both answers convey the same essential information - that API rate limits can be viewed in a Rate Limits tab.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 10/100 questions. Current Accuracy: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  11%|█         | 11/100 [01:01<08:39,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it misses the specific key metrics mentioned in the correct answer. While the generated answer provides a broader and more detailed set of potential metrics, it fails to mention the two critical metrics specified in the correct answer: the 95th percentile response time and average cost per classification. The generated answer talks about speed/latency and cost-effectiveness in general terms, but doesn't specify these precise measurements. When comparing documentation-based answers, we need to ensure the specific metrics mentioned in the documentation are captured accurately. While the additional metrics suggested in the generated answer might be valuable, they don't align with what's specifically stated in Anthropic's documentation as represented by the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  12%|█▏        | 12/100 [01:09<09:27,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It conveys the same key information as the Correct Answer:\n",
      "\n",
      "1. For Text Completions API: Both answers specify that the system prompt goes before the first \"\\n\\nHuman:\" turn in the prompt text.\n",
      "\n",
      "2. For Messages API: Both answers indicate that the system prompt is specified as a separate \"system\" parameter in the API request.\n",
      "\n",
      "The Generated Answer actually provides more detail through code examples, but the core substance matches the Correct Answer perfectly. There are no contradictions or missing critical pieces of information between the two answers. The differences are merely in presentation and level of detail, not in substance.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:XML parsing error: mismatched tag: line 3, column 805\n",
      "Evaluating End-to-End:  13%|█▎        | 13/100 [01:16<09:32,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer, while detailed and structured, misses a critical element from the correct answer. The correct answer specifically mentions combining XML tags with chain of thought reasoning where \"Claude explains its step-by-step reasoning process\" and gives a specific example using the <thinking> tag for showing Claude's reasoning process. While the generated answer talks about using XML tags and breaking things into steps, it doesn't explicitly address the core concept of using tags to capture Claude's own reasoning process. The correct answer emphasizes the combination of XML tags with Claude's chain of thought reasoning, while the generated answer focuses more on general prompt structuring and task organization. This represents a significant omission of the key concept.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  14%|█▍        | 14/100 [01:22<09:21,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect for several reasons:\n",
      "\n",
      "1. While it correctly identifies the three types of metrics (accuracy, cost, and response time), it fails to provide the specific numerical results that were given in the correct answer.\n",
      "\n",
      "2. The generated answer uses placeholder text ([RESULT_ACCURACY], [RESULT_COST], [RESULT_RESPONSE_TIME]) instead of the actual values from the correct answer (89.01%, $0.0004, and 1.61 seconds).\n",
      "\n",
      "3. The generated answer refers to \"average\" response time, while the correct answer specifically mentions \"95th percentile\" response time, which is a meaningful difference in how the metric is calculated.\n",
      "\n",
      "Even though the general structure and categories of metrics are correct, the absence of the specific numerical results and the difference in how the response time is characterized (average vs 95th percentile) make this answer incomplete and partially incorrect compared to the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  15%|█▌        | 15/100 [01:27<08:35,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures all three key elements from the Correct Answer:\n",
      "1. Having clear success criteria\n",
      "2. Having ways to empirically test against those criteria\n",
      "3. Having a first draft prompt to improve\n",
      "\n",
      "The Generated Answer even presents these points in the same order as the Correct Answer. While it adds an additional detail about using the prompt generator in the Anthropic Console, this extra information doesn't contradict the core message and doesn't affect the fundamental correctness of the answer. The substance and main requirements are identical between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  16%|█▌        | 16/100 [01:34<08:37,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. While it provides more detail and context about the Messages API's response structure, the key point about mid-response prompting matches the Correct Answer. Both answers explain that:\n",
      "\n",
      "1. In the Messages API, you can continue a response by setting the last message to have the \"assistant\" role\n",
      "2. In the Text Completions API, you can pre-fill part of Claude's response in the prompt\n",
      "\n",
      "The Generated Answer expresses these same core concepts, just with additional technical details. There are no contradictions between the answers, and no critical information is missing from the Generated Answer compared to the Correct Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  17%|█▋        | 17/100 [01:40<08:32,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures the essential point made in the Correct Answer - that Claude's response with a role-based system prompt leads to more insightful, structured, and actionable financial analysis compared to not having a specific role. The Generated Answer actually provides more detailed examples and specifics about how the analysis differs, but these additional details don't contradict the core message of the Correct Answer. Both answers emphasize that the role-based response provides better structure, more strategic insights, and more actionable recommendations. There are no critical pieces of information from the Correct Answer that are missing from the Generated Answer, nor are there any contradictions between the two.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  18%|█▊        | 18/100 [01:47<08:50,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>\n",
      "The Generated Answer is correct as it captures the key elements from the Correct Answer and expands on them appropriately:\n",
      "\n",
      "1. It mentions key quantitative metrics (F1 score is specifically mentioned in both answers)\n",
      "2. It discusses how targets should be determined based on industry benchmarks and prior experiments, which aligns with the Correct Answer\n",
      "3. While the Generated Answer provides more specific examples and numerical targets, this additional detail doesn't contradict the Correct Answer - it merely elaborates on it\n",
      "4. Both answers emphasize the importance of having clear metrics and establishing appropriate targets based on relevant benchmarks and requirements\n",
      "\n",
      "The Generated Answer actually provides more detail than the Correct Answer, but importantly, it doesn't contradict or miss any critical elements from the Correct Answer. The core concepts about types of metrics and how to determine targets are consistent between both answers.\n",
      "</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:XML parsing error: mismatched tag: line 9, column 182\n",
      "Evaluating End-to-End:  19%|█▉        | 19/100 [01:51<07:50,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the key elements from the Correct Answer:\n",
      "1. The core concept of combining XML tags with other prompt engineering techniques\n",
      "2. Specifically mentions multishot prompting using <examples> tags\n",
      "3. Mentions chain of thought using <thinking> and <answer> tags\n",
      "4. Notes that this creates \"super-structured, high-performance prompts\"\n",
      "\n",
      "While the wording is slightly different, the substance and meaning are identical. There are no missing critical pieces of information and no contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  20%|██        | 20/100 [01:59<08:20,  6.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures all the essential elements from the Correct Answer and even provides additional helpful detail while maintaining the core concept. Both answers emphasize:\n",
      "\n",
      "1. The need to provide a rubric to the LLM\n",
      "2. Having the LLM evaluate outputs against this rubric\n",
      "3. Getting a simple \"correct\" or \"incorrect\" result\n",
      "4. Having the LLM think through its reasoning\n",
      "\n",
      "The Generated Answer breaks this down into more detailed steps, but doesn't contradict or omit any crucial information from the Correct Answer. The additional detail actually helps clarify the process while staying true to the core concept presented in the Correct Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 20/100 questions. Current Accuracy: 0.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  21%|██        | 21/100 [02:04<07:58,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures all the essential steps from the Correct Answer. Both answers outline the same key process:\n",
      "1. Subscribing to the model package on AWS Marketplace\n",
      "2. Getting the Product ARN\n",
      "3. Creating a JupyterLab space in SageMaker Studio\n",
      "4. Using Voyage's notebook to deploy the model with the ARN\n",
      "\n",
      "The Generated Answer breaks down the steps in a slightly different way but conveys the same essential information. There are no critical omissions or contradictions between the two answers. The minor differences in wording and organization don't affect the substantive accuracy of the information provided.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  22%|██▏       | 22/100 [02:12<08:26,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it misses some key critical points from the Correct Answer and provides different guidance. Specifically:\n",
      "\n",
      "1. The Correct Answer emphasizes using a SINGLE tool, while the Generated Answer talks about tools in plural without this specification.\n",
      "\n",
      "2. The Correct Answer specifically mentions setting the tool_choice parameter to explicitly instruct the model to use the tool, which is completely missing from the Generated Answer.\n",
      "\n",
      "3. The Correct Answer emphasizes that tool names and descriptions should be from the model's perspective since it will pass the input to the tool - this important perspective consideration is missing from the Generated Answer.\n",
      "\n",
      "Instead, the Generated Answer focuses more on the general process of tool usage and implementation details, which, while not necessarily wrong, misses the specific key points that are crucial for getting JSON output using tools. The Generated Answer describes a more general tool interaction flow rather than the specific requirements for JSON output formatting.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  23%|██▎       | 23/100 [02:19<08:41,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct and actually provides more detailed information than the Correct Answer while maintaining all the key points. The Generated Answer covers all the essential differences mentioned in the Correct Answer:\n",
      "\n",
      "1. It mentions the vision capabilities of Claude 3 Haiku\n",
      "2. It states that Haiku is faster and more performant\n",
      "3. It indicates that Haiku has better capabilities and intelligence\n",
      "4. While not explicitly stating \"more up-to-date training data,\" it implies this through mentioning better language understanding and capabilities\n",
      "\n",
      "The Generated Answer then goes beyond the Correct Answer by providing additional details about cost differences and context window sizes. These additional details don't contradict anything in the Correct Answer, they simply provide more comprehensive information. Since all key points from the Correct Answer are covered and there are no contradictions, the Generated Answer should be considered correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  24%|██▍       | 24/100 [02:23<07:33,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers emphasize the same key point that using examples helps reduce misinterpretation of instructions when working with Claude. While the Generated Answer includes additional information about enforcing uniform structure and style, this doesn't contradict the core message about reducing misinterpretation. The Generated Answer effectively captures the main benefit mentioned in the Correct Answer, just with slightly different wording and some extra detail. Since there are no contradictions and the essential point about reducing misinterpretation is present, this should be considered correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  25%|██▌       | 25/100 [02:30<07:53,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer, while providing additional details about resource efficiency and other benefits, does not directly address the key point made in the Correct Answer about the ability to adapt models by providing domain-specific context in prompts. While the Generated Answer isn't wrong in what it states, it misses the central advantage highlighted in the Correct Answer - the ability to easily adapt models to new domains through contextual prompts without retraining. The Generated Answer focuses more on resource efficiency and speed benefits rather than the core adaptability advantage mentioned in the Correct Answer. Since this represents a significant omission of the main point, the Generated Answer should be considered incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  26%|██▌       | 26/100 [02:36<07:31,  6.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. While it provides more detailed steps, the core information matches the Correct Answer - that users can get started quickly by making a copy of Anthropic's provided Claude for Sheets workbook template. The additional details in the Generated Answer don't contradict the Correct Answer, they simply elaborate on the implementation. The fundamental message about using a provided template as a quick start method is preserved in both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  27%|██▋       | 27/100 [02:41<07:07,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it conveys the same essential meaning as the Correct Answer. Both answers explain that:\n",
      "\n",
      "1. The \"index\" field indicates which content block the text relates to\n",
      "2. The field is used to identify/track specific content blocks within the response\n",
      "3. The field is connected to the streaming of text content\n",
      "\n",
      "While the Generated Answer uses slightly different wording and adds some additional detail about the \"delta\" object, it maintains the core concept that the index field serves to identify which content block is being modified/streamed. There are no contradictions between the answers, and no critical information is missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  28%|██▊       | 28/100 [02:47<07:00,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. While it provides additional accurate details about image handling, it misses a critical piece of information from the Correct Answer: that images need to be provided as base64-encoded content blocks within the messages array. Instead, the Generated Answer incorrectly states that you can \"upload the image file directly to the API.\" This is a direct contradiction of the correct implementation method specified in the Correct Answer. While both answers correctly list the supported image formats (JPEG, PNG, GIF, and WebP), the method of including images in the API request is fundamentally different between the two answers.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  29%|██▉       | 29/100 [02:54<07:08,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same core concept that TTFT is a specific component of overall latency, measuring specifically the time to generate the first token of a response. The Generated Answer actually provides additional relevant context about factors affecting TTFT and latency, but this extra information doesn't contradict the Correct Answer - it merely elaborates on it. The key relationship between TTFT and latency is accurately captured in both answers, with both emphasizing that TTFT is a specific measure that contributes to overall latency. The Generated Answer also maintains the emphasis on responsiveness that's mentioned in the Correct Answer, though it describes this through different wording.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  30%|███       | 30/100 [03:00<07:08,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same core message that providing edge case examples to Claude can improve its performance in routing support tickets. The Generated Answer actually expands on the Correct Answer by providing more detailed explanations of how examples can help with different types of edge cases (implicit requests, emotional prioritization, intent vs. routing, and issue prioritization). While it provides more detail, it doesn't contradict the Correct Answer, and it covers all the key points mentioned in the Correct Answer about improving ticket routing performance through examples. The substance and main point of both answers align completely.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 30/100 questions. Current Accuracy: 0.7333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  31%|███       | 31/100 [03:06<07:06,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures all the essential elements of the Correct Answer and even provides additional helpful context while maintaining accuracy. Both answers describe:\n",
      "\n",
      "1. Claude's determination that a tool is needed\n",
      "2. The construction of a tool use request\n",
      "3. The stop_reason=\"tool_use\" as a signal of Claude's intent\n",
      "4. The need for the client/external system to extract and execute the tool\n",
      "5. The requirement to return results back to Claude\n",
      "\n",
      "The Generated Answer expands on these points in a more detailed way but doesn't contradict or omit any critical information from the Correct Answer. The core workflow and the role of the stop_reason=\"tool_use\" are accurately represented in both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  32%|███▏      | 32/100 [03:11<06:32,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It contains all the key elements from the Correct Answer:\n",
      "1. It correctly identifies the error event as \"overloaded_error\"\n",
      "2. It specifies that this occurs during periods of high usage\n",
      "3. It correctly states that this corresponds to HTTP 529 error code in non-streaming contexts\n",
      "4. It properly contextualizes this within streaming responses\n",
      "\n",
      "The Generated Answer simply rephrases the same information in a slightly different way, but maintains all the critical substance and technical details. There are no contradictions or missing pieces of information between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  33%|███▎      | 33/100 [03:15<05:52,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It identifies both types of deltas that can be contained in a content_block_delta event: text_delta and input_json_delta. While the formatting and presentation are slightly different (using a numbered list instead of prose), the substance and key information are exactly the same as the Correct Answer. Both answers convey the same two specific delta types without any omissions or contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  34%|███▍      | 34/100 [03:21<05:59,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. According to the Correct Answer, Claude 3.5 Sonnet and tool use became generally available on different dates:\n",
      "- Claude 3.5 Sonnet: June 20th, 2024\n",
      "- Tool use: May 30th, 2024\n",
      "\n",
      "The Generated Answer incorrectly states that both became available on June 20th, 2024. This is a critical factual error as it misses the distinction between the two release dates. The timing difference of several weeks between these features' availability is an important detail that shouldn't be overlooked.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  35%|███▌      | 35/100 [03:25<05:19,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct in substance. While it doesn't include the specific timing (May 2024 for Europe and June 2024 for Canada), it accurately captures the key information about the order of the launches - that Anthropic launched Claude.ai and the Claude iOS app in Europe first, followed by Canada. The core sequence of events is preserved, which is the essential information being conveyed. The omission of specific months doesn't change the fundamental accuracy of the answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  36%|███▌      | 36/100 [03:32<05:57,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures all the essential elements from the Correct Answer:\n",
      "\n",
      "1. It correctly explains that a \"tool_use\" stop_reason indicates Claude has decided to use a tool\n",
      "2. It outlines the exact same steps that need to be taken:\n",
      "   - Extracting the tool name and input from Claude's request\n",
      "   - Executing the tool code on the client side\n",
      "   - Sending back a new message with a tool_result content block\n",
      "\n",
      "While the wording and structure differ slightly, the substance and key information are identical. The Generated Answer even provides additional context about why this process is useful (to help Claude formulate a final response), but this extra information doesn't contradict anything in the Correct Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  37%|███▋      | 37/100 [03:37<05:43,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it conveys the same essential information as the Correct Answer. Both answers indicate that the anthropic library is used to interact with Claude/Anthropic's AI capabilities. While the Generated Answer provides slightly more detail by explaining what the anthropic library does, the core substance - that the anthropic library is the Python library used in the example - is consistent between both answers. There are no contradictions or missing critical pieces of information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  38%|███▊      | 38/100 [03:41<05:05,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures both main authentication methods described in the Correct Answer:\n",
      "\n",
      "1. Direct provision of AWS credentials (access key, secret key, and optional session token)\n",
      "2. Using default AWS credential providers (including both the ~/.aws/credentials file and environment variables)\n",
      "\n",
      "The Generated Answer conveys the same essential information as the Correct Answer, just with slightly different wording. There are no missing critical pieces of information and no contradictions between the two answers. The substance and meaning are equivalent.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  39%|███▉      | 39/100 [03:47<05:30,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the same two key factors mentioned in the Correct Answer:\n",
      "\n",
      "1. The risk/potential of prompt leaks (protecting sensitive information)\n",
      "2. The impact on model performance due to added complexity\n",
      "\n",
      "While the Generated Answer elaborates more on each factor with additional examples and details, the core substance and trade-off described is identical to the Correct Answer. Both answers emphasize the need to balance protecting against leaks with maintaining model performance. There are no contradictions between the two answers, and no critical information from the Correct Answer is missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  40%|████      | 40/100 [03:52<05:17,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it conveys the same core message as the Correct Answer. Both answers emphasize that:\n",
      "\n",
      "1. Anthropic offers different Claude models with varying capabilities and performance characteristics\n",
      "2. Selecting the right model allows you to optimize for the best balance of speed, intelligence, and cost\n",
      "3. The main goal is to choose a model that best fits your specific requirements to reduce latency\n",
      "\n",
      "While the Generated Answer provides additional details about model families and metrics, these details don't contradict the Correct Answer but rather expand upon it. The fundamental point about choosing the appropriate model to reduce latency is preserved in both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 40/100 questions. Current Accuracy: 0.7750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  41%|████      | 41/100 [03:58<05:17,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It contains all the essential information from the Correct Answer and even provides more detailed implementation examples. Both answers highlight the key points that:\n",
      "\n",
      "1. You use the client.messages.stream() method\n",
      "2. You iterate over the stream.text_stream attribute in a for loop\n",
      "\n",
      "The Generated Answer expands on this with a practical code example and additional context, but the core information matches the Correct Answer completely. There are no contradictions or missing critical pieces between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  42%|████▏     | 42/100 [04:02<04:53,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures the two key points from the Correct Answer:\n",
      "\n",
      "1. It explains that you can guide Claude's response by pre-filling part of it in the messages list (though it specifically mentions the \"assistant\" message, which is just a more detailed explanation of the same concept)\n",
      "\n",
      "2. It correctly identifies that the \"max_tokens\" parameter is used to generate short responses by limiting the length of the output\n",
      "\n",
      "The substance and main concepts are the same between both answers, even though they use slightly different wording. There are no critical missing pieces of information or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  43%|████▎     | 43/100 [04:08<05:06,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers make the same core assertion that having a larger volume of automated test cases is more important than having fewer human-graded test cases when building an eval set. The Generated Answer expands on this with additional context and explanation, but the fundamental point being made is identical to the Correct Answer. There are no contradictions between the two answers, and no critical pieces of information from the Correct Answer are missing from the Generated Answer. While the Generated Answer is more verbose, the substance and main conclusion are the same.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  44%|████▍     | 44/100 [04:12<04:40,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it misses a critical required field mentioned in the Correct Answer. While it correctly identifies that there needs to be a \"type\" and \"text\" field, these are actually part of the \"delta\" field. The Correct Answer states that the two required fields are \"index\" and \"delta\". The Generated Answer completely omits the required \"index\" field, which makes it incomplete and therefore incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  45%|████▌     | 45/100 [04:17<04:22,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it misses one of the key interactive learning methods mentioned in the correct answer. While it correctly identifies the Anthropic Cookbook as one interactive way to learn Claude's capabilities, it fails to mention the Developer Console and its prompt generator tool, which is the second key method specified in the correct answer. Instead, it incorrectly references \"Claude for Sheets usage examples\" as the second method. This represents a significant omission and deviation from the correct answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  46%|████▌     | 46/100 [04:22<04:23,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. The core concept from the Correct Answer - that breaking tasks into subtasks improves accuracy because each subtask gets Claude's full attention and reduces errors compared to handling everything at once - is fully captured in the Generated Answer's first point about accuracy. While the Generated Answer goes on to provide additional points about clarity and traceability, these are supplementary details that don't contradict the core concept. The essential message about improved accuracy through focused attention on subtasks is present in both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  47%|████▋     | 47/100 [04:30<05:04,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. The key point from the Correct Answer - that Messages streaming responses can contain multiple content blocks of varying types, making it more complex than Text Completions streaming - is accurately captured in the Generated Answer's first point. While the Generated Answer provides additional details about the differences between the two formats, these extra details don't contradict the core concept presented in the Correct Answer. The Generated Answer effectively communicates the main distinction between the two streaming formats, which is the ability to handle multiple content blocks of different types in Messages responses versus the simpler format of Text Completions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  48%|████▊     | 48/100 [04:36<05:08,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is partially incorrect. While it correctly mentions claude.ai (the web Console) as one way to experiment with Claude, it incorrectly lists the Quickstart guide/API call as the second method instead of just the web Console. The Correct Answer specifically states that the two ways are \"claude.ai\" and \"Anthropic's web Console\" - which are actually referring to the same interface, just described differently. The Generated Answer introduces the API/Quickstart guide which isn't mentioned in the Correct Answer as one of the two primary ways to experiment with Claude. This represents a substantive difference in the information provided.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  49%|████▉     | 49/100 [04:42<05:02,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct and actually provides more detailed elaboration on the core concept presented in the Correct Answer. Both answers share the same fundamental point: that chain prompts help reduce errors and inconsistencies by breaking complex tasks into smaller, more manageable subtasks that Claude can focus on individually. The Generated Answer expands on this concept by providing additional benefits and mechanisms, but it doesn't contradict or miss any critical information from the Correct Answer. The additional detail in the Generated Answer (like traceability and mitigation of hallucinations) simply provides more context and explanation around the core concept, rather than changing or contradicting the main point.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  50%|█████     | 50/100 [04:45<04:18,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers state that an overloaded_error event corresponds to HTTP status code 529 in a non-streaming context for the Anthropic API. While the Correct Answer uses slightly more formal language (\"would normally correspond to\"), the core information - the 529 status code - is identical in both answers. The difference in phrasing does not change the fundamental meaning or accuracy of the response.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 50/100 questions. Current Accuracy: 0.7600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  51%|█████     | 51/100 [04:51<04:18,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the same two key ways to specify the embedding format as described in the Correct Answer:\n",
      "\n",
      "1. Both answers indicate that leaving the format unspecified will return embeddings as lists of floating-point numbers\n",
      "2. Both answers state that setting the format to \"base64\" will return the embeddings in Base64 encoded format\n",
      "\n",
      "The Generated Answer presents the same information in a slightly more structured format with bullet points, but the substance and technical details are identical to the Correct Answer. There are no missing critical pieces of information or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  52%|█████▏    | 52/100 [04:57<04:27,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it conveys the same essential information as the Correct Answer. Both answers explain that:\n",
      "\n",
      "1. Tool use content blocks are sent as partial JSON strings in content_block_delta events\n",
      "2. The client needs to accumulate these partial JSON strings\n",
      "3. The complete JSON can be parsed once a content_block_stop event is received\n",
      "4. Parsing can be done using Pydantic or SDK helpers\n",
      "\n",
      "The Generated Answer actually provides additional helpful detail by showing an example of the delta structure, but this doesn't contradict anything in the Correct Answer. The core concepts and process are described accurately and consistently between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  53%|█████▎    | 53/100 [05:03<04:22,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect. While it correctly identifies that there are two tutorials - one on GitHub and one involving Google Sheets - it makes incorrect claims about their nature. The generated answer states that the Claude for Sheets tutorial is \"in-depth\" and requires an API key, while the correct answer specifically states that the Google Sheets tutorial is \"lighter-weight.\" This is a direct contradiction. Additionally, the generated answer adds details about API key requirements that aren't mentioned in the correct answer and could be misleading. The core characterization of the tutorials is reversed - according to the correct answer, it's the GitHub tutorial that is in-depth, not the Google Sheets one.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  54%|█████▍    | 54/100 [05:11<04:50,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct and actually provides more comprehensive detail than the Correct Answer. It covers all the key points mentioned in the Correct Answer:\n",
      "\n",
      "1. The 200K token context window\n",
      "2. Tool use capabilities for integration with specialized applications\n",
      "3. Multimodal input capabilities\n",
      "4. Enterprise-grade security and data handling for sensitive information\n",
      "\n",
      "The Generated Answer then goes beyond these points to provide additional relevant details about enterprise capabilities, such as HIPAA compliance, SOC II certification, developer features, and global language capabilities. While it provides more information than the Correct Answer, it doesn't contradict any points and includes all the critical elements mentioned in the Correct Answer. The additional information simply provides more context and depth to the core capabilities mentioned in the Correct Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  55%|█████▌    | 55/100 [05:15<04:10,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it omits a key region where Claude.ai API and iOS app are available - the United States. While the Generated Answer correctly mentions Canada and Europe, leaving out the United States represents a significant omission of information. The availability in the United States is a critical piece of information present in the Correct Answer but missing from the Generated Answer. Therefore, despite getting two regions correct, the Generated Answer is incomplete and thus incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  56%|█████▌    | 56/100 [05:21<04:18,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures both main approaches (push-based using webhooks and pull-based) and accurately describes their key differences in terms of scalability and implementation ease. The Generated Answer expands on the Correct Answer with more detail, but maintains the same core information:\n",
      "\n",
      "1. It correctly identifies that push-based is more scalable but has security implications due to requiring a public endpoint\n",
      "2. It correctly states that pull-based is easier to implement but less efficient due to unnecessary system calls\n",
      "\n",
      "The Generated Answer doesn't contradict any points in the Correct Answer, and includes all the critical information. While it provides more detailed explanations, the substance of both answers is the same.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  57%|█████▋    | 57/100 [05:25<03:48,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is completely correct. It contains all the key information from the Correct Answer: the release date (May 10th, 2024), what was released (a prompt generator tool), and where it's available (through the Developer Console). The wording is slightly different but conveys exactly the same information and meaning. There are no missing critical pieces of information and no contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  58%|█████▊    | 58/100 [05:32<03:57,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same core message that Claude 3 Sonnet provides the optimal balance between intelligence and speed for high-throughput tasks like sales forecasting and targeted marketing. The Generated Answer actually provides additional supporting detail about Sonnet's \"Strong utility, balanced for scaled deployments\" but this extra information doesn't change or contradict the fundamental point. The key elements from the Correct Answer (Sonnet model, balance of intelligence/speed, suitability for high-throughput tasks) are all present in the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  59%|█████▉    | 59/100 [05:37<03:44,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It conveys the same core information as the Correct Answer - that similarity between Voyage embedding vectors can be calculated using either dot product or cosine similarity, and these are equivalent because the vectors are normalized to length 1. In fact, the Generated Answer goes into more helpful detail explaining why this is the case mathematically, but the fundamental point being made is identical to the Correct Answer. There are no contradictions between the two answers, and no critical information is missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  60%|██████    | 60/100 [05:43<03:44,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures all the key points from the Correct Answer and even expands on them in a helpful way. Both answers emphasize that examples can:\n",
      "1. Reduce misinterpretation of instructions\n",
      "2. Help enforce consistent structure and style\n",
      "3. Guide Claude toward desired output/performance\n",
      "\n",
      "The Generated Answer provides additional details and examples, but these don't contradict the Correct Answer - they simply elaborate on the core concepts. The substance of both answers is fundamentally the same, even though the Generated Answer is more detailed. There are no critical omissions or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 60/100 questions. Current Accuracy: 0.7667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  61%|██████    | 61/100 [05:49<03:49,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It accurately describes both types of content block deltas:\n",
      "\n",
      "1. It correctly identifies that there are text deltas and input JSON deltas\n",
      "2. For text deltas, both answers indicate they contain text content/updates\n",
      "3. For input JSON deltas, both answers explain that they contain partial JSON content related to tool input\n",
      "4. The substance of what each delta type contains matches between both answers\n",
      "\n",
      "While the wording and level of detail varies slightly between the answers, the core information about the two types of deltas and their contents is consistent and accurate. There are no contradictions or missing critical pieces of information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  62%|██████▏   | 62/100 [05:55<03:39,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it focuses on different capabilities than those mentioned in the Correct Answer. The Correct Answer specifically highlights question answering and text analysis capabilities as key enablers for interactive systems and personalization. In contrast, the Generated Answer discusses text/code generation and tool use capabilities. While these are valid capabilities of Claude, they are not the specific ones identified in the Correct Answer as enabling interactive systems and personalized experiences. Additionally, the Generated Answer misses the important point about understanding sentiment and preferences that is mentioned in the Correct Answer. The two answers are discussing different sets of capabilities, making them substantively different.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  63%|██████▎   | 63/100 [06:00<03:24,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures all the key elements from the Correct Answer and presents them in essentially the same order:\n",
      "\n",
      "1. Both answers mention the message_start event coming first\n",
      "2. Both describe the content blocks structure with start, delta, and stop events\n",
      "3. Both mention message_delta events\n",
      "4. Both include the final message_stop event\n",
      "5. Both note that ping events may be dispersed throughout\n",
      "\n",
      "The Generated Answer actually provides slightly more detail by explicitly mentioning that the message_start contains a Message object with empty content, but this additional detail doesn't contradict the Correct Answer. The core sequence and components are identical between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  64%|██████▍   | 64/100 [06:04<03:12,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It conveys the same key information as the Correct Answer - specifically that the Anthropic API allows up to 20 images per request while the claude.ai interface has a 5 image limit. While the Correct Answer uses slightly different wording (\"per turn\" vs \"per request\"), the substance and numerical limits stated are identical. There are no critical missing pieces of information or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  65%|██████▌   | 65/100 [06:09<02:56,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the key substance of the Correct Answer, which is that when Claude's response contains an incomplete tool use block due to hitting the max_tokens limit, you should retry with a higher max_tokens value to get the complete tool use. The wording is almost identical, with the Generated Answer just being slightly more concise but conveying the exact same meaning and instruction. There are no missing critical pieces of information and no contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  66%|██████▌   | 66/100 [06:13<02:41,  4.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. While both answers mention developing test cases as one of the steps, they differ on the second step. The Correct Answer states that you need to \"take a look at Anthropic's guide to developing test cases\" while the Generated Answer states you need to \"build a strong input prompt.\" These are substantively different steps. The Generated Answer is missing the critical piece about consulting Anthropic's guide, and instead introduces a different step that wasn't mentioned in the Correct Answer. This represents a meaningful difference in the substance of what needs to be done before running a classification evaluation.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  67%|██████▋   | 67/100 [06:19<02:54,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is correct and actually provides more detail than the correct answer while maintaining the core concept. Both answers emphasize the key point that you can use the content parameter with an \"assistant\" role message to pre-fill and shape Claude's response. The generated answer expands on this with additional context about system prompts and conversation simulation, but the fundamental mechanism described matches the correct answer. There are no contradictions between the two answers, and the generated answer includes all critical information from the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  68%|██████▊   | 68/100 [06:29<03:28,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures both key advantages mentioned in the Correct Answer:\n",
      "\n",
      "1. It correctly states that prompt engineering preserves general knowledge while fine-tuning risks catastrophic forgetting\n",
      "2. It accurately notes that prompt engineering is more effective at helping models understand and utilize external content/retrieved documents\n",
      "\n",
      "The Generated Answer essentially restates the same two key points from the Correct Answer, just with slightly different wording. There are no missing critical pieces of information and no contradictions between the two answers. The substance and meaning are identical.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  69%|██████▉   | 69/100 [06:34<03:05,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. The key difference is that the Generated Answer describes steps for using Anthropic's direct API (obtaining an Anthropic account/API key), while the Correct Answer specifically addresses Bedrock API integration, which requires AWS CLI configuration and a Bedrock SDK. These are fundamentally different authentication and setup processes. The Generated Answer misses the critical AWS-specific components required for Bedrock integration and instead describes a different access path altogether. This represents a substantive difference in the technical requirements and implementation approach, not just a difference in wording.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  70%|███████   | 70/100 [06:38<02:49,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It provides the exact same command structure and functionality as the Correct Answer, including:\n",
      "1. The correct AWS CLI command `aws bedrock list-foundation-models`\n",
      "2. The correct use of the `--region` parameter\n",
      "3. The correct use of `--by-provider anthropic`\n",
      "4. The correct query parameter to get model IDs\n",
      "5. A specific example using `us-west-2` region\n",
      "\n",
      "The Generated Answer conveys the same essential information and instructions as the Correct Answer, just with slightly different wording in the explanatory text. There are no missing critical pieces of information or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 70/100 questions. Current Accuracy: 0.7571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  71%|███████   | 71/100 [06:44<02:46,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same core information - that the `input_type` argument/parameter can be passed with either \"query\" or \"document\" as values to specify the type of input text being embedded. The Generated Answer simply restates this information in a slightly more verbose way, but the substance and technical accuracy are identical to the Correct Answer. There are no missing critical pieces of information or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  72%|███████▏  | 72/100 [06:49<02:29,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is missing a critical piece of information that is present in the Correct Answer. While it correctly describes the basic difference between tool_use deltas (partial JSON strings) and text deltas (direct text updates), it fails to mention that tool_use deltas may have delays between streaming events as the model emits one complete key-value pair at a time. This is an important characteristic of the tool_use streaming behavior that is explicitly mentioned in the Correct Answer but absent from the Generated Answer. Since this represents a meaningful omission of functionality, the Generated Answer should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  73%|███████▎  | 73/100 [06:53<02:15,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It provides the exact same file size limits as the Correct Answer - 5MB for API uploads and 10MB for claude.ai uploads. The Generated Answer simply presents this information in a slightly different format (bullet points) and adds a minor detail about error messages, but the core information about the file size limits matches perfectly with the Correct Answer. There are no contradictions or missing critical pieces of information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  74%|███████▍  | 74/100 [06:58<02:06,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers emphasize the key point of choosing a model that appropriately balances requirements for the specific use case. The Generated Answer actually provides more detail by mentioning Claude 3 Haiku as a specific example, but the core message about selecting a model based on the balance of speed/latency and output quality is present in both answers. There are no contradictions between the two answers, and the Generated Answer captures the essential consideration mentioned in the Correct Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  75%|███████▌  | 75/100 [07:03<02:04,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it contains all the key information from the Correct Answer:\n",
      "1. It correctly identifies the recommended model as \"voyage-code-2\"\n",
      "2. It mentions the 17% better performance compared to alternatives\n",
      "3. It notes the state-of-the-art performance on general-purpose corpora\n",
      "4. The overall meaning and substance is identical, with only minor differences in wording and sentence structure\n",
      "\n",
      "There are no critical omissions or contradictions between the two answers. The only difference is in how the information is phrased, but the core facts and claims remain the same.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  76%|███████▌  | 76/100 [07:07<01:56,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is essentially correct. Both answers highlight that the Anthropic Cookbook provides interactive Jupyter notebooks that demonstrate API functionality, specifically mentioning PDF uploads and embeddings. While the Generated Answer splits this into two points and adds some additional context about hands-on learning, the core information matches the Correct Answer. There are no contradictions or missing critical pieces of information between the two answers - they're conveying the same fundamental message about how the Cookbook helps developers learn through interactive notebooks and demonstrations.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  77%|███████▋  | 77/100 [07:13<01:56,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it conveys the same core concept as the Correct Answer. Both answers emphasize that:\n",
      "\n",
      "1. The context window size directly impacts how much retrieved information can be utilized in RAG\n",
      "2. A larger context window enables better utilization of retrieved information\n",
      "3. This affects the quality and effectiveness of the model's responses\n",
      "\n",
      "While the Generated Answer includes additional details about coherence and complex prompts, it doesn't contradict the Correct Answer. The fundamental understanding that context window size determines how much retrieved information can be used for RAG is present in both answers. The Generated Answer simply elaborates on the implications of this relationship.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  78%|███████▊  | 78/100 [07:20<02:07,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures all the key points from the Correct Answer and even expands on them in a helpful way. Both answers emphasize:\n",
      "\n",
      "1. The tool's ability to identify edge cases where prompts might not perform well\n",
      "2. The capability to rate individual results to assess prompt performance\n",
      "3. The importance of ensuring consistent performance across different inputs\n",
      "4. The ability to review results and spot patterns for making improvements\n",
      "\n",
      "The Generated Answer adds an additional point about feedback and iteration, but this doesn't contradict anything in the Correct Answer - it's just supplementary information. The core substance about how the Evaluation tool helps improve prompts and build robust AI applications is consistent between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  79%|███████▉  | 79/100 [07:25<01:51,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers state that Claude 3 Haiku has the fastest comparative latency. The Generated Answer simply adds a bit more context by specifying \"among the Claude models\" but the core information - that Claude 3 Haiku is the fastest - is identical. There are no contradictions between the answers, and no critical information is missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  80%|████████  | 80/100 [07:33<02:04,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the key point from the Correct Answer that the Messages API is stateless and requires sending the full conversation history with each request. The Generated Answer actually provides more detailed information and a practical example of how to implement this, but the core concept matches exactly with the Correct Answer. Both answers emphasize that you need to include the complete conversation history (both user and assistant messages) in each API call to maintain context. There are no contradictions between the answers, and the Generated Answer doesn't miss any critical information from the Correct Answer - it just expands upon it with implementation details.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 80/100 questions. Current Accuracy: 0.7750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  81%|████████  | 81/100 [07:40<02:00,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures the core message of the Correct Answer. Both answers emphasize that using XML tags with a specific role (like General Counsel) helps Claude identify critical legal issues and risks in contracts that might otherwise be missed. While the Generated Answer provides more detail and additional benefits, it doesn't contradict the Correct Answer, and it maintains the key point about improved analysis leading to better risk identification. The mention of \"saving the company millions of dollars\" from the Correct Answer is implicit in the Generated Answer's discussion of identifying critical risks and issues. The additional detail in the Generated Answer simply expands on the core concept without changing its fundamental meaning.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  82%|████████▏ | 82/100 [07:45<01:47,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is essentially correct. While it uses different wording and provides some additional context about the models' general characteristics, the core distinction it describes aligns with the Correct Answer: Claude 3 Opus is more likely to seek clarification (or \"ask for missing information\" in the Correct Answer's words), while Claude 3 Sonnet is more likely to make inferences/assumptions about missing parameters. The substance of how each model handles missing information in tool calls is consistent between both answers, even though they express it differently.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  83%|████████▎ | 83/100 [07:51<01:44,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it covers all the key points mentioned in the Correct Answer and even provides additional helpful detail. Both answers emphasize:\n",
      "\n",
      "1. Implementing retry logic for error handling\n",
      "2. Conducting thorough staging/testing\n",
      "3. Load testing\n",
      "4. Error handling and logging setup\n",
      "5. Gradual rollout process\n",
      "6. Documentation and training\n",
      "7. Monitoring and alerting\n",
      "\n",
      "The Generated Answer expands on these points with more specific implementation details, but the core recommendations align perfectly with the Correct Answer. There are no contradictions between the two answers, and no critical pieces of information from the Correct Answer are missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  84%|████████▍ | 84/100 [07:57<01:38,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It covers all three key elements mentioned in the Correct Answer:\n",
      "\n",
      "1. Accuracy - explicitly mentioned and discussed\n",
      "2. Cost - covered through \"Average Cost per Classification\"\n",
      "3. Speed - addressed through \"95th Percentile Response Time\"\n",
      "\n",
      "While the Generated Answer provides additional details and context beyond what's in the Correct Answer, it doesn't contradict anything in the Correct Answer and includes all the critical components. The extra information (like consistency across ticket types, handling edge cases, etc.) is supplementary and doesn't detract from the core evaluation criteria specified in the Correct Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  85%|████████▌ | 85/100 [08:02<01:24,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers identify the same two recommended methods for learning prompt engineering with Claude: the GitHub prompting tutorial and the Google Sheets prompting tutorial. While the Generated Answer provides slightly more detail by describing the GitHub tutorial as \"example-filled\" and the Google Sheets version as \"lighter weight,\" the core substance of both answers is identical. Both answers convey that these are preliminary learning tools to use before diving into the main techniques. There are no contradictions between the answers, and no critical information is missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  86%|████████▌ | 86/100 [08:09<01:25,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is partially correct but contains some unsupported claims that go beyond the correct answer. The first two points about training process and capabilities align well with the correct answer - both mention the distinction between pretrained LLMs (trained on unlabeled text) and Claude's additional RLHF training, as well as the difference in their ability to handle tasks without prompt engineering. However, points 3 and 4 about adaptability and interpretability make claims that aren't supported by the correct answer. While these additional points don't directly contradict the correct answer, they represent speculation beyond what's established in the correct answer. Since we're asked to evaluate based on the substance of what's in the correct answer, and these additional claims aren't verified there, we should mark this as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  87%|████████▋ | 87/100 [08:17<01:27,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct and actually provides a more detailed expansion of the key points mentioned in the Correct Answer. It covers all the main advantages mentioned in the Correct Answer:\n",
      "\n",
      "1. Cost and resource efficiency (points 1 and 2)\n",
      "2. Speed and faster implementation (point 4)\n",
      "3. Less data requirements (point 5)\n",
      "4. Flexibility and rapid iteration (point 6)\n",
      "5. Preservation of general knowledge (point 9)\n",
      "6. Transparency (point 10)\n",
      "\n",
      "The Generated Answer adds some additional advantages but doesn't contradict any points in the Correct Answer. It effectively elaborates on the same core concepts while providing more specific examples and explanations. The substance and main message of both answers align completely, with the Generated Answer simply offering more detail and structure.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  88%|████████▊ | 88/100 [08:22<01:12,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same key information - that you need to run the command `gcloud auth application-default login` to authenticate with GCP before accessing Claude models on Vertex AI. The Generated Answer adds a bit more context about why this authentication is needed (to access resources), but this additional detail doesn't change or contradict the core instruction. The substance and critical information is identical between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  89%|████████▉ | 89/100 [08:29<01:10,  6.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer captures the core information about the Prompt Generator tool being introduced on May 10th, 2024, and its main purpose of helping users create tailored prompts for specific tasks. However, there are two issues with the Generated Answer:\n",
      "\n",
      "1. It includes unverified information about \"fine-tuning with legacy models\" that isn't mentioned in the Correct Answer.\n",
      "\n",
      "2. It omits the contextual information about the Claude iOS app and Claude Team plan that were mentioned in the Correct Answer as part of Anthropic's platform expansion.\n",
      "\n",
      "While the central message about the Prompt Generator tool and its purpose is accurate, the inclusion of unverified information and omission of related platform developments makes this answer partially incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  90%|█████████ | 90/100 [08:32<00:55,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It conveys exactly the same information as the Correct Answer - that both Claude 3.5 Sonnet and the Artifacts feature became available on June 20th, 2024. While the wording is slightly different (omitting \"both\" and having a slightly different sentence structure), the core information and meaning are identical. There are no missing critical details or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 90/100 questions. Current Accuracy: 0.7778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  91%|█████████ | 91/100 [08:36<00:44,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same key information - that \"max_tokens\": 1 can be used to limit Claude's response to a single token. The Generated Answer refers to it as a \"header\" while the Correct Answer mentions it as part of the \"request\", but this minor difference in terminology doesn't change the fundamental meaning. Both answers correctly identify both the parameter name (\"max_tokens\") and the value (1) that should be used to achieve the desired single-token limitation.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  92%|█████████▏| 92/100 [08:40<00:37,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same core concept that temperature controls randomness in the model's output generation. The Generated Answer simply provides more detail and elaboration about what higher and lower temperatures do specifically, but the fundamental meaning matches the Correct Answer. There are no contradictions between the two answers, and the Generated Answer includes all critical information from the Correct Answer while providing additional context.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  93%|█████████▎| 93/100 [08:45<00:33,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is partially correct but misses a critical piece of information. While it correctly identifies that API parameters can be specified as additional arguments after the prompt and model (the first way), it completely fails to mention the second key way mentioned in the Correct Answer - the ability to pass in an API key for a specific cell using \"api_key\". Instead, the Generated Answer just describes the basic CLAUDE() function usage. Since it's missing one of the two main ways to specify API parameters that was mentioned in the Correct Answer, this constitutes a significant omission.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  94%|█████████▍| 94/100 [08:49<00:26,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the key points from the Correct Answer:\n",
      "1. Prefilling with { causes Claude to skip the preamble\n",
      "2. Results in direct JSON object output\n",
      "3. Makes the response more concise\n",
      "4. Makes it easier for programs to parse\n",
      "\n",
      "The Generated Answer conveys the same essential information as the Correct Answer, just with slightly different wording. There are no missing critical pieces of information or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  95%|█████████▌| 95/100 [08:53<00:22,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is partially correct but contains extra information that is not verified by the correct answer. The first two points about the multimodal cookbook and API reference documentation match the correct answer's substance. However, the third point about the developer community is not mentioned in the correct answer and appears to be additional unverified information. Since this addition could potentially mislead users about available resources, and we want to be precise about what Anthropic officially provides, the generated answer should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  96%|█████████▌| 96/100 [09:00<00:19,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it conveys the same essential information as the Correct Answer. Both answers indicate that:\n",
      "\n",
      "1. You can specify the API key directly as a parameter when creating a new Anthropic client\n",
      "2. If no API key is provided, it will use the ANTHROPIC_API_KEY environment variable\n",
      "\n",
      "The Generated Answer actually provides more detail by showing code examples in both Python and TypeScript, but the core information matches the Correct Answer. There are no contradictions between the two answers, and no critical information from the Correct Answer is missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  97%|█████████▋| 97/100 [09:04<00:14,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the same two key benefits mentioned in the Correct Answer:\n",
      "\n",
      "1. Both answers mention identifying edge cases where prompts might fail/falter\n",
      "2. Both answers discuss ensuring consistent performance across different test inputs\n",
      "\n",
      "The Generated Answer even expands slightly on these points by mentioning the rating of individual results, but maintains the same core meaning. There are no contradictions between the answers, and no critical information from the Correct Answer is missing from the Generated Answer. The minor differences in wording don't affect the substance of the response.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  98%|█████████▊| 98/100 [09:10<00:10,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it misses a crucial element from the Correct Answer. While the Generated Answer discusses various technical aspects of fine-tuning and deployment, it fails to mention the key point that the pretrained model is not inherently good at following instructions or answering questions, and that specifically reinforcement learning from human feedback (RLHF) was used to create the helpful and safe Claude assistant. The Correct Answer emphasizes this transformation from a basic language model to an instruction-following assistant through RLHF, which is a critical distinction. The Generated Answer instead focuses on more general aspects of deployment and maintenance without capturing this fundamental difference in capabilities and the specific training approach used to achieve them.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  99%|█████████▉| 99/100 [09:13<00:04,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is exactly identical to the Correct Answer, stating that Anthropic's IPv6 address range is 2607:6bc0::/48. There are no differences in wording or substance, and all critical information is included.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End: 100%|██████████| 100/100 [09:18<00:00,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It identifies the same two methods for specifying the API key as mentioned in the Correct Answer:\n",
      "1. Using the environment variable ANTHROPIC_API_KEY\n",
      "2. Passing the API key directly when initializing the client via the api_key parameter\n",
      "\n",
      "While the Generated Answer is more concise, it captures all the essential information from the Correct Answer. There are no contradictions between the two answers, and no critical information is missing. The differences are merely in the level of detail and wording, which we were instructed to ignore when evaluating correctness.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 100/100 questions. Current Accuracy: 0.7700\n",
      "Detailed results saved to evaluation_results_detailed.csv\n",
      "Average Precision: 0.4500\n",
      "Average Recall: 0.7092\n",
      "Average MRR: 0.7700\n",
      "Average F1: 0.5506\n",
      "End-to-End Accuracy: 0.7700\n",
      "Evaluation complete. Results saved to evaluation_results_level_two.json, evaluation_results_detailed_level_two.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SummaryIndexedVectorDB\n",
    "level_two_db = SummaryIndexedVectorDB(\"anthropic_docs_v2\")\n",
    "level_two_db.load_data('data/anthropic_summary_indexed_docs.json')\n",
    "\n",
    "# Run the evaluations\n",
    "avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs  = evaluate_retrieval(retrieve_level_two, eval_data, level_two_db)\n",
    "e2e_accuracy, e2e_results = evaluate_end_to_end(answer_query_level_two, level_two_db, eval_data)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'question': [item['question'] for item in eval_data],\n",
    "    'retrieval_precision': precisions,\n",
    "    'retrieval_recall': recalls,\n",
    "    'retrieval_mrr': mrrs,\n",
    "    'e2e_correct': e2e_results\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('evaluation/csvs/evaluation_results_detailed_level_two.csv', index=False)\n",
    "print(\"Detailed results saved to evaluation_results_detailed.csv\")\n",
    "\n",
    "# Print the results\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average MRR: {avg_mrr:.4f}\")\n",
    "print(f\"Average F1: {f1:.4f}\")\n",
    "print(f\"End-to-End Accuracy: {e2e_accuracy:.4f}\")\n",
    "\n",
    "# Save the results to a file\n",
    "with open('evaluation/json_results/evaluation_results_level_two.json', 'w') as f:\n",
    "    json.dump({\n",
    "        \"name\": \"Summary Indexing\",\n",
    "        \"average_precision\": avg_precision,\n",
    "        \"average_recall\": avg_recall,\n",
    "        \"average_f1\": f1,\n",
    "        \"average_mrr\": avg_mrr,\n",
    "        \"end_to_end_accuracy\": e2e_accuracy\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"Evaluation complete. Results saved to evaluation_results_level_two.json, evaluation_results_detailed_level_two.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_performance('evaluation/json_results', ['Basic RAG', 'Summary Indexing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 3: Re-ranking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `rerank_results` function uses Claude to reassess and reorder the initially retrieved documents:\n",
    "\n",
    "- It presents Claude with the query and summaries of all retrieved documents.\n",
    "\n",
    "- Claude is asked to select and rank the most relevant documents.\n",
    "\n",
    "- The function parses Claude's response to get the reranked document indices.\n",
    "\n",
    "- It includes fallback mechanisms in case of errors or insufficient results.\n",
    "\n",
    "- Finally, it assigns descending relevance scores to the reranked results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `retrieve_advanced` function implements the new retrieval pipeline:\n",
    "\n",
    "- We initially retrieve more documents than needed (default 20, configurable via initial_k) from the vector database.\n",
    "\n",
    "- We then use the rerank_results function to refine this larger set down to the most relevant documents (default 3, configurable via k).\n",
    "\n",
    "- Finally, it generates a new context string from these reranked documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluations show significant improvements:\n",
    "\n",
    "- Accuracy increased from 78% in our previous system to 85%.\n",
    "\n",
    "- Precision was improved by using our re-ranker to reduce the number of documents shown to the LLM.\n",
    "\n",
    "- MRR (Mean Reciprocal Rank) was likely improved by asking Claude to rank the relevance of each document in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def rerank_results(query: str, results: List[Dict], k: int = 5) -> List[Dict]:\n",
    "    # Prepare the summaries with their indices\n",
    "    summaries = []\n",
    "    print(len(results))\n",
    "\n",
    "    for i, result in enumerate(results):\n",
    "        summary = f\"[{i}] Document Summary: {result['metadata']['summary']}\"\n",
    "        summaries.append(summary)\n",
    "    joined_summaries = \"\\n\\n\".join(summaries)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "    You are about to be given a group of documents, each preceded by its index number in square brackets. Your task is to select the only {k} most relevant documents from the list to help us answer the query.\n",
    "    \n",
    "    <documents>\n",
    "    {joined_summaries}\n",
    "    </documents>\n",
    "\n",
    "    Output only the indices of {k} most relevant documents in order of relevance, separated by commas, enclosed in XML tags here:\n",
    "    <relevant_indices>put the numbers of your indices here, seeparted by commas</relevant_indices>\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-haiku-20240307\",\n",
    "            max_tokens=50,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}, {\"role\": \"assistant\", \"content\": \"<relevant_indices>\"}],\n",
    "            temperature=0,\n",
    "            stop_sequences=[\"</relevant_indices>\"]\n",
    "        )\n",
    "        \n",
    "        # Extract the indices from the response\n",
    "        response_text = response.content[0].text.strip()\n",
    "        indices_str = response_text\n",
    "        relevant_indices = []\n",
    "        for idx in indices_str.split(','):\n",
    "            try:\n",
    "                relevant_indices.append(int(idx.strip()))\n",
    "            except ValueError:\n",
    "                continue  # Skip invalid indices\n",
    "        print(indices_str)\n",
    "        print(relevant_indices)\n",
    "        # If we didn't get enough valid indices, fall back to the top k by original order\n",
    "        if len(relevant_indices) == 0:\n",
    "            relevant_indices = list(range(min(k, len(results))))\n",
    "        \n",
    "        # Ensure we don't have out-of-range indices\n",
    "        relevant_indices = [idx for idx in relevant_indices if idx < len(results)]\n",
    "        \n",
    "        # Return the reranked results\n",
    "        reranked_results = [results[idx] for idx in relevant_indices[:k]]\n",
    "        # Assign descending relevance scores\n",
    "        for i, result in enumerate(reranked_results):\n",
    "            result['relevance_score'] = 100 - i  # Highest score is 100, decreasing by 1 for each rank\n",
    "        \n",
    "        return reranked_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during reranking: {str(e)}\")\n",
    "        # Fall back to returning the top k results without reranking\n",
    "        return results[:k]\n",
    "\n",
    "def retrieve_advanced(query: str, db: SummaryIndexedVectorDB, k: int = 3, initial_k: int = 20) -> Tuple[List[Dict], str]:\n",
    "    # Step 1: Get initial results\n",
    "    initial_results = db.search(query, k=initial_k)\n",
    "\n",
    "    # Step 2: Re-rank results\n",
    "    reranked_results = rerank_results(query, initial_results, k=k)\n",
    "    \n",
    "    # Step 3: Generate new context string from re-ranked results\n",
    "    new_context = \"\"\n",
    "    for result in reranked_results:\n",
    "        chunk = result['metadata']\n",
    "        new_context += f\"\\n <document> \\n {chunk['chunk_heading']}\\n\\n{chunk['text']} \\n </document> \\n\"\n",
    "     \n",
    "    return reranked_results, new_context\n",
    "\n",
    "# The answer_query_advanced function remains unchanged\n",
    "def answer_query_advanced(query: str, db: SummaryIndexedVectorDB):\n",
    "    documents, context = retrieve_advanced(query, db)\n",
    "    prompt = f\"\"\"\n",
    "    You have been tasked with helping us to answer the following query: \n",
    "    <query>\n",
    "    {query}\n",
    "    </query>\n",
    "    You have access to the following documents which are meant to provide context as you answer the query:\n",
    "    <documents>\n",
    "    {context}\n",
    "    </documents>\n",
    "    Please remain faithful to the underlying context, and only deviate from it if you are 100% sure that you know the answer already. \n",
    "    Answer the question now, and avoid providing preamble such as 'Here is the answer', etc\n",
    "    \"\"\"\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=2500,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database loaded and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:   1%|          | 1/100 [00:01<01:49,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,2,7\n",
      "[0, 2, 7]\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:   2%|▏         | 2/100 [00:02<01:40,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2\n",
      "[0, 1, 2]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:   3%|▎         | 3/100 [00:02<01:32,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,13,15\n",
      "[1, 13, 15]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:   4%|▍         | 4/100 [00:03<01:27,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,6\n",
      "[0, 1, 6]\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:   5%|▌         | 5/100 [00:04<01:24,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2\n",
      "[0, 1, 2]\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:   6%|▌         | 6/100 [00:05<01:26,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2\n",
      "[0, 1, 2]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:   7%|▋         | 7/100 [00:06<01:29,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,5,11\n",
      "[0, 5, 11]\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:   8%|▊         | 8/100 [00:07<01:24,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,6\n",
      "[0, 1, 6]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:   9%|▉         | 9/100 [00:08<01:22,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,19,10\n",
      "[1, 19, 10]\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  10%|█         | 10/100 [00:09<01:22,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,0,1\n",
      "[2, 0, 1]\n",
      "Processed 10/100 items. Current Avg Precision: 0.5000, Avg Recall: 0.8000, Avg MRR: 1.0000\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  11%|█         | 11/100 [00:10<01:24,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,4,11\n",
      "[0, 4, 11]\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  12%|█▏        | 12/100 [00:11<01:18,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,3,2\n",
      "[0, 3, 2]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  13%|█▎        | 13/100 [00:11<01:16,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,3,6\n",
      "[4, 3, 6]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  14%|█▍        | 14/100 [00:13<01:20,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9,5,0\n",
      "[9, 5, 0]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  15%|█▌        | 15/100 [00:13<01:20,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,7,12\n",
      "[2, 7, 12]\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  16%|█▌        | 16/100 [00:14<01:18,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2\n",
      "[0, 1, 2]\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  17%|█▋        | 17/100 [00:15<01:15,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 3, 4\n",
      "[1, 3, 4]\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  18%|█▊        | 18/100 [00:16<01:15,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 5, 3\n",
      "[1, 5, 3]\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  19%|█▉        | 19/100 [00:17<01:12,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,3\n",
      "[0, 1, 3]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  20%|██        | 20/100 [00:18<01:11,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,17,18\n",
      "[0, 17, 18]\n",
      "Processed 20/100 items. Current Avg Precision: 0.4333, Avg Recall: 0.7250, Avg MRR: 0.9417\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  21%|██        | 21/100 [00:19<01:16,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,5,6\n",
      "[0, 5, 6]\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  22%|██▏       | 22/100 [00:20<01:16,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,8,9\n",
      "[1, 8, 9]\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  23%|██▎       | 23/100 [00:21<01:15,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2\n",
      "[0, 1, 2]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  24%|██▍       | 24/100 [00:22<01:13,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,11,14\n",
      "[0, 11, 14]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  25%|██▌       | 25/100 [00:23<01:12,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,14,16\n",
      "[0, 14, 16]\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  26%|██▌       | 26/100 [00:24<01:09,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,4\n",
      "[0, 1, 4]\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  27%|██▋       | 27/100 [00:25<01:06,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,3,4\n",
      "[1, 3, 4]\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  28%|██▊       | 28/100 [00:26<01:04,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2,4\n",
      "[1, 2, 4]\n",
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  29%|██▉       | 29/100 [00:26<01:01,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2,11\n",
      "[1, 2, 11]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  30%|███       | 30/100 [00:27<01:06,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,4,13\n",
      "[0, 4, 13]\n",
      "Processed 30/100 items. Current Avg Precision: 0.4667, Avg Recall: 0.7556, Avg MRR: 0.9389\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  31%|███       | 31/100 [00:28<01:04,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,3,4\n",
      "[0, 3, 4]\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  32%|███▏      | 32/100 [00:29<01:02,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2,0\n",
      "[1, 2, 0]\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  33%|███▎      | 33/100 [00:30<01:03,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,0,3\n",
      "[1, 0, 3]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  34%|███▍      | 34/100 [00:31<01:01,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 1, 3\n",
      "[0, 1, 3]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  35%|███▌      | 35/100 [00:32<00:59,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,7\n",
      "[0, 1, 7]\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  36%|███▌      | 36/100 [00:33<01:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2\n",
      "[0, 1, 2]\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  37%|███▋      | 37/100 [00:34<00:59,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,5,8\n",
      "[4, 5, 8]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  38%|███▊      | 38/100 [00:35<00:59,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,11,3\n",
      "[4, 11, 3]\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  39%|███▉      | 39/100 [00:37<01:12,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 0, 0\n",
      "[1, 0, 0]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  40%|████      | 40/100 [00:38<01:06,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,6,16\n",
      "[2, 6, 16]\n",
      "Processed 40/100 items. Current Avg Precision: 0.4667, Avg Recall: 0.7292, Avg MRR: 0.8875\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  41%|████      | 41/100 [00:39<01:02,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,5\n",
      "[0, 1, 5]\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  42%|████▏     | 42/100 [00:40<01:01,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,8,2\n",
      "[0, 8, 2]\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  43%|████▎     | 43/100 [00:41<00:59,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,9,0\n",
      "[1, 9, 0]\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  44%|████▍     | 44/100 [00:42<00:57,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,3\n",
      "[0, 1, 3]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  45%|████▌     | 45/100 [00:43<01:01,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,3,18\n",
      "[1, 3, 18]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  46%|████▌     | 46/100 [00:44<00:59,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,4,5\n",
      "[0, 4, 5]\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  47%|████▋     | 47/100 [00:45<00:56,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,3\n",
      "[0, 1, 3]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  48%|████▊     | 48/100 [00:46<00:52,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,0,3\n",
      "[1, 0, 3]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  49%|████▉     | 49/100 [00:47<00:52,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,1,0\n",
      "[2, 1, 0]\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  50%|█████     | 50/100 [00:48<00:52,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2\n",
      "[0, 1, 2]\n",
      "Processed 50/100 items. Current Avg Precision: 0.4600, Avg Recall: 0.7333, Avg MRR: 0.8733\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  51%|█████     | 51/100 [00:49<00:48,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,3\n",
      "[0, 1, 3]\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  52%|█████▏    | 52/100 [00:50<00:50,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,3,1\n",
      "[0, 3, 1]\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  53%|█████▎    | 53/100 [00:51<00:45,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3\n",
      "[1, 2, 3]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  54%|█████▍    | 54/100 [00:52<00:44,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 4, 5\n",
      "[1, 4, 5]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  55%|█████▌    | 55/100 [00:53<00:44,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,8\n",
      "[0, 1, 8]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  56%|█████▌    | 56/100 [00:54<00:46,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,2,6\n",
      "[0, 2, 6]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  57%|█████▋    | 57/100 [00:55<00:45,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,14,4\n",
      "[0, 14, 4]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  58%|█████▊    | 58/100 [00:56<00:42,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2\n",
      "[0, 1, 2]\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  59%|█████▉    | 59/100 [00:57<00:41,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,3\n",
      "[0, 1, 3]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  60%|██████    | 60/100 [00:58<00:39,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,5,15\n",
      "[1, 5, 15]\n",
      "Processed 60/100 items. Current Avg Precision: 0.4611, Avg Recall: 0.7444, Avg MRR: 0.8833\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  61%|██████    | 61/100 [00:59<00:42,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,4,1\n",
      "[2, 4, 1]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  62%|██████▏   | 62/100 [01:00<00:40,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,7,11\n",
      "[1, 7, 11]\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  63%|██████▎   | 63/100 [01:02<00:47,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,3\n",
      "[0, 1, 3]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  64%|██████▍   | 64/100 [01:03<00:43,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2,4\n",
      "[1, 2, 4]\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  65%|██████▌   | 65/100 [01:04<00:42,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,3,4\n",
      "[2, 3, 4]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  66%|██████▌   | 66/100 [01:05<00:39,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,15,12\n",
      "[2, 15, 12]\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  67%|██████▋   | 67/100 [01:06<00:36,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,3,4\n",
      "[1, 3, 4]\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  68%|██████▊   | 68/100 [01:07<00:33,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 2, 3\n",
      "[0, 2, 3]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  69%|██████▉   | 69/100 [01:08<00:30,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,1,3\n",
      "[2, 1, 3]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  70%|███████   | 70/100 [01:09<00:29,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,13,15\n",
      "[0, 13, 15]\n",
      "Processed 70/100 items. Current Avg Precision: 0.4429, Avg Recall: 0.7167, Avg MRR: 0.8548\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  71%|███████   | 71/100 [01:10<00:29,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,0,2\n",
      "[1, 0, 2]\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  72%|███████▏  | 72/100 [01:11<00:26,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2\n",
      "[0, 1, 2]\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  73%|███████▎  | 73/100 [01:12<00:27,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,3,8\n",
      "[0, 3, 8]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  74%|███████▍  | 74/100 [01:13<00:27,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3,1,16\n",
      "[3, 1, 16]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  75%|███████▌  | 75/100 [01:14<00:25,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,3,4\n",
      "[0, 3, 4]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  76%|███████▌  | 76/100 [01:15<00:23,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3,0,2\n",
      "[3, 0, 2]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  77%|███████▋  | 77/100 [01:16<00:22,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,18\n",
      "[0, 1, 18]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  78%|███████▊  | 78/100 [01:17<00:20,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,4,13\n",
      "[0, 4, 13]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  79%|███████▉  | 79/100 [01:18<00:21,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10,19,6\n",
      "[10, 19, 6]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  80%|████████  | 80/100 [01:19<00:19,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,10\n",
      "[0, 1, 10]\n",
      "Processed 80/100 items. Current Avg Precision: 0.4458, Avg Recall: 0.7208, Avg MRR: 0.8479\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  81%|████████  | 81/100 [01:20<00:19,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2,3,6\n",
      "[2, 3, 6]\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  82%|████████▏ | 82/100 [01:21<00:17,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 3, 9\n",
      "[0, 3, 9]\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  83%|████████▎ | 83/100 [01:22<00:16,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 2, 6\n",
      "[0, 2, 6]\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  84%|████████▍ | 84/100 [01:23<00:15,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 10, 0\n",
      "[1, 10, 0]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  85%|████████▌ | 85/100 [01:24<00:14,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,4,10\n",
      "[0, 4, 10]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  86%|████████▌ | 86/100 [01:25<00:13,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3,1,16\n",
      "[3, 1, 16]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  87%|████████▋ | 87/100 [01:26<00:12,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,5,11\n",
      "[0, 5, 11]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  88%|████████▊ | 88/100 [01:27<00:12,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,12,14\n",
      "[1, 12, 14]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  89%|████████▉ | 89/100 [01:28<00:10,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,11\n",
      "[0, 1, 11]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  90%|█████████ | 90/100 [01:29<00:10,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,3\n",
      "[0, 1, 3]\n",
      "Processed 90/100 items. Current Avg Precision: 0.4407, Avg Recall: 0.7111, Avg MRR: 0.8519\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  91%|█████████ | 91/100 [01:30<00:09,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2,4\n",
      "[1, 2, 4]\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  92%|█████████▏| 92/100 [01:31<00:07,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,5\n",
      "[0, 1, 5]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  93%|█████████▎| 93/100 [01:32<00:06,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2,11\n",
      "[1, 2, 11]\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  94%|█████████▍| 94/100 [01:33<00:05,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,2\n",
      "[0, 1, 2]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  95%|█████████▌| 95/100 [01:34<00:05,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 13, 14\n",
      "[1, 13, 14]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  96%|█████████▌| 96/100 [01:35<00:04,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,0,2\n",
      "[1, 0, 2]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  97%|█████████▋| 97/100 [01:36<00:02,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,4,15\n",
      "[1, 4, 15]\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  98%|█████████▊| 98/100 [01:37<00:02,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4,14,7\n",
      "[4, 14, 7]\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval:  99%|█████████▉| 99/100 [01:38<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[0]\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Retrieval: 100%|██████████| 100/100 [01:39<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,3,8\n",
      "[0, 3, 8]\n",
      "Processed 100/100 items. Current Avg Precision: 0.4433, Avg Recall: 0.7033, Avg MRR: 0.8500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "0,2,7\n",
      "[0, 2, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   1%|          | 1/100 [00:07<11:45,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It contains all the essential elements from the Correct Answer - namely that you need to click the 'Add Test Case' button and fill in values for variables to create different test cases. The Generated Answer actually provides more detail by mentioning you can re-run the evaluation suite, but this additional information doesn't contradict the core information. The key steps and process are fundamentally the same between both answers. There are no critical missing pieces or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "15\n",
      "0,1,2\n",
      "[0, 1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   2%|▏         | 2/100 [00:13<10:57,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the key points from the Correct Answer:\n",
      "1. It correctly identifies Voyage AI as Anthropic's recommended embeddings provider\n",
      "2. It mentions that Voyage AI offers customized/domain-specific models (including specific examples for finance and healthcare)\n",
      "3. It notes that Voyage AI provides bespoke fine-tuned models for individual customers\n",
      "\n",
      "While the Generated Answer provides more specific details about Voyage AI's model offerings than the Correct Answer, this additional information doesn't contradict anything in the Correct Answer - it simply elaborates further. The core substance and main points are aligned between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "1,13,15\n",
      "[1, 13, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   3%|▎         | 3/100 [00:22<12:32,  7.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It covers all the key success metrics mentioned in the Correct Answer (accuracy, F1 score, consistency, structure, speed, bias and fairness) and even provides more detailed explanations for each metric. Both answers emphasize the importance of choosing the right model to balance performance and latency requirements. While the Generated Answer goes into more detail about specific latency considerations (baseline latency and TTFT), this additional information doesn't contradict the Correct Answer but rather expands upon it. The core message about evaluating performance metrics and selecting an appropriate model to reduce latency while meeting performance requirements is consistent between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,1,6\n",
      "[0, 1, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   4%|▍         | 4/100 [00:30<12:16,  7.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures the same two key advantages of Claude for Sheets mentioned in the Correct Answer:\n",
      "\n",
      "1. Both answers highlight the ability to test prompts in parallel across evaluation suites, noting this is more efficient than sequential chained prompts.\n",
      "\n",
      "2. Both answers mention that Claude for Sheets is better suited for office tasks like survey analysis and data processing compared to chained prompts.\n",
      "\n",
      "The Generated Answer essentially conveys the same information as the Correct Answer, just with slightly different phrasing. There are no missing critical pieces of information or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "8\n",
      "0,1,2\n",
      "[0, 1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   5%|▌         | 5/100 [00:35<10:59,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same core information - that missing \"\\n\\nHuman:\" and \"\\n\\nAssistant:\" turns in the Text Completions API prompt will result in an API error. The Generated Answer provides some additional context about formatting and ordering, but this extra detail doesn't contradict or detract from the main point. The essential message about what happens when these turns are missing is identical between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "11\n",
      "0,1,2\n",
      "[0, 1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   6%|▌         | 6/100 [00:44<11:47,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures all the key points from the Correct Answer and even provides additional detail while maintaining the same core message. Both answers emphasize that:\n",
      "\n",
      "1. Tool use requests are priced the same way as regular requests (based on total tokens)\n",
      "2. There are additional tokens involved in tool use requests\n",
      "3. These additional tokens come from various sources (tools parameter, tool use blocks, tool results, system prompt)\n",
      "4. All these additional tokens contribute to the total cost\n",
      "\n",
      "The Generated Answer actually provides more granular detail about the token sources and even mentions specific token ranges for the system prompt, but this additional detail doesn't contradict the Correct Answer - it simply elaborates on it. The fundamental message about how tool use impacts pricing is consistent between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,5,11\n",
      "[0, 5, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   7%|▋         | 7/100 [00:49<10:36,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It contains all the essential information from the Correct Answer - specifically the release date (June 27th, 2024) and what features will be available (API usage, billing details, and rate limits). While the Correct Answer provides slightly more detail by mentioning the specific tabs (Usage, Cost, and Rate Limits), this is a minor detail that doesn't change the core meaning. Both answers convey the same fundamental information about what will be available and when.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "9\n",
      "0,1,6\n",
      "[0, 1, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   8%|▊         | 8/100 [00:57<10:49,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures both key factors mentioned in the Correct Answer:\n",
      "\n",
      "1. It addresses the need to consider whether the task requires in-depth thinking that a human would need to work through (mentioned in both answers)\n",
      "\n",
      "2. It acknowledges the impact on latency due to increased output length (mentioned in both answers)\n",
      "\n",
      "While the Generated Answer provides more detail and elaboration around these points (like mentioning TTFT and giving specific examples of complex tasks), the core substance matches the Correct Answer. The additional detail doesn't contradict the Correct Answer but rather expands upon it in a helpful way.\n",
      "\n",
      "There are no critical pieces of information from the Correct Answer missing in the Generated Answer, and there are no contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "1,19,10\n",
      "[1, 19, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:   9%|▉         | 9/100 [01:05<11:04,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is essentially correct. While it provides much more detailed steps and implementation guidance than the Correct Answer, the core concept remains the same - that Claude can be used to summarize PDF documents to make them easier to digest. The Generated Answer expands on HOW to do this, but doesn't contradict or miss any critical information from the Correct Answer. Both answers convey the fundamental capability of Claude to help users understand PDF content through summarization. The additional detail in the Generated Answer doesn't change or contradict the basic truth presented in the Correct Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "10\n",
      "2,0,1\n",
      "[2, 0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  10%|█         | 10/100 [01:10<09:58,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers indicate that you can view the API rate limits in a Rate Limits tab within Anthropic's console interface. The only difference is minor wording variations (\"Developer Console\" vs \"Anthropic Console\" and the addition of the word \"new\"), but the core information about being able to view rate limits in a dedicated Rate Limits tab is consistent between both answers. There are no critical missing pieces of information or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 10/100 questions. Current Accuracy: 1.0000\n",
      "20\n",
      "0,4,11\n",
      "[0, 4, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  11%|█         | 11/100 [01:18<10:34,  7.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it misses a critical piece of information from the correct answer - specifically, the measurement of average cost per classification. While the generated answer does mention the 95th percentile response time (which matches the correct answer) and provides many additional metrics that could be useful, it fails to mention cost as a key performance metric. Since the correct answer explicitly states that both response time AND cost per classification are important metrics beyond accuracy, and the generated answer omits cost entirely, it must be marked as incorrect despite providing other potentially valuable metrics.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n",
      "8\n",
      "0,3,2\n",
      "[0, 3, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  12%|█▏        | 12/100 [01:27<11:14,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the key distinction between how system prompts are specified in both APIs:\n",
      "\n",
      "1. For Text Completions API: Both answers indicate that the system prompt goes before the first \"\\n\\nHuman:\" turn in the prompt text\n",
      "2. For Messages API: Both answers specify that the system prompt is set using a dedicated \"system\" parameter in the API request\n",
      "\n",
      "The Generated Answer actually provides helpful concrete examples to illustrate these concepts, which goes beyond but doesn't contradict the Correct Answer. The substance and core information about how to specify system prompts in both APIs is consistent between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "4,3,6\n",
      "[4, 3, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:XML parsing error: mismatched tag: line 13, column 2\n",
      "Evaluating End-to-End:  13%|█▎        | 13/100 [01:35<11:09,  7.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>\n",
      "The Generated Answer is essentially correct as it captures the key concept of combining XML tags with chain of thought reasoning to create structured prompts. Both answers emphasize:\n",
      "\n",
      "1. The use of XML tags for structure (<thinking>, <answer>, etc.)\n",
      "2. The incorporation of step-by-step reasoning processes\n",
      "3. The goal of creating clear, well-structured prompts for better performance\n",
      "\n",
      "While the Generated Answer provides more specific details about nesting tags and maintaining consistency, and the Correct Answer includes a specific example of a prompt instruction, these are just different ways of explaining the same core concept. There are no contradictions between the answers, and no critical information from the Correct Answer is missing from the Generated Answer.\n",
      "\n",
      "The Generated Answer even demonstrates the concept it's explaining by using the XML tags and chain of thought structure in its own response, which reinforces the correct understanding of the concept.\n",
      "</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "9,5,0\n",
      "[9, 5, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  14%|█▍        | 14/100 [01:40<10:06,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It contains all the same key information as the Correct Answer, just presented in a slightly different format:\n",
      "\n",
      "1. Both answers identify the same three metrics being measured\n",
      "2. Both provide the exact same values for each metric:\n",
      "   - 89.01% accuracy\n",
      "   - 1.61 seconds for 95th percentile response time\n",
      "   - $0.0004 for average cost per request/classification\n",
      "\n",
      "The only difference is in minor wording choices (e.g., \"Classification\" vs \"request routing\") which doesn't affect the substantive meaning. The Generated Answer captures all the critical information from the Correct Answer without any contradictions or omissions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "2,7,12\n",
      "[2, 7, 12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  15%|█▌        | 15/100 [01:47<09:58,  7.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It contains all the key elements from the Correct Answer:\n",
      "1. Having clear success criteria\n",
      "2. Having ways to empirically test against those criteria\n",
      "3. Having a first draft prompt to improve\n",
      "\n",
      "The Generated Answer actually provides slightly more detail by mentioning specific documentation sections, but the core substance perfectly matches the Correct Answer. There are no contradictions or missing critical pieces of information. The minor differences in phrasing (like listing the items with numbers vs. combining them in a sentence) don't affect the correctness of the answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "7\n",
      "0,1,2\n",
      "[0, 1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  16%|█▌        | 16/100 [01:54<09:32,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the key distinction between how mid-response prompting works in both APIs:\n",
      "1. For Text Completions API: You can pre-fill part of Claude's response in the prompt\n",
      "2. For Messages API: You continue responses by setting the last message to have the \"assistant\" role\n",
      "\n",
      "The Generated Answer conveys the same essential information as the Correct Answer, just with slightly different wording. There are no missing critical pieces of information and no contradictions between the two answers. The substance and technical accuracy are equivalent.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "19\n",
      "1, 3, 4\n",
      "[1, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  17%|█▋        | 17/100 [02:02<10:10,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same core message: that Claude's responses are more insightful, structured, and actionable when given a specific role (CFO) compared to not having a role. The Generated Answer actually provides more detailed examples and specifics about how the role-based response differs, but this additional detail doesn't contradict the Correct Answer - it simply elaborates on it. The key points about the response being more structured, insightful, and actionable are present in both answers. There are no critical omissions or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "9\n",
      "1, 5, 3\n",
      "[1, 5, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  18%|█▊        | 18/100 [02:11<10:28,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it covers all the essential elements from the Correct Answer and even provides additional helpful detail. Both answers mention key quantitative metrics like F1 score, accuracy, precision, and recall. Both answers also emphasize that specific targets should be determined based on industry benchmarks, prior experiments, and expert knowledge. The Generated Answer expands on this with more specific examples and additional metrics like response time and uptime, but this additional detail doesn't contradict the core message of the Correct Answer. The substance and main points align between both answers, with the Generated Answer effectively elaborating on the core concepts presented in the Correct Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "5\n",
      "0,1,3\n",
      "[0, 1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:XML parsing error: mismatched tag: line 9, column 182\n",
      "Evaluating End-to-End:  19%|█▉        | 19/100 [02:15<09:13,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the key elements from the Correct Answer:\n",
      "1. The core concept of combining XML tags with other prompt engineering techniques\n",
      "2. Specifically mentions multishot prompting using <examples> tags\n",
      "3. Mentions chain of thought using <thinking> and <answer> tags\n",
      "4. Notes that this creates \"super-structured, high-performance prompts\"\n",
      "\n",
      "While the wording is slightly different, the substance and meaning are identical. There are no missing critical pieces of information and no contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,17,18\n",
      "[0, 17, 18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  20%|██        | 20/100 [02:23<09:26,  7.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures all the essential elements from the Correct Answer and even provides additional helpful detail while maintaining the core concepts. Both answers emphasize:\n",
      "\n",
      "1. The need to provide a detailed rubric\n",
      "2. The importance of giving the LLM both the output to grade and the rubric\n",
      "3. Having the LLM evaluate against the criteria\n",
      "4. Getting a simple correct/incorrect result\n",
      "\n",
      "While the Generated Answer is more detailed and breaks down the process into more specific steps, it doesn't contradict anything in the Correct Answer. Rather, it expands upon the same fundamental approach. The additional detail about aggregating results and structuring prompts simply provides more implementation guidance while staying true to the core concept presented in the Correct Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 20/100 questions. Current Accuracy: 0.8500\n",
      "9\n",
      "0,5,6\n",
      "[0, 5, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  21%|██        | 21/100 [02:33<10:14,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer contains all the essential steps from the correct answer and actually provides more detailed information. The core steps are the same:\n",
      "1. Subscribe to the model package on AWS Marketplace\n",
      "2. Select and agree to terms\n",
      "3. Get the Product ARN for your region\n",
      "4. Create a JupyterLab space in SageMaker Studio\n",
      "5. Upload and follow Voyage's notebook for deployment\n",
      "\n",
      "While the generated answer includes additional information about alternative methods (HTTP API and Python package), this extra information doesn't contradict the correct answer - it just provides additional deployment options. The fundamental process for AWS Marketplace deployment matches the correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "17\n",
      "1,8,9\n",
      "[1, 8, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  22%|██▏       | 22/100 [02:39<09:43,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect because it misses several critical elements from the correct answer and provides different guidance. The key missing elements are:\n",
      "\n",
      "1. The instruction to use a single tool (not multiple tools)\n",
      "2. The need to set tool_choice to explicitly instruct the model to use that tool\n",
      "3. The importance of writing tool descriptions from the model's perspective\n",
      "\n",
      "The generated answer instead focuses on different aspects like providing detailed descriptions, prioritizing descriptions over examples, and defining output formats. While these might be helpful general practices, they don't address the specific requirements for using tools to get JSON output as outlined in the correct answer. The generated answer doesn't contradict the correct answer directly, but it fails to include the essential technical requirements that are core to the correct implementation.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n",
      "16\n",
      "0,1,2\n",
      "[0, 1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  23%|██▎       | 23/100 [02:47<09:41,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct and actually provides more detailed information than the Correct Answer while maintaining all the key points. The Generated Answer covers all the essential elements mentioned in the Correct Answer:\n",
      "1. Vision capabilities\n",
      "2. Improved speed/performance\n",
      "3. Enhanced intelligence/capabilities\n",
      "4. More recent/up-to-date capabilities (implied by it being the newer model)\n",
      "\n",
      "The Generated Answer adds additional specific details about context windows and pricing, but these additions don't contradict the Correct Answer - they simply provide more detail. The core substance of both answers aligns perfectly, with the Generated Answer essentially being a more detailed expansion of the key points in the Correct Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,11,14\n",
      "[0, 11, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  24%|██▍       | 24/100 [02:53<08:50,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers emphasize the same key point - that using examples helps reduce misinterpretation of instructions and leads to more accurate outputs from Claude. While the Generated Answer includes additional benefits (enforcing uniform structure/style and helping with complex tasks), it fully encompasses the core benefit mentioned in the Correct Answer. There are no contradictions between the two answers, and the Generated Answer doesn't miss any critical information from the Correct Answer - it actually provides more detail while maintaining the same fundamental point.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,14,16\n",
      "[0, 14, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  25%|██▌       | 25/100 [02:59<08:24,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it focuses on different advantages than what is specified in the Correct Answer. The Correct Answer emphasizes the ability to adapt models through providing domain-specific context in prompts without retraining, while the Generated Answer focuses on resource efficiency and cost-effectiveness. While the Generated Answer may state valid benefits of prompt engineering, it misses the key advantage specified in the Correct Answer about being able to easily adapt models through contextual prompts. The answers are discussing different aspects and advantages, making them substantively different.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n",
      "15\n",
      "0,1,4\n",
      "[0, 1, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  26%|██▌       | 26/100 [03:05<08:00,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. While it provides more detailed steps than the Correct Answer, the core information about using Anthropic's pre-made template by making a copy of the Claude for Sheets workbook template is present and accurate. The additional information about installation and API key setup doesn't contradict the core message, it simply provides extra context. The essential point about using a pre-made template by making a copy is preserved in both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "6\n",
      "1,3,4\n",
      "[1, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  27%|██▋       | 27/100 [03:11<07:38,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures the key concepts present in the Correct Answer. Both answers explain that:\n",
      "\n",
      "1. The \"index\" field identifies which content block the delta applies to\n",
      "2. Multiple deltas can be associated with the same content block\n",
      "3. The index is used to track/identify which part of the response is being updated\n",
      "\n",
      "While the Generated Answer uses slightly different wording and structure, it conveys the same fundamental meaning as the Correct Answer. There are no contradictions between the two answers, and no critical information is missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "9\n",
      "1,2,4\n",
      "[1, 2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  28%|██▊       | 28/100 [03:17<07:24,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same key information about:\n",
      "1. Using base64-encoded images in API requests\n",
      "2. Supporting the same image formats (JPEG, PNG, GIF, WebP)\n",
      "\n",
      "The Generated Answer actually provides additional details about image limits and file sizes, but this extra information doesn't contradict the Correct Answer - it simply provides more context. The core substance about how to include images and supported formats matches between both answers.\n",
      "\n",
      "There are no critical pieces of information from the Correct Answer that are missing in the Generated Answer, and there are no contradictions between them.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "18\n",
      "1,2,11\n",
      "[1, 2, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  29%|██▉       | 29/100 [03:23<07:19,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the key relationship between TTFT and latency, accurately explaining that TTFT is a component/subset of overall latency. Both answers convey that TTFT specifically measures the time to generate the first token of a response. While the Generated Answer provides additional details about factors affecting latency and TTFT, these extras don't contradict the Correct Answer - they merely elaborate on it. The core concept that TTFT is a specific measure of latency focused on the first token generation time is preserved in both answers. The Generated Answer also maintains the connection to model responsiveness, though it expresses this through practical examples rather than explicitly stating it's important for interactive applications.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,4,13\n",
      "[0, 4, 13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  30%|███       | 30/100 [03:30<07:41,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it conveys the same core message as the Correct Answer. Both answers emphasize that providing examples of edge cases to Claude can improve its performance in routing support tickets. The Generated Answer actually expands on the concept by providing more detailed examples and explanations of how this works in different scenarios (implicit requests, emotional prioritization, intent vs. routing, and issue prioritization). While it provides more detail than the Correct Answer, it doesn't contradict it and maintains the same fundamental point about examples improving Claude's performance in handling edge cases for support ticket routing. The substance and main message are aligned between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 30/100 questions. Current Accuracy: 0.8333\n",
      "9\n",
      "0,3,4\n",
      "[0, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  31%|███       | 31/100 [03:38<07:55,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures all the essential elements of the Correct Answer. Both answers describe:\n",
      "\n",
      "1. That the stop_reason of \"tool_use\" indicates Claude's determination to use an external tool\n",
      "2. That Claude constructs a tool use request when this happens\n",
      "3. That the tool input needs to be extracted and executed client-side/externally\n",
      "4. That the results need to be sent back to Claude\n",
      "\n",
      "While the Generated Answer provides slightly more detail and structures the information differently, it doesn't contradict the Correct Answer and contains all the critical information. The core workflow and the role of the stop_reason=\"tool_use\" signal is accurately represented in both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "9\n",
      "1,2,0\n",
      "[1, 2, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  32%|███▏      | 32/100 [03:43<07:07,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It contains all the critical information present in the Correct Answer, including:\n",
      "1. The specific error event type (\"overloaded_error\")\n",
      "2. The context (during periods of high usage)\n",
      "3. The corresponding HTTP error code (529)\n",
      "4. The fact that this applies to streaming responses\n",
      "5. The distinction that the HTTP code would apply in a non-streaming context\n",
      "\n",
      "The Generated Answer is essentially a restatement of the Correct Answer with very minor wording differences that don't affect the substance of the response.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "6\n",
      "1,0,3\n",
      "[1, 0, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  33%|███▎      | 33/100 [03:53<08:13,  7.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. While it correctly identifies \"text delta\" as one type, it fails to correctly identify \"input_json_delta\" as the second type. Instead, it states that the second type is \"Unknown (not specified in the provided documents)\". This is a direct contradiction with the Correct Answer which clearly specifies both types. Since the Generated Answer misses one of the two critical pieces of information and contradicts the Correct Answer by claiming uncertainty about the second type, it cannot be considered correct.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0, 1, 3\n",
      "[0, 1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  34%|███▍      | 34/100 [03:58<07:19,  6.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is incorrect. While it correctly states that Claude 3.5 Sonnet became generally available on June 20th, 2024, it fails to mention the separate date for tool use availability (May 30th, 2024). The correct answer clearly indicates these were two separate events with different dates. This is a critical piece of missing information, as the question specifically asked about both Claude 3.5 Sonnet AND tool use availability dates. The generated answer only addresses one of these two important elements.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,1,7\n",
      "[0, 1, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  35%|███▌      | 35/100 [04:05<07:20,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It conveys the same essential information as the Correct Answer - that Anthropic launched Claude.ai and the Claude iOS app first in Europe in May 2024, followed by Canada in June 2024. While the Generated Answer includes specific dates (May 13th and June 5th) that aren't in the Correct Answer, and mentions the API for Canada's launch, these are just additional details that don't contradict the core sequence of events. The fundamental ordering and timing (Europe in May, Canada in June) matches perfectly between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "16\n",
      "0,1,2\n",
      "[0, 1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  36%|███▌      | 36/100 [04:13<07:42,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures all the essential elements from the Correct Answer:\n",
      "\n",
      "1. It correctly explains that a \"tool_use\" stop_reason indicates Claude has decided to use a tool\n",
      "2. It outlines the same key steps needed to continue the conversation:\n",
      "   - Extracting the tool name and input from Claude's request\n",
      "   - Executing the tool code client-side\n",
      "   - Sending back results via a tool_result content block\n",
      "\n",
      "The Generated Answer even provides slightly more detail by mentioning that Claude will analyze the results to formulate a final response, but this additional information doesn't contradict the Correct Answer. The core substance and workflow described in both answers is identical, just expressed with slightly different wording.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "10\n",
      "4,5,8\n",
      "[4, 5, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  37%|███▋      | 37/100 [04:19<07:21,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. While it correctly mentions the anthropic library, which is the key library mentioned in the Correct Answer, it adds additional libraries (time and typing) that are not mentioned in the Correct Answer. More importantly, it contradicts the purpose stated in the Correct Answer by saying \"The code snippet does not directly evaluate tone and style, but rather focuses on evaluating the accuracy, response time, and cost.\" The Correct Answer clearly states that the code snippet is for \"evaluating tone and style in a customer service chatbot.\" This represents a direct contradiction between the two answers regarding the fundamental purpose of the code snippet.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "4,11,3\n",
      "[4, 11, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  38%|███▊      | 38/100 [04:26<07:10,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. It describes authentication methods for the standard Anthropic API, not for accessing Claude models through Amazon Bedrock. The correct authentication methods involve AWS credentials (either direct credentials or using AWS credential providers), while the Generated Answer talks about using ANTHROPIC_API_KEY. These are fundamentally different authentication approaches since Bedrock requires AWS-specific credentials. The Generated Answer shows no awareness of AWS authentication requirements and instead provides completely different, incorrect authentication methods.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n",
      "2\n",
      "1, 0, 0\n",
      "[1, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  39%|███▉      | 39/100 [04:33<07:08,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the same two key factors mentioned in the Correct Answer:\n",
      "\n",
      "1. The risk/potential of prompt leaks\n",
      "2. The impact on model performance due to added complexity\n",
      "\n",
      "While the Generated Answer provides more detail and elaboration, the core substance matches the Correct Answer perfectly - both emphasize the need to balance leak prevention against potential performance degradation. There are no contradictions between the two answers, and no critical information is missing from the Generated Answer. The differences are merely in the level of detail and specific wording used, not in the fundamental concepts being conveyed.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "2,6,16\n",
      "[2, 6, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  40%|████      | 40/100 [04:41<07:07,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures the core concept from the Correct Answer - that selecting the appropriate Claude model helps optimize for speed/latency based on specific use case requirements. While the Generated Answer provides more detailed breakdown and examples, the fundamental message aligns with the Correct Answer about choosing the right model to balance speed, intelligence and specific needs. There are no contradictions between the two answers, and the Generated Answer doesn't miss any critical information from the Correct Answer. In fact, it expands upon the core concept in a complementary way.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 40/100 questions. Current Accuracy: 0.7750\n",
      "20\n",
      "0,1,5\n",
      "[0, 1, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  41%|████      | 41/100 [04:47<06:46,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It contains all the essential information from the Correct Answer and even provides more detailed implementation examples. Both answers highlight the key points that:\n",
      "\n",
      "1. You use the client.messages.stream() method\n",
      "2. You iterate over the stream.text_stream attribute in a for loop\n",
      "\n",
      "The Generated Answer expands on this with a practical code example and additional context, but the core information matches perfectly with the Correct Answer. There are no contradictions or missing critical pieces between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "11\n",
      "0,8,2\n",
      "[0, 8, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  42%|████▏     | 42/100 [04:53<06:12,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same key information:\n",
      "\n",
      "1. Both explain that you can guide/shape Claude's response by pre-filling part of it in the messages list\n",
      "2. Both identify \"max_tokens\" as the parameter used to generate short responses\n",
      "3. Both explain that max_tokens limits the length of the response\n",
      "\n",
      "While the exact wording differs slightly (e.g., \"last position\" vs \"content field\"), and the Correct Answer specifically mentions \"1\" as an example value while the Generated Answer is more general about limiting tokens/words, these are minor differences that don't change the fundamental correctness of the answer. The Generated Answer captures all the critical information present in the Correct Answer without any contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "12\n",
      "1,9,0\n",
      "[1, 9, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  43%|████▎     | 43/100 [04:59<06:01,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it aligns with the core message of the Correct Answer. Both answers emphasize that having a larger volume of test cases with automated grading is preferable to having fewer high-quality human-graded test cases. The Generated Answer expands on this by providing additional reasoning and context (scalability, reliability, coverage), but the fundamental conclusion is the same. There are no contradictions between the two answers, and no critical information from the Correct Answer is missing from the Generated Answer. While the Generated Answer is more detailed, it maintains the same essential point about prioritizing quantity with automated grading over fewer human-graded cases.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "4\n",
      "0,1,3\n",
      "[0, 1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  44%|████▍     | 44/100 [05:03<05:23,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. According to the Correct Answer, the two required fields are \"index\" and \"delta\", where \"delta\" contains the \"type\" and \"text\". The Generated Answer incorrectly states that the two required fields are \"type\" and \"text\". This is a substantive difference, not just a wording variation, as it misidentifies the top-level required fields in the event structure. The Generated Answer is missing the critical \"index\" field requirement and incorrectly elevates \"type\" and \"text\" (which are actually nested within the \"delta\" field) to be the main required fields.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "1,3,18\n",
      "[1, 3, 18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  45%|████▌     | 45/100 [05:08<05:09,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the two key interactive learning methods mentioned in the Correct Answer:\n",
      "\n",
      "1. The Anthropic Cookbook with its interactive Jupyter notebooks for learning capabilities like PDF uploads and embeddings\n",
      "2. The Developer Console with its prompt generator tool for easier prompting\n",
      "\n",
      "While the Generated Answer provides some additional details (like mentioning VoyageAI and Workbench), the core substance matches the Correct Answer. There are no contradictions or missing critical pieces of information between the two answers. The differences are merely in the level of detail provided, not in the fundamental content.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,4,5\n",
      "[0, 4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  46%|████▌     | 46/100 [05:13<04:51,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. The core concept from the Correct Answer - that breaking tasks into subtasks improves accuracy because each subtask gets Claude's full attention and reduces errors compared to handling everything at once - is fully captured in the Generated Answer's first point about accuracy. While the Generated Answer includes additional points about clarity and traceability, these are supplementary details that don't contradict the core concept. The fundamental explanation for why chained prompts improve accuracy is consistent between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "6\n",
      "0,1,3\n",
      "[0, 1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  47%|████▋     | 47/100 [05:19<04:49,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. The key point from the Correct Answer - that Messages streaming responses can contain multiple content blocks of varying types, making them more complex than Text Completions (which only include completion, ping, and error server-sent-events) - is accurately captured in point #1 of the Generated Answer. While the Generated Answer provides additional details about input formats and response structures in points #2 and #3, these extra details don't contradict the Correct Answer and simply provide supplementary information. The core distinction between the streaming formats is properly conveyed.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "1,0,3\n",
      "[1, 0, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  48%|████▊     | 48/100 [05:24<04:42,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. While it correctly identifies claude.ai and the web Console as one way to experiment with Claude (matching the Correct Answer), it adds a second method about following the Quickstart guide for API calls that is not mentioned in the Correct Answer. Since the question specifically asks about what is stated in Anthropic's documentation, and the Correct Answer only mentions claude.ai and the web Console, adding this additional method makes the Generated Answer incorrect. We should stick strictly to what is confirmed in the Correct Answer as being documented by Anthropic.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "2,1,0\n",
      "[2, 1, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  49%|████▉     | 49/100 [05:30<04:46,  5.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same core concept that chain prompts help reduce errors by breaking complex tasks into smaller, more manageable subtasks that Claude can focus on individually. The Generated Answer expands on this basic principle with additional details and examples, but it doesn't contradict or omit any critical information from the Correct Answer. The fundamental mechanism described (breaking tasks down to reduce errors) is consistent between both answers. While the Generated Answer provides more detail about specific benefits like traceability and clarity, these are natural extensions of the core concept rather than contradictions.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "4\n",
      "0,1,2\n",
      "[0, 1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  50%|█████     | 50/100 [05:34<04:16,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers state that an overloaded_error event corresponds to HTTP status code 529 in a non-streaming context for the Anthropic API. While the Correct Answer uses slightly more formal language (\"would normally correspond to\"), the core information - the 529 status code - is identical in both answers. The difference in phrasing does not change the fundamental meaning or accuracy of the response.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 50/100 questions. Current Accuracy: 0.7800\n",
      "8\n",
      "0,1,3\n",
      "[0, 1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  51%|█████     | 51/100 [05:39<04:05,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the same two key ways to specify the embedding format as described in the Correct Answer:\n",
      "\n",
      "1. Both answers indicate that leaving the format unspecified will return embeddings as lists of floating-point numbers\n",
      "2. Both answers state that setting the format to \"base64\" will return the embeddings in Base64 encoded format\n",
      "\n",
      "The Generated Answer presents the same information in a slightly more structured format with bullet points, but the substance and technical details are identical to the Correct Answer. There are no missing critical pieces of information or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "4\n",
      "0,3,1\n",
      "[0, 3, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  52%|█████▏    | 52/100 [05:47<04:40,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it conveys the same essential information as the Correct Answer. Both answers explain that:\n",
      "\n",
      "1. Tool use content blocks are sent as partial JSON strings in content_block_delta events\n",
      "2. These partial JSON strings need to be accumulated by the client\n",
      "3. The complete JSON can be parsed once a content_block_stop event is received\n",
      "4. Parsing can be done using Pydantic or SDK helpers\n",
      "\n",
      "The Generated Answer actually provides more detail with an example of the event format, but this additional information doesn't contradict the Correct Answer. The core concepts and process described in both answers align completely.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "17\n",
      "1, 2, 3\n",
      "[1, 2, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  53%|█████▎    | 53/100 [05:52<04:23,  5.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It accurately identifies both tutorials (GitHub and Google Sheets) and correctly characterizes their key differences. The Generated Answer maintains the same core information as the Correct Answer - that the GitHub tutorial is more in-depth with examples, while the Google Sheets tutorial is lighter-weight. While the Generated Answer provides slightly more detail in its explanation of the differences, the substance and main points are entirely consistent with the Correct Answer. There are no contradictions or missing critical pieces of information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "1, 4, 5\n",
      "[1, 4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  54%|█████▍    | 54/100 [05:59<04:33,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct and actually provides more comprehensive detail than the Correct Answer. It covers all the key points mentioned in the Correct Answer:\n",
      "\n",
      "1. The 200K token context window\n",
      "2. Tool use capabilities for integration with specialized applications\n",
      "3. Multimodal input capabilities\n",
      "4. Enterprise-grade security and data handling for sensitive information\n",
      "\n",
      "The Generated Answer expands on these points and provides additional relevant details about Claude's enterprise capabilities, but does not contradict any information in the Correct Answer. While it is more detailed, it maintains the same core message about Claude's suitability for enterprise use cases requiring specialized integration and handling of sensitive data.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,1,8\n",
      "[0, 1, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  55%|█████▌    | 55/100 [06:03<04:02,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect because it omits a key region where Claude.ai API and iOS app are available - the United States. While the Generated Answer correctly mentions Canada and Europe, leaving out the United States represents a significant omission of information. The availability in the United States is a critical piece of information present in the Correct Answer but missing from the Generated Answer. Therefore, despite getting two regions correct, the Generated Answer is incomplete and thus incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,2,6\n",
      "[0, 2, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  56%|█████▌    | 56/100 [06:09<04:06,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures all the key points from the Correct Answer and even provides additional helpful detail while maintaining the same core information:\n",
      "\n",
      "1. It correctly identifies the two main approaches (push-based with webhooks and pull-based)\n",
      "2. It accurately describes that push-based is more scalable but has security implications due to requiring a public endpoint\n",
      "3. It correctly states that pull-based is easier to implement but has the drawback of making unnecessary calls to the ticket system\n",
      "\n",
      "The Generated Answer expands on these points with more implementation details, but the fundamental information matches the Correct Answer without any contradictions or missing critical pieces.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,14,4\n",
      "[0, 14, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  57%|█████▋    | 57/100 [06:13<03:39,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is completely correct. It contains all the key information from the Correct Answer: the release date (May 10th, 2024), what was released (a prompt generator tool), and where it's available (through the Developer Console). The wording is slightly different but conveys exactly the same information and meaning. There are no missing critical pieces of information or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,1,2\n",
      "[0, 1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  58%|█████▊    | 58/100 [06:17<03:20,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same core message that Claude 3 Sonnet provides the optimal balance between intelligence and speed for high-throughput tasks, specifically mentioning sales forecasting and targeted marketing as examples. The Generated Answer actually provides slightly more detail by directly quoting from the documentation, but the fundamental meaning is identical to the Correct Answer. There are no contradictions or missing critical information between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "7\n",
      "0,1,3\n",
      "[0, 1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  59%|█████▉    | 59/100 [06:22<03:25,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It contains all the key information from the Correct Answer and even provides additional helpful context and implementation details. The core points that:\n",
      "1. Similarity can be calculated using dot product\n",
      "2. This is equivalent to cosine similarity\n",
      "3. This equivalence is due to Voyage embeddings being normalized to length 1\n",
      "\n",
      "are all present and accurately explained. The additional explanation about why dot product equals cosine similarity for normalized vectors and the code example are helpful additions that don't contradict the core correct answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "1,5,15\n",
      "[1, 5, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  60%|██████    | 60/100 [06:29<03:41,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures the key substance of the Correct Answer, just with more detail and elaboration. Both answers emphasize that examples help improve Claude's performance on complex tasks by:\n",
      "\n",
      "1. Providing better guidance and context (reducing misinterpretation as mentioned in the Correct Answer)\n",
      "2. Helping maintain consistent structure/format (mentioned in both answers)\n",
      "3. Leading to more accurate and desired outputs (mentioned in both answers)\n",
      "\n",
      "While the Generated Answer goes into more specific detail about things like reducing hallucinations and breaking down complex tasks, it doesn't contradict the Correct Answer. Rather, it expands upon the same core concepts. The fundamental message about examples improving performance through better guidance, consistency, and accuracy is preserved between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 60/100 questions. Current Accuracy: 0.8000\n",
      "6\n",
      "2,4,1\n",
      "[2, 4, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  61%|██████    | 61/100 [06:34<03:24,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It accurately identifies and describes both types of content block deltas:\n",
      "\n",
      "1. Text deltas containing the \"text\" field for text content updates\n",
      "2. Input JSON deltas containing the \"partial_json\" field for JSON input updates\n",
      "\n",
      "While the wording is slightly different from the Correct Answer, the substance and key information is the same. The Generated Answer effectively communicates that these deltas represent partial/incremental updates to their respective content types (text and JSON input). There are no missing critical pieces of information or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "1,7,11\n",
      "[1, 7, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  62%|██████▏   | 62/100 [06:39<03:20,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures the same key capabilities mentioned in the Correct Answer. Both answers highlight:\n",
      "\n",
      "1. Question answering/interactive capabilities for building systems like chatbots\n",
      "2. Text analysis capabilities for personalization through understanding sentiment and preferences\n",
      "\n",
      "The Generated Answer simply provides slightly more detail and specific examples (like customer support chatbots and educational AI tutors), but the core capabilities described are the same. There are no contradictions or missing critical pieces of information between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "5\n",
      "0,1,3\n",
      "[0, 1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  63%|██████▎   | 63/100 [06:45<03:18,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures all the key elements from the Correct Answer and presents them in essentially the same order:\n",
      "\n",
      "1. Both answers mention the message_start event coming first\n",
      "2. Both describe the content blocks structure with start, delta, and stop events\n",
      "3. Both mention message_delta events\n",
      "4. Both include the message_stop event at the end\n",
      "5. Both note that ping events may be dispersed throughout\n",
      "\n",
      "The Generated Answer actually provides slightly more detail in its structure, but the core information matches perfectly with the Correct Answer. There are no contradictions between the two answers, and no critical pieces of information are missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "1,2,4\n",
      "[1, 2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  64%|██████▍   | 64/100 [06:50<03:10,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It conveys the same key information as the Correct Answer - specifically that the Anthropic API allows up to 20 images per request while the claude.ai interface has a lower limit of 5 images per turn. While the Generated Answer is more concise and uses slightly different wording, it captures the essential numerical limits accurately and maintains the key comparison between the two interfaces. There are no missing critical details or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "7\n",
      "2,3,4\n",
      "[2, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  65%|██████▌   | 65/100 [06:56<03:18,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the key substance of the Correct Answer, which is that when Claude's response contains an incomplete tool use block due to hitting the max_tokens limit, you should retry with a higher max_tokens value. The Generated Answer conveys the same essential instruction and solution as the Correct Answer, just with slightly different wording. There are no missing critical pieces of information or contradictions between the two answers. Both answers communicate the same core concept and recommended action.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "2,15,12\n",
      "[2, 15, 12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  66%|██████▌   | 66/100 [07:01<03:05,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. While both answers agree on the first step (developing test cases), they differ on the second step. The Correct Answer states that the second step is to \"take a look at Anthropic's guide to developing test cases\", while the Generated Answer states it is to \"build a strong input prompt\". These are substantively different steps. The Generated Answer misses the critical guidance about consulting Anthropic's documentation/guide, which is an important preparatory step mentioned in the Correct Answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n",
      "16\n",
      "1,3,4\n",
      "[1, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  67%|██████▋   | 67/100 [07:07<03:03,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same core concept - that you can use the content parameter with an \"assistant\" role message to pre-fill or influence Claude's response. The Generated Answer provides more detail and an example, but the fundamental meaning matches the Correct Answer. Both answers explain that this technique allows you to shape or guide Claude's output. There are no contradictions between the answers, and no critical information from the Correct Answer is missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "5\n",
      "0, 2, 3\n",
      "[0, 2, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  68%|██████▊   | 68/100 [07:12<02:52,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures both key advantages mentioned in the Correct Answer:\n",
      "\n",
      "1. It correctly states that prompt engineering preserves general knowledge while fine-tuning risks catastrophic forgetting\n",
      "2. It accurately notes that prompt engineering is more effective at helping models understand and utilize external content/retrieved documents\n",
      "\n",
      "The Generated Answer essentially restates the same two main points from the Correct Answer, just with slightly different wording. There are no missing critical pieces of information and no contradictions between the two answers. The substance and meaning are identical.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "2,1,3\n",
      "[2, 1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  69%|██████▉   | 69/100 [07:17<02:46,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. The key difference is that the Generated Answer describes steps for using Anthropic's direct API (obtaining an Anthropic account/API key), while the Correct Answer specifically addresses Bedrock API integration, which requires AWS CLI configuration and an AWS SDK. These are fundamentally different authentication and setup processes. The Generated Answer misses the critical AWS-specific components required for Bedrock API access and instead describes a different integration path. This represents a substantive difference in the setup process, not just a minor variation in wording.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,13,15\n",
      "[0, 13, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:XML parsing error: mismatched tag: line 3, column 591\n",
      "Evaluating End-to-End:  70%|███████   | 70/100 [07:22<02:38,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It provides exactly the same AWS CLI command as the Correct Answer (`aws bedrock list-foundation-models --region=<region> --by-provider anthropic --query \"modelSummaries[*].modelId\"`), explains that you need to replace `<region>` with your desired region (using the same example of \"us-west-2\"), and correctly states that this will list the available Claude models in that region. The substance and technical details are identical between both answers, with only minor differences in phrasing that don't affect the accuracy of the information.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 70/100 questions. Current Accuracy: 0.7857\n",
      "6\n",
      "1,0,2\n",
      "[1, 0, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  71%|███████   | 71/100 [07:27<02:27,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same core information - that the `input_type` argument can be passed to specify whether the input text is a \"query\" or \"document\". The Generated Answer actually provides additional detail about how the input_type affects processing, but this extra information doesn't contradict the Correct Answer. The essential point about the existence and purpose of the `input_type` parameter is accurately conveyed in both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "6\n",
      "0,1,2\n",
      "[0, 1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  72%|███████▏  | 72/100 [07:31<02:13,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is incomplete compared to the correct answer. While it correctly explains that tool_use deltas contain partial JSON strings and text deltas contain direct text updates, it misses a key piece of information: that tool_use deltas may have delays between streaming events as the model emits one complete key-value pair at a time. This timing/delay characteristic is an important distinction between the two formats that was specified in the correct answer but omitted from the generated answer.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n",
      "17\n",
      "0,3,8\n",
      "[0, 3, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  73%|███████▎  | 73/100 [07:35<02:04,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It contains the exact same key information as the Correct Answer - specifically that the API has a 5MB per image limit while claude.ai has a 10MB per image limit. The Generated Answer simply presents this information in a slightly different format (bullet points) and adds a minor additional detail about error messages, but the core substance regarding the file size limits is identical to the Correct Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "3,1,16\n",
      "[3, 1, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  74%|███████▍  | 74/100 [07:41<02:05,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers emphasize the key point of choosing a model that appropriately balances speed and output quality based on specific requirements. The Generated Answer actually provides additional details about factors that might affect latency (like output length and Chain of Thought), but the core message aligns with the Correct Answer. There are no contradictions between the two answers, and the Generated Answer includes all critical information from the Correct Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,3,4\n",
      "[0, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  75%|███████▌  | 75/100 [07:47<02:15,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer captures the key points from the Correct Answer:\n",
      "1. It correctly identifies the voyage-code-2 model as the recommended model\n",
      "2. It correctly mentions the 17% performance improvement\n",
      "\n",
      "However, the Generated Answer misses one important detail from the Correct Answer: it doesn't mention that the model achieves state-of-the-art results on general-purpose corpora. This is a significant piece of information about the model's capabilities that was included in the Correct Answer.\n",
      "\n",
      "Additionally, there's a small error in attributing this to \"Anthropic's Voyage AI\" - the Correct Answer simply mentions \"Voyage AI\" as a separate entity.\n",
      "\n",
      "Since there is a critical piece of information missing (the state-of-the-art performance on general-purpose corpora), this should be marked as incorrect, even though much of the basic information is accurate.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "3,0,2\n",
      "[3, 0, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  76%|███████▌  | 76/100 [07:53<02:13,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is partially correct but not fully aligned with the Correct Answer. While both answers mention interactive Jupyter notebooks and working with PDFs, the Generated Answer diverges by discussing extending Claude's capabilities and VoyageAI, which aren't mentioned in the Correct Answer. Most importantly, the Correct Answer specifically mentions \"embeddings\" as one of the two key ways, but while the Generated Answer mentions embeddings, it associates them with VoyageAI rather than Anthropic's APIs. Since the Correct Answer focuses specifically on PDF handling and embeddings as the two key ways, and the Generated Answer introduces different elements while not properly capturing the embeddings aspect, it should be marked as incorrect.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,1,18\n",
      "[0, 1, 18]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  77%|███████▋  | 77/100 [07:59<02:10,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures the key relationship between context window size and RAG effectiveness described in the Correct Answer. Both answers emphasize that:\n",
      "\n",
      "1. A larger context window enables the model to utilize more retrieved information\n",
      "2. This leads to better/more accurate outputs\n",
      "3. The fundamental connection between context window size and the model's ability to process retrieved information is maintained\n",
      "\n",
      "While the Generated Answer provides additional details about smaller context windows and their limitations, this extra information doesn't contradict the Correct Answer. The core concept - that context window size determines how much retrieved information can be used in RAG - is preserved in both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,4,13\n",
      "[0, 4, 13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  78%|███████▊  | 78/100 [08:06<02:10,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures all the key elements from the Correct Answer and even expands on them in a helpful way. Both answers emphasize:\n",
      "\n",
      "1. The tool's ability to identify edge cases where prompts might not work well\n",
      "2. The capability to rate and assess individual results\n",
      "3. The importance of ensuring consistent performance across different inputs\n",
      "4. The use of the tool for prompt refinement and improvement\n",
      "5. The goal of building more robust AI applications\n",
      "\n",
      "The Generated Answer actually provides more detailed structure and adds information about the beta status and feedback loop, but this additional information doesn't contradict the Correct Answer - it simply provides more context. The core substance and main points are aligned between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "10,19,6\n",
      "[10, 19, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  79%|███████▉  | 79/100 [08:10<01:55,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. The Correct Answer specifically states that Claude 3 Haiku has the fastest comparative latency, while the Generated Answer is vague and non-specific, merely referring to whichever model best fits user requirements. This is a critical missing piece of information. The Generated Answer does not actually identify which model has the fastest latency, which was the specific ask of the question. The Generated Answer appears to be hedging rather than providing the concrete information that was requested.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,1,10\n",
      "[0, 1, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  80%|████████  | 80/100 [08:18<02:03,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It conveys the same core concept as the Correct Answer - that to have a multi-turn conversation using the Anthropic Messages API, you need to send the full conversation history with each request because the API is stateless. The Generated Answer actually provides more detail and a concrete code example, but the fundamental principle matches the Correct Answer exactly. Both answers emphasize that:\n",
      "\n",
      "1. The full conversation history must be included with each request\n",
      "2. The API is stateless\n",
      "3. Previous messages from both user and assistant need to be included\n",
      "\n",
      "There are no contradictions between the answers, and the Generated Answer doesn't miss any critical information from the Correct Answer. The additional implementation details in the Generated Answer don't change the core correctness of the response.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 80/100 questions. Current Accuracy: 0.7625\n",
      "12\n",
      "2,3,6\n",
      "[2, 3, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  81%|████████  | 81/100 [08:24<01:56,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it conveys the same core message as the Correct Answer. Both answers emphasize that using XML tags to provide a specific role context (like General Counsel) helps Claude perform a more thorough and critical analysis of legal contracts, potentially identifying important issues that might otherwise be missed. While the Generated Answer provides more detailed examples and elaboration, the fundamental point about role context improving Claude's legal analysis capabilities remains the same. There are no contradictions between the two answers, and no critical information from the Correct Answer is missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "11\n",
      "0, 3, 9\n",
      "[0, 3, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  82%|████████▏ | 82/100 [08:30<01:50,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is essentially correct in its core claim about how the two models handle missing information differently. While it includes some additional details about chain of thought reasoning that aren't mentioned in the correct answer, its key point about how Opus seeks clarification while Sonnet tends to infer missing parameters aligns with the correct answer. The substance of both answers conveys the same fundamental distinction: Opus is more likely to ask for missing information, while Sonnet is more likely to make inferences and proceed with reasonable values. The additional information in the generated answer doesn't contradict the correct answer, it just provides extra context.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "13\n",
      "0, 2, 6\n",
      "[0, 2, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  83%|████████▎ | 83/100 [08:37<01:47,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it covers all the key points mentioned in the Correct Answer and even provides additional helpful detail. Both answers emphasize:\n",
      "\n",
      "1. Implementing retry logic for error handling\n",
      "2. Conducting thorough staging/testing\n",
      "3. Load testing\n",
      "4. Error handling and logging setup\n",
      "5. Gradual rollout process\n",
      "6. Documentation and training\n",
      "7. Monitoring and alerting\n",
      "\n",
      "The Generated Answer expands on these points with more specific implementation details, but the core recommendations align perfectly with the Correct Answer. There are no contradictions between the two answers, and no critical pieces of information from the Correct Answer are missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "12\n",
      "1, 10, 0\n",
      "[1, 10, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  84%|████████▍ | 84/100 [08:44<01:43,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. While it provides much more detail and additional metrics compared to the Correct Answer, it explicitly covers all three key elements mentioned in the Correct Answer:\n",
      "\n",
      "1. Accuracy - Directly mentioned and explained\n",
      "2. Cost - Discussed in terms of calculating average cost per classification\n",
      "3. Speed - Covered through discussion of response time metrics\n",
      "\n",
      "The additional information provided (F1-score, consistency, structure, etc.) doesn't contradict the Correct Answer but rather expands upon it. Since all core elements from the Correct Answer are present in the Generated Answer, and there are no contradictions, the Generated Answer should be considered correct.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,4,10\n",
      "[0, 4, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  85%|████████▌ | 85/100 [08:49<01:29,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers identify the same two recommended methods for learning prompt engineering with Claude:\n",
      "1. The GitHub prompting tutorial\n",
      "2. The Google Sheets prompting tutorial\n",
      "\n",
      "The Generated Answer provides slightly more detail by mentioning that the Google Sheets version is a \"lighter weight version\" and that the GitHub tutorial is \"example-filled,\" but these are just additional descriptive details that don't change the core substance. The fundamental recommendation of these two specific tutorials as learning methods is identical between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "3,1,16\n",
      "[3, 1, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  86%|████████▌ | 86/100 [08:55<01:25,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct as it captures all the key substantive points from the Correct Answer. Both answers emphasize:\n",
      "\n",
      "1. The distinction between initial pretraining on text data vs additional RLHF training for Claude\n",
      "2. The difference in capabilities - pretrained models being less effective at following instructions and helpful interactions compared to Claude\n",
      "3. The purpose of the additional training being to make Claude more helpful and effective at tasks\n",
      "\n",
      "The Generated Answer actually provides additional detail about behavioral alignment through RLHF, but this elaboration is consistent with and supports the core distinctions outlined in the Correct Answer. There are no contradictions between the two answers, and no critical information from the Correct Answer is missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,5,11\n",
      "[0, 5, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  87%|████████▋ | 87/100 [09:04<01:29,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct and actually provides a more detailed expansion of the key points mentioned in the Correct Answer. It covers all the main advantages mentioned in the Correct Answer:\n",
      "\n",
      "1. Speed and time efficiency (\"faster\" in Correct Answer matches \"time-saving\" in Generated)\n",
      "2. Cost-effectiveness (mentioned in both)\n",
      "3. Lower resource requirements (mentioned in both)\n",
      "4. Preservation of general knowledge (mentioned in both)\n",
      "5. Flexibility and rapid iteration (mentioned in both)\n",
      "6. Transparency (mentioned in both)\n",
      "\n",
      "The Generated Answer elaborates on these points with more specific examples and explanations, but doesn't contradict any points in the Correct Answer. It maintains the same core message while providing additional supporting details. There are no critical pieces of information from the Correct Answer that are missing from the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "1,12,14\n",
      "[1, 12, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  88%|████████▊ | 88/100 [09:09<01:16,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. The core instruction about running `gcloud auth application-default login` to authenticate with GCP is present and matches exactly with the Correct Answer. While the Generated Answer provides additional context about using the SDK and making requests afterward, this extra information doesn't contradict the core authentication step specified in the Correct Answer. The substance of how to authenticate is identical between both answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,1,11\n",
      "[0, 1, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  89%|████████▉ | 89/100 [09:14<01:06,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer captures the core information about the Prompt Generator tool introduced by Anthropic on May 10th, 2024, and its main purpose of helping users create tailored prompts for specific tasks. While the Correct Answer includes additional context about the Claude iOS app and Claude Team plan, these are supplementary details that don't affect the central point about the Prompt Generator's capabilities. The Generated Answer accurately conveys the key functionality and purpose of the new feature, even if it's more concise. There are no contradictions between the two answers, and the essential information about the tool's purpose and functionality is preserved in the Generated Answer.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "0,1,3\n",
      "[0, 1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  90%|█████████ | 90/100 [09:18<00:54,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It conveys exactly the same information as the Correct Answer - that both Claude 3.5 Sonnet and the Artifacts feature became available on June 20th, 2024. While the wording is slightly different (omitting \"both\" and having a slightly different sentence structure), the core information and meaning are identical. There are no missing critical details or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 90/100 questions. Current Accuracy: 0.7889\n",
      "6\n",
      "1,2,4\n",
      "[1, 2, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  91%|█████████ | 91/100 [09:22<00:45,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same core information - that to limit Claude's response to a single token, you should use the \"max_tokens\" parameter set to 1 in the request. The Generated Answer uses slightly different wording by mentioning \"request body\" instead of just \"request,\" but this is a minor detail that doesn't change the fundamental meaning. Both answers accurately describe how to achieve the desired single-token limitation.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "7\n",
      "0,1,5\n",
      "[0, 1, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  92%|█████████▏| 92/100 [09:27<00:38,  4.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. Both answers convey the same core concept that temperature controls randomness in the model's output generation. The Generated Answer simply provides more detail and elaboration about what higher and lower temperatures do specifically, but the fundamental meaning matches the Correct Answer. There are no contradictions between the two answers, and the Generated Answer includes the key concept about randomness control that is present in the Correct Answer. The additional details in the Generated Answer serve to explain the concept further rather than change its meaning.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "1,2,11\n",
      "[1, 2, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  93%|█████████▎| 93/100 [09:32<00:35,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is incorrect. While it correctly identifies one way to specify API parameters (adding them as additional arguments after the prompt and model), it misses the second key method mentioned in the Correct Answer - the ability to pass in an API key for a specific cell. Instead, it incorrectly states that CLAUDEMESSAGES is the second method. The CLAUDEMESSAGES function is not mentioned in the Correct Answer at all, and appears to be incorrect information. Since one of the two main methods is completely different from what's specified in the Correct Answer, this constitutes a significant error.</explanation>\n",
      "<is_correct>false</is_correct>\n",
      "</content>\n",
      "\n",
      "5\n",
      "0,1,2\n",
      "[0, 1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  94%|█████████▍| 94/100 [09:37<00:30,  5.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer captures all the key points from the Correct Answer:\n",
      "1. Prefilling with { causes Claude to skip the preamble\n",
      "2. Results in direct JSON object output\n",
      "3. Makes the response more concise\n",
      "4. Makes it easier for programs to parse\n",
      "\n",
      "While the exact wording differs slightly, the substance and meaning are identical. The Generated Answer effectively communicates the same information about how prefilling with a curly brace affects Claude's output behavior. There are no missing critical pieces of information or contradictions between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "1, 13, 14\n",
      "[1, 13, 14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  95%|█████████▌| 95/100 [09:43<00:25,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The generated answer is partially correct but contains extra information that is not verified by the correct answer. The first two points about the multimodal cookbook and API reference documentation match the correct answer's substance. However, the third point about the developer community is not mentioned in the correct answer and appears to be additional unverified information. Since this addition doesn't contradict the correct information but rather adds to it, and the core resources (cookbook and API reference) are accurately captured, the generated answer can be considered substantially correct in terms of the key information provided.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "1,0,2\n",
      "[1, 0, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  96%|█████████▌| 96/100 [09:48<00:21,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct and actually provides more detailed information than the Correct Answer while maintaining the same core information. Both answers convey that:\n",
      "\n",
      "1. The API key can be specified as a parameter when creating a new Anthropic client\n",
      "2. If not provided explicitly, the SDK will default to using the ANTHROPIC_API_KEY environment variable\n",
      "\n",
      "The Generated Answer goes further by providing specific code examples in both Python and TypeScript, but this additional detail doesn't contradict or omit any of the key information from the Correct Answer. The substance of both answers is essentially the same.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "1,4,15\n",
      "[1, 4, 15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  97%|█████████▋| 97/100 [09:54<00:16,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the same two key benefits mentioned in the Correct Answer:\n",
      "\n",
      "1. Both answers mention identifying edge cases where prompts might fail/falter\n",
      "2. Both answers discuss ensuring consistent performance across test inputs/range of inputs\n",
      "\n",
      "The Generated Answer breaks these points out more explicitly but conveys the same core information. While the wording differs slightly, the substance and meaning are essentially identical. There are no contradictions or missing critical pieces of information between the two answers.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "20\n",
      "4,14,7\n",
      "[4, 14, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  98%|█████████▊| 98/100 [10:01<00:11,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures the key distinction presented in the Correct Answer - that the pretrained model is not inherently good at answering questions or following instructions, and that the final version of Claude required fine-tuning and RLHF to become the helpful assistant available through the API. While the Generated Answer provides additional details about capabilities, biases, and availability, these details don't contradict the Correct Answer, they merely expand upon it. The core message about the transformation from pretrained model to final assistant through fine-tuning and RLHF is preserved and accurately conveyed.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "1\n",
      "0\n",
      "[0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End:  99%|█████████▉| 99/100 [10:04<00:04,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is exactly identical to the Correct Answer, stating that Anthropic's IPv6 address range is 2607:6bc0::/48. There are no differences in wording or substance, and all critical information is included.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "17\n",
      "0,3,8\n",
      "[0, 3, 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating End-to-End: 100%|██████████| 100/100 [10:08<00:00,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<content>\n",
      "<explanation>The Generated Answer is correct. It captures both key methods of specifying the API key that are mentioned in the Correct Answer:\n",
      "1. Using an environment variable (ANTHROPIC_API_KEY)\n",
      "2. Passing the API key directly when initializing the client\n",
      "\n",
      "While the Generated Answer is more concise, it contains the same essential information as the Correct Answer. The wording differences are minor and don't change the substance of the answer. Both answers describe the same two methods for providing the API key when using the Python SDK.</explanation>\n",
      "<is_correct>true</is_correct>\n",
      "</content>\n",
      "\n",
      "Processed 100/100 questions. Current Accuracy: 0.8000\n",
      "Detailed results saved to evaluation_results_detailed_level_three.csv\n",
      "Average Precision: 0.4433\n",
      "Average Recall: 0.7033\n",
      "Average F1: 0.5439\n",
      "Average Mean Reciprocal Rank: 0.850000\n",
      "End-to-End Accuracy: 0.8000\n",
      "Evaluation complete. Results saved to evaluation_results_level_three.json, evaluation_results_detailed_level_three.csv, and evaluation_results_level_three.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SummaryIndexedVectorDB\n",
    "level_three_db = SummaryIndexedVectorDB(\"anthropic_docs_v3\")\n",
    "level_three_db.load_data('data/anthropic_summary_indexed_docs.json')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Run the evaluations\n",
    "avg_precision, avg_recall, avg_mrr, f1, precisions, recalls, mrrs  = evaluate_retrieval(retrieve_advanced, eval_data, level_three_db)\n",
    "e2e_accuracy, e2e_results = evaluate_end_to_end(answer_query_advanced, level_two_db, eval_data)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'question': [item['question'] for item in eval_data],\n",
    "    'retrieval_precision': precisions,\n",
    "    'retrieval_recall': recalls,\n",
    "    'retrieval_mrr': mrrs,\n",
    "    'e2e_correct': e2e_results\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('evaluation/csvs/evaluation_results_detailed_level_three.csv', index=False)\n",
    "print(\"Detailed results saved to evaluation_results_detailed_level_three.csv\")\n",
    "\n",
    "# Plot the results\n",
    "# Print the results\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average F1: {f1:.4f}\")\n",
    "print(f\"Average Mean Reciprocal Rank: {avg_mrr:4f}\")\n",
    "print(f\"End-to-End Accuracy: {e2e_accuracy:.4f}\")\n",
    "\n",
    "# Save the results to a file\n",
    "with open('evaluation/json_results/evaluation_results_level_three.json', 'w') as f:\n",
    "    json.dump({\n",
    "        \"name\": \"Summary Indexing + Re-Ranking\",\n",
    "        \"average_precision\": avg_precision,\n",
    "        \"average_recall\": avg_recall,\n",
    "        \"average_f1\": f1,\n",
    "        \"average_mrr\": avg_mrr,\n",
    "        \"end_to_end_accuracy\": e2e_accuracy\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"Evaluation complete. Results saved to evaluation_results_level_three.json, evaluation_results_detailed_level_three.csv, and evaluation_results_level_three.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing our performance\n",
    "plot_performance('evaluation/json_results', ['Basic RAG', 'Summary Indexing', 'Summary Indexing + Re-Ranking'], colors=['skyblue', 'lightgreen', 'salmon'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This guide has illustrated the importance of measuring prompt performance empirically when prompt engineering. You can read more about our empirical methodology to prompt engineering here. Using a Jupyter Notebook is a great way to start prompt engineering but as your datasets grow larger and your prompts more numerous it is important to leverage tooling that will scale with you.\n",
    "\n",
    "In this section of the guide we will explore using Promptfoo an open source LLM evaluation toolkit. To get started head over to the ./evaluation directory and checkout the ./evaluation/README.md.\n",
    "\n",
    "Promptfoo makes it very easy to build automated test suites that compare different models, hyperparameter choices, and prompts against one another.\n",
    "\n",
    "As an example, you can run the below cell to see the average performance of Haiku vs 3.5 Sonnet across all of our test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the JSON file\n",
    "with open('data/end_to_end_results.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the results\n",
    "results = data['results']['results']\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Extract provider, prompt, and score information\n",
    "df['provider'] = df['provider'].apply(lambda x: x['label'] if isinstance(x, dict) else x)\n",
    "df['prompt'] = df['prompt'].apply(lambda x: x['label'] if isinstance(x, dict) else x)\n",
    "\n",
    "# Function to safely extract scores\n",
    "def extract_score(x):\n",
    "    if isinstance(x, dict) and 'score' in x:\n",
    "        return x['score'] * 100  # Convert to percentage\n",
    "    return np.nan\n",
    "\n",
    "df['score'] = df['gradingResult'].apply(extract_score)\n",
    "\n",
    "# Group by provider and prompt, then calculate mean scores\n",
    "result = df.groupby(['provider', 'prompt'])['score'].mean().unstack()\n",
    "\n",
    "# Fill NaN values with 0\n",
    "result = result.fillna(0)\n",
    "\n",
    "# Calculate the average score across all prompts for each provider\n",
    "result['Average'] = result.mean(axis=1)\n",
    "\n",
    "# Sort the result by the average score\n",
    "result = result.sort_values(by='Average', ascending=False)\n",
    "\n",
    "# Round the results to 2 decimal places\n",
    "result = result.round(2)\n",
    "# Calculate overall statistics\n",
    "overall_average = result['Average'].mean()\n",
    "overall_std = result['Average'].std()\n",
    "best_provider = result['Average'].idxmax()\n",
    "worst_provider = result['Average'].idxmin()\n",
    "\n",
    "print(f\"\\nOverall Statistics:\")\n",
    "print(f\"Best Performing Provider: {best_provider} ({result.loc[best_provider, 'Average']:.2f}%)\")\n",
    "print(f\"Worst Performing Provider: {worst_provider} ({result.loc[worst_provider, 'Average']:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
